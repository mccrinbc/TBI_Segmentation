{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm #loading bar  \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as ParentDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "import segmentation_models_pytorch as smp #model we're using for now. \n",
    "import evaluateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Errors associated to potantial randomness / non-deterministic behaviour is a VERY common issue in PT. \n",
    "#Look at the following github discussion for more information: \n",
    "#https://github.com/pytorch/pytorch/issues/7068\n",
    "#      sbelharbi commented on Apr 19, 2019\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is related to the init function of the worker(s) to be fed to the DataLoader\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBI_dataset(ParentDataset): #Obtain the attributes of ParentDataset from torch.utils.data\n",
    "#Finds Image and Label locations, creates random list of indicies for training / val / testing sets to be called\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        labels_dir,\n",
    "        train_size = 0.75, #fraction of total number of samples to be used in training set\n",
    "        subset=\"train\",\n",
    "        transform = None, #base transformation is into Tensor. \n",
    "        random_sampling=True,\n",
    "        seed=42, #We'll get the same thing everytime if we keep using the same seed. \n",
    "    ):\n",
    "        #filter and sort the list\n",
    "        self.ImageIds = sorted(list(filter(('.DS_Store').__ne__,os.listdir(images_dir)))) \n",
    "        self.LabelIds = sorted(list(filter(('.DS_Store').__ne__,os.listdir(labels_dir))))\n",
    "        \n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ImageIds] #full_paths to slices\n",
    "        self.labels_fps = [os.path.join(labels_dir, image_id) for image_id in self.LabelIds] #full_paths to labels\n",
    "        \n",
    "        if random_sampling == True:\n",
    "            samples = list(range(0,len(self.images_fps))) #create a list of numbers\n",
    "            random.seed(seed) #set the seed\n",
    "            \n",
    "            #random sample train_size amount and then do a train/validation split \n",
    "            indicies = random.sample(samples,round(train_size*len(samples)))\n",
    "            self.val_indicies = indicies[0:round(len(indicies)*0.15)]\n",
    "            self.train_indicies = indicies[round(len(indicies)*0.15) : len(indicies)]\n",
    "            \n",
    "            test_indicies = samples\n",
    "            for j in sorted(indicies, reverse = True): #remove the train/val indicies from test set\n",
    "                del test_indicies[j]\n",
    "            \n",
    "            #suffles without replacement. \n",
    "            self.test_indicies = random.sample(test_indicies, len(test_indicies)) \n",
    "\n",
    "        #We define a mapping to use when calling the Dataset loader based on the parameter \"subset\"\n",
    "        if subset == \"train\":\n",
    "            self.mapping = self.train_indicies\n",
    "        elif subset == \"val\":\n",
    "            self.mapping = self.val_indicies\n",
    "        elif subset == \"test\":\n",
    "            self.mapping = self.test_indicies\n",
    "        else:\n",
    "            print(\"subset parameter requires train, val, or test exactly.\")\n",
    "            \n",
    "        self.transform = transform #trasform given by transform_function\n",
    "            \n",
    "    def __getitem__(self, ii): #ii is the index\n",
    "        \n",
    "        #Current implementations of transforms only use PIL images.\n",
    "        #Apparently we can use np.array(Image.open(...)) to remove the error that happens each epoch\n",
    "        image = Image.open(self.images_fps[self.mapping[ii]]) #open as PIL image. \n",
    "        label = Image.open(self.labels_fps[self.mapping[ii]])\n",
    "         \n",
    "        image = self.transform(image)\n",
    "        label = self.transform(label)\n",
    "             \n",
    "        return image, label #, self.images_fps[self.mapping[ii]],self.labels_fps[self.mapping[ii]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "    \n",
    "    \n",
    "def datasets(images_dir, labels_dir, train_size, aug_angle, aug_scale, flip_prob):\n",
    "    train = TBI_dataset(\n",
    "        images_dir = images_dir,\n",
    "        labels_dir = labels_dir,\n",
    "        train_size = 0.75,\n",
    "        subset = \"train\",\n",
    "        transform = transform_function(degrees=aug_angle, scale=aug_scale, flip_prob=flip_prob),\n",
    "    )\n",
    "    valid = TBI_dataset(\n",
    "        images_dir = images_dir,\n",
    "        labels_dir = labels_dir,\n",
    "        train_size = 0.75,\n",
    "        subset = \"val\",\n",
    "        transform = transform_function(degrees=aug_angle, scale=aug_scale, flip_prob=flip_prob),\n",
    "    )\n",
    "    \n",
    "    test = TBI_dataset(\n",
    "        images_dir = images_dir,\n",
    "        labels_dir = labels_dir,\n",
    "        train_size = 0.75,\n",
    "        subset=\"test\",\n",
    "        transform = transform_function(degrees=0, scale = [1,1], flip_prob = 0), #make sure nothing changes. \n",
    "    )\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def transform_function(degrees,scale,flip_prob):\n",
    "    transform_list = []\n",
    "    \n",
    "    transform_list.append(transforms.RandomAffine(degrees, scale = scale))\n",
    "    transform_list.append(transforms.RandomHorizontalFlip(p=flip_prob))\n",
    "    transform_list.append(transforms.Pad(37)) #all images should be 182x182 before padding. \n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    \n",
    "    return Compose(transform_list)\n",
    "\n",
    "def Weights(labels):\n",
    "    #expects an [batch_size,c,n,n] input \n",
    "    \n",
    "    weights = []\n",
    "    for batch_num in range(0,labels.shape[0]):\n",
    "        num_ones = torch.sum(labels[batch_num,0,:,:]);\n",
    "        resolution = labels.shape[2] * labels.shape[3]\n",
    "        num_zeros = resolution - num_ones \n",
    "        weights.append(num_zeros / (num_ones + 1))\n",
    "        \n",
    "    #this keeps the clas imbalance in check\n",
    "    return torch.Tensor(weights) #to ensure that we're getting a real number in the division  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.75\n",
    "batch_size = 12\n",
    "EPOCHS = 40\n",
    "lr = 0.0001\n",
    "aug_angle = 25\n",
    "aug_scale = [1,1.5]\n",
    "flip_prob = 0.5\n",
    "num_workers = 1\n",
    "images_dir = \"/home/mccrinbc/Registered_Brains_FA/normalized_slices\"\n",
    "labels_dir = \"/home/mccrinbc/Registered_Brains_FA/slice_labels\"\n",
    "\n",
    "#images_dir = \"/Users/brianmccrindle/Documents/Research/TBIFinder_Final/Registered_Brains_FA/test_slices\"\n",
    "#labels_dir = \"/Users/brianmccrindle/Documents/Research/TBIFinder_Final/Registered_Brains_FA/test_labels\"\n",
    "\n",
    "#smp specific variables\n",
    "ENCODER = 'resnet101'\n",
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.5,               # dropout ratio, default is None\n",
    "    #activation='softmax2d',    # activation function, default is None. This is the output activation. softmax2d specifies dim = 1 \n",
    "    classes=1,                 # define number of output labels\n",
    ")\n",
    "\n",
    "#classes = 2 for the softmax transformation. \n",
    "model = smp.Unet(encoder_name = ENCODER, in_channels=1, classes = 1, aux_params = aux_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = datasets(images_dir, labels_dir, train_size, aug_angle, aug_scale, flip_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomAffine(degrees=(0, 0), scale=[1, 1])\n",
       "    RandomHorizontalFlip(p=0)\n",
       "    Pad(padding=37, fill=0, padding_mode=constant)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(train_dataset, valid_dataset, lr):\n",
    "    \n",
    "    earlystop = False \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        dev =\"cuda:2\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "        \n",
    "    dev = torch.device(dev)\n",
    "    \n",
    "    #this might break, remove worker_init_fn = _init_fn(num_workers)) if so\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle = True, num_workers = num_workers, worker_init_fn = _init_fn(num_workers))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size, shuffle = True, num_workers = num_workers, worker_init_fn = _init_fn(num_workers))\n",
    "    \n",
    "    model.to(dev) #cast the model onto the device \n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr) #learning rate should change \n",
    "    \n",
    "    loss_function = torch.nn.BCELoss() #this takes in a weighted input and incorporates a sigmoid transformation\n",
    "    #loss_function = smp.utils.losses.DiceLoss()\n",
    "    #loss_function = DiceLoss()\n",
    "    #metrics = [smp.utils.metrics.IoU(threshold=0.5)]\n",
    "    \n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    epochLoss_train = []\n",
    "    epochLoss_valid = []\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        image_count = 0\n",
    "        for phase in [\"train\",\"val\"]:\n",
    "            \n",
    "            #This determines which portions of the model will have gradients turned off or on. \n",
    "            if phase == \"train\":\n",
    "                model.train() #put into training mode\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval() #evaluation mode.\n",
    "                loader = valid_loader\n",
    "                  \n",
    "            for ii, data in enumerate(loader): \n",
    "                \n",
    "                brains = data[0] #[batch_size,channels,height,width] \n",
    "                labels = data[1]\n",
    "                \n",
    "                image_count += len(brains)\n",
    "                print(epoch, phase, ii, image_count)\n",
    "                \n",
    "                brains,labels = brains.to(dev), labels.to(dev) #put the data onto the device\n",
    "                predictions, single_class = model(brains) #single class is not a useful output. \n",
    "                \n",
    "                predictions = torch.sigmoid(predictions) #using this so that the output is bounded [0,1]\n",
    "                single_class = torch.sigmoid(single_class)\n",
    "                \n",
    "                weights = Weights(labels) #generate the weights for each slice in the batch\n",
    "                loss_function.pos_weight = weights                    \n",
    "                \n",
    "                loss = loss_function(predictions, labels) #loss changes here. \n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    #employ this so we don't get multiples in the same list. \n",
    "                    if (loss_valid and ii == 0): #if loss_valid is NOT empty AND it's the first time we see this\n",
    "                        epochLoss_valid.append(loss_valid[-1]) #append the last value in the \n",
    "                        \n",
    "                    model.zero_grad() #recommended way to perform validation\n",
    "                    loss_train.append(loss.item())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    print(f\"Phase: {phase}. Epoch: {epoch}. Loss: {loss.item()}\") \n",
    "               \n",
    "                else:\n",
    "                    if (loss_train and ii == 0):#if loss_valid is NOT empty AND it's the first time we see this\n",
    "                        epochLoss_train.append(loss_train[-1]) #append the last value in the loss_train list.\n",
    "                        \n",
    "                    loss_valid.append(loss.item())\n",
    "                    print(f\"Phase: {phase}. Epoch: {epoch}. Loss: {loss.item()}\") \n",
    "                    \n",
    "                    #learning rate changes and early stopping\n",
    "                    if epoch > 0:\n",
    "                        if (epoch % 10) == 0: #if the epoch is divisable by 10\n",
    "                            meanVal = np.mean(loss_valid[epoch - 10 : epoch])\n",
    "                            if np.abs((meanVal - loss.item()) / meanVal) <= 0.05: #if the %difference is small\n",
    "                                for param_group in optimizer.param_groups:\n",
    "                                    lr = lr * 0.1 #reduce the learning rate by a factor of 10. \n",
    "                                    param_group['lr'] = lr\n",
    "                        \n",
    "                        if (epoch % 50) == 0:\n",
    "                            meanVal = np.mean(loss_valid[epoch - 50 : epoch])\n",
    "                            if np.abs((meanVal - loss.item()) / meanVal) <= 0.05:\n",
    "                                earlystop = True \n",
    "               \n",
    "                #Implementation of early stopping\n",
    "                if earlystop == True:\n",
    "                    date = datetime.now()\n",
    "                    torch.save(model.state_dict(), os.path.join(os.getcwd(), \"Registered_Brains_FA/models_saved\", \"TBI_model-epoch\" + str(epoch) + '-' + str(date.date()) + '-' + str(date.hour) + '-' + str(date.minute) +\"-EARLYSTOP.pt\")) #save the model \n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            #save the model at the end of this epoch.\n",
    "            #date = datetime.now()\n",
    "            #torch.save(model.state_dict(), os.path.join(os.getcwd(), \"Registered_Brains_FA/models_saved\", \"TBI_model-epoch\" + str(epoch) + '-' + str(date.date()) + '-' + str(date.hour) + '-' + str(date.minute) + \".pt\"))\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    #Need to add the last element from loss_valid to epochLoss_valid to equal the number of epochs. \n",
    "    epochLoss_valid.append(loss_valid[-1])\n",
    "    return brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(test_dataset, modelPath, threshold): #model = the model class = smp.UNet()\n",
    "\n",
    "    total_images = 0\n",
    "    CM_values = [0,0,0,0] #tp, fn, fp, tn\n",
    "    model.load_state_dict(torch.load(modelPath))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        dev =\"cuda:2\"\n",
    "        print(\"GPU is active\")\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "        \n",
    "    dev = torch.device(dev)\n",
    "    model.to(dev) \n",
    "    model.eval() #evaluation mode to turn off the gradients / training. \n",
    "    \n",
    "    loader = DataLoader(test_dataset, batch_size, shuffle = True, num_workers = num_workers)\n",
    "    for ii, data in tqdm(enumerate(loader)):\n",
    "        \n",
    "        brains = data[0]\n",
    "        labels = data[1]\n",
    "        \n",
    "        #move the data to the GPU \n",
    "        brains = brains.to(dev)\n",
    "        labels = labels.to(dev)\n",
    "        \n",
    "        total_images += brains.shape[0] #this would be the same if we used labels or predictions. \n",
    "        #print(total_images)\n",
    "        \n",
    "        predictions, _ = model(brains)\n",
    "        predictions = torch.sigmoid(predictions) \n",
    "        \n",
    "        predictions_numpy = predictions.cpu().detach().numpy()\n",
    "        labels_numpy = labels.cpu().detach().numpy()\n",
    "        for j in range(predictions.shape[0]):\n",
    "            #labels = [False, True] are needed to make sure we don't have errors with the shape of CM\n",
    "            CM = sklearn.metrics.confusion_matrix(labels_numpy[j,0,:,:].ravel(), predictions_numpy[j,0,:,:].ravel() > threshold, labels = [False,True])\n",
    "            try: \n",
    "                CM_values[0] = CM_values[0] + CM[0][0]\n",
    "                CM_values[1] = CM_values[1] + CM[0][1]\n",
    "                CM_values[2] = CM_values[2] + CM[1][0]\n",
    "                CM_values[3] = CM_values[3] + CM[1][1]\n",
    "            except:\n",
    "                print(\"Error in Appending\")\n",
    "                return CM, CM_values\n",
    "            \n",
    "    del loader #delete loader, might be wrong to do this\n",
    "    return np.divide(CM_values , (total_images*(256*256)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train 0 12\n",
      "Phase: train. Epoch: 0. Loss: 0.6250379085540771\n",
      "0 train 1 24\n",
      "Phase: train. Epoch: 0. Loss: 0.6110647916793823\n",
      "0 train 2 36\n",
      "Phase: train. Epoch: 0. Loss: 0.5951761603355408\n",
      "0 train 3 48\n",
      "Phase: train. Epoch: 0. Loss: 0.5900740027427673\n",
      "0 train 4 60\n",
      "Phase: train. Epoch: 0. Loss: 0.5789482593536377\n",
      "0 train 5 72\n",
      "Phase: train. Epoch: 0. Loss: 0.5733581781387329\n",
      "0 train 6 84\n",
      "Phase: train. Epoch: 0. Loss: 0.5606772899627686\n",
      "0 train 7 96\n",
      "Phase: train. Epoch: 0. Loss: 0.5485560894012451\n",
      "0 train 8 108\n",
      "Phase: train. Epoch: 0. Loss: 0.5412015318870544\n",
      "0 train 9 120\n",
      "Phase: train. Epoch: 0. Loss: 0.5280758738517761\n",
      "0 train 10 132\n",
      "Phase: train. Epoch: 0. Loss: 0.5128855109214783\n",
      "0 train 11 144\n",
      "Phase: train. Epoch: 0. Loss: 0.5076863765716553\n",
      "0 train 12 156\n",
      "Phase: train. Epoch: 0. Loss: 0.4938661456108093\n",
      "0 train 13 168\n",
      "Phase: train. Epoch: 0. Loss: 0.48401588201522827\n",
      "0 train 14 180\n",
      "Phase: train. Epoch: 0. Loss: 0.4794387221336365\n",
      "0 train 15 192\n",
      "Phase: train. Epoch: 0. Loss: 0.46219801902770996\n",
      "0 train 16 204\n",
      "Phase: train. Epoch: 0. Loss: 0.45934781432151794\n",
      "0 train 17 216\n",
      "Phase: train. Epoch: 0. Loss: 0.4553930461406708\n",
      "0 train 18 228\n",
      "Phase: train. Epoch: 0. Loss: 0.43318304419517517\n",
      "0 train 19 240\n",
      "Phase: train. Epoch: 0. Loss: 0.42232227325439453\n",
      "0 train 20 252\n",
      "Phase: train. Epoch: 0. Loss: 0.42183321714401245\n",
      "0 train 21 264\n",
      "Phase: train. Epoch: 0. Loss: 0.4113485515117645\n",
      "0 train 22 276\n",
      "Phase: train. Epoch: 0. Loss: 0.40880391001701355\n",
      "0 train 23 288\n",
      "Phase: train. Epoch: 0. Loss: 0.40773171186447144\n",
      "0 train 24 300\n",
      "Phase: train. Epoch: 0. Loss: 0.3930414617061615\n",
      "0 train 25 312\n",
      "Phase: train. Epoch: 0. Loss: 0.3871525526046753\n",
      "0 train 26 324\n",
      "Phase: train. Epoch: 0. Loss: 0.38256001472473145\n",
      "0 train 27 336\n",
      "Phase: train. Epoch: 0. Loss: 0.40184861421585083\n",
      "0 train 28 348\n",
      "Phase: train. Epoch: 0. Loss: 0.3775932192802429\n",
      "0 train 29 360\n",
      "Phase: train. Epoch: 0. Loss: 0.36543214321136475\n",
      "0 train 30 372\n",
      "Phase: train. Epoch: 0. Loss: 0.3737385869026184\n",
      "0 train 31 384\n",
      "Phase: train. Epoch: 0. Loss: 0.35767412185668945\n",
      "0 train 32 396\n",
      "Phase: train. Epoch: 0. Loss: 0.3614426851272583\n",
      "0 train 33 408\n",
      "Phase: train. Epoch: 0. Loss: 0.35249561071395874\n",
      "0 train 34 420\n",
      "Phase: train. Epoch: 0. Loss: 0.3582819402217865\n",
      "0 train 35 432\n",
      "Phase: train. Epoch: 0. Loss: 0.3540987968444824\n",
      "0 train 36 444\n",
      "Phase: train. Epoch: 0. Loss: 0.34535714983940125\n",
      "0 train 37 456\n",
      "Phase: train. Epoch: 0. Loss: 0.3388552665710449\n",
      "0 train 38 468\n",
      "Phase: train. Epoch: 0. Loss: 0.33756542205810547\n",
      "0 train 39 480\n",
      "Phase: train. Epoch: 0. Loss: 0.3351719081401825\n",
      "0 train 40 492\n",
      "Phase: train. Epoch: 0. Loss: 0.332856148481369\n",
      "0 train 41 504\n",
      "Phase: train. Epoch: 0. Loss: 0.3272949159145355\n",
      "0 train 42 516\n",
      "Phase: train. Epoch: 0. Loss: 0.31482499837875366\n",
      "0 train 43 528\n",
      "Phase: train. Epoch: 0. Loss: 0.31925517320632935\n",
      "0 train 44 540\n",
      "Phase: train. Epoch: 0. Loss: 0.32722267508506775\n",
      "0 train 45 552\n",
      "Phase: train. Epoch: 0. Loss: 0.3146078884601593\n",
      "0 train 46 564\n",
      "Phase: train. Epoch: 0. Loss: 0.32209283113479614\n",
      "0 train 47 576\n",
      "Phase: train. Epoch: 0. Loss: 0.31868451833724976\n",
      "0 train 48 588\n",
      "Phase: train. Epoch: 0. Loss: 0.30734992027282715\n",
      "0 train 49 600\n",
      "Phase: train. Epoch: 0. Loss: 0.30238234996795654\n",
      "0 train 50 612\n",
      "Phase: train. Epoch: 0. Loss: 0.31415343284606934\n",
      "0 train 51 624\n",
      "Phase: train. Epoch: 0. Loss: 0.3126545250415802\n",
      "0 train 52 636\n",
      "Phase: train. Epoch: 0. Loss: 0.30192121863365173\n",
      "0 train 53 648\n",
      "Phase: train. Epoch: 0. Loss: 0.30471348762512207\n",
      "0 train 54 660\n",
      "Phase: train. Epoch: 0. Loss: 0.2986472249031067\n",
      "0 train 55 672\n",
      "Phase: train. Epoch: 0. Loss: 0.2872423529624939\n",
      "0 train 56 684\n",
      "Phase: train. Epoch: 0. Loss: 0.2892601490020752\n",
      "0 train 57 696\n",
      "Phase: train. Epoch: 0. Loss: 0.27605336904525757\n",
      "0 train 58 708\n",
      "Phase: train. Epoch: 0. Loss: 0.28994661569595337\n",
      "0 train 59 720\n",
      "Phase: train. Epoch: 0. Loss: 0.2797720432281494\n",
      "0 train 60 732\n",
      "Phase: train. Epoch: 0. Loss: 0.29046082496643066\n",
      "0 train 61 744\n",
      "Phase: train. Epoch: 0. Loss: 0.27811533212661743\n",
      "0 train 62 751\n",
      "Phase: train. Epoch: 0. Loss: 0.2969524562358856\n",
      "0 val 0 763\n",
      "Phase: val. Epoch: 0. Loss: 0.2910517156124115\n",
      "0 val 1 775\n",
      "Phase: val. Epoch: 0. Loss: 0.2614721953868866\n",
      "0 val 2 787\n",
      "Phase: val. Epoch: 0. Loss: 0.31043750047683716\n",
      "0 val 3 799\n",
      "Phase: val. Epoch: 0. Loss: 0.2889252305030823\n",
      "0 val 4 811\n",
      "Phase: val. Epoch: 0. Loss: 0.2820032238960266\n",
      "0 val 5 823\n",
      "Phase: val. Epoch: 0. Loss: 0.2866315543651581\n",
      "0 val 6 835\n",
      "Phase: val. Epoch: 0. Loss: 0.28378283977508545\n",
      "0 val 7 847\n",
      "Phase: val. Epoch: 0. Loss: 0.29446476697921753\n",
      "0 val 8 859\n",
      "Phase: val. Epoch: 0. Loss: 0.2628417909145355\n",
      "0 val 9 871\n",
      "Phase: val. Epoch: 0. Loss: 0.27819499373435974\n",
      "0 val 10 883\n",
      "Phase: val. Epoch: 0. Loss: 0.3061392307281494\n",
      "0 val 11 884\n",
      "Phase: val. Epoch: 0. Loss: 0.35934507846832275\n",
      "1 train 0 12\n",
      "Phase: train. Epoch: 1. Loss: 0.2819698452949524\n",
      "1 train 1 24\n",
      "Phase: train. Epoch: 1. Loss: 0.2710683345794678\n",
      "1 train 2 36\n",
      "Phase: train. Epoch: 1. Loss: 0.28286370635032654\n",
      "1 train 3 48\n",
      "Phase: train. Epoch: 1. Loss: 0.2704860270023346\n",
      "1 train 4 60\n",
      "Phase: train. Epoch: 1. Loss: 0.27923470735549927\n",
      "1 train 5 72\n",
      "Phase: train. Epoch: 1. Loss: 0.2713046669960022\n",
      "1 train 6 84\n",
      "Phase: train. Epoch: 1. Loss: 0.26970431208610535\n",
      "1 train 7 96\n",
      "Phase: train. Epoch: 1. Loss: 0.29386430978775024\n",
      "1 train 8 108\n",
      "Phase: train. Epoch: 1. Loss: 0.2840508222579956\n",
      "1 train 9 120\n",
      "Phase: train. Epoch: 1. Loss: 0.27166032791137695\n",
      "1 train 10 132\n",
      "Phase: train. Epoch: 1. Loss: 0.25348395109176636\n",
      "1 train 11 144\n",
      "Phase: train. Epoch: 1. Loss: 0.25344720482826233\n",
      "1 train 12 156\n",
      "Phase: train. Epoch: 1. Loss: 0.26211807131767273\n",
      "1 train 13 168\n",
      "Phase: train. Epoch: 1. Loss: 0.2624962031841278\n",
      "1 train 14 180\n",
      "Phase: train. Epoch: 1. Loss: 0.2703861892223358\n",
      "1 train 15 192\n",
      "Phase: train. Epoch: 1. Loss: 0.2458411306142807\n",
      "1 train 16 204\n",
      "Phase: train. Epoch: 1. Loss: 0.25776854157447815\n",
      "1 train 17 216\n",
      "Phase: train. Epoch: 1. Loss: 0.2504149079322815\n",
      "1 train 18 228\n",
      "Phase: train. Epoch: 1. Loss: 0.2508152723312378\n",
      "1 train 19 240\n",
      "Phase: train. Epoch: 1. Loss: 0.24749045073986053\n",
      "1 train 20 252\n",
      "Phase: train. Epoch: 1. Loss: 0.259482204914093\n",
      "1 train 21 264\n",
      "Phase: train. Epoch: 1. Loss: 0.23198503255844116\n",
      "1 train 22 276\n",
      "Phase: train. Epoch: 1. Loss: 0.24785730242729187\n",
      "1 train 23 288\n",
      "Phase: train. Epoch: 1. Loss: 0.2500225901603699\n",
      "1 train 24 300\n",
      "Phase: train. Epoch: 1. Loss: 0.26014217734336853\n",
      "1 train 25 312\n",
      "Phase: train. Epoch: 1. Loss: 0.23945209383964539\n",
      "1 train 26 324\n",
      "Phase: train. Epoch: 1. Loss: 0.2503420114517212\n",
      "1 train 27 336\n",
      "Phase: train. Epoch: 1. Loss: 0.23680369555950165\n",
      "1 train 28 348\n",
      "Phase: train. Epoch: 1. Loss: 0.2613705098628998\n",
      "1 train 29 360\n",
      "Phase: train. Epoch: 1. Loss: 0.25167226791381836\n",
      "1 train 30 372\n",
      "Phase: train. Epoch: 1. Loss: 0.23614370822906494\n",
      "1 train 31 384\n",
      "Phase: train. Epoch: 1. Loss: 0.2190515398979187\n",
      "1 train 32 396\n",
      "Phase: train. Epoch: 1. Loss: 0.2418241947889328\n",
      "1 train 33 408\n",
      "Phase: train. Epoch: 1. Loss: 0.23915861546993256\n",
      "1 train 34 420\n",
      "Phase: train. Epoch: 1. Loss: 0.2416144460439682\n",
      "1 train 35 432\n",
      "Phase: train. Epoch: 1. Loss: 0.24145150184631348\n",
      "1 train 36 444\n",
      "Phase: train. Epoch: 1. Loss: 0.2403026819229126\n",
      "1 train 37 456\n",
      "Phase: train. Epoch: 1. Loss: 0.2258377969264984\n",
      "1 train 38 468\n",
      "Phase: train. Epoch: 1. Loss: 0.2374352514743805\n",
      "1 train 39 480\n",
      "Phase: train. Epoch: 1. Loss: 0.23036590218544006\n",
      "1 train 40 492\n",
      "Phase: train. Epoch: 1. Loss: 0.22051987051963806\n",
      "1 train 41 504\n",
      "Phase: train. Epoch: 1. Loss: 0.2279617041349411\n",
      "1 train 42 516\n",
      "Phase: train. Epoch: 1. Loss: 0.23697249591350555\n",
      "1 train 43 528\n",
      "Phase: train. Epoch: 1. Loss: 0.23033766448497772\n",
      "1 train 44 540\n",
      "Phase: train. Epoch: 1. Loss: 0.21919159591197968\n",
      "1 train 45 552\n",
      "Phase: train. Epoch: 1. Loss: 0.22282877564430237\n",
      "1 train 46 564\n",
      "Phase: train. Epoch: 1. Loss: 0.20677396655082703\n",
      "1 train 47 576\n",
      "Phase: train. Epoch: 1. Loss: 0.23922589421272278\n",
      "1 train 48 588\n",
      "Phase: train. Epoch: 1. Loss: 0.1989375799894333\n",
      "1 train 49 600\n",
      "Phase: train. Epoch: 1. Loss: 0.22961346805095673\n",
      "1 train 50 612\n",
      "Phase: train. Epoch: 1. Loss: 0.20683777332305908\n",
      "1 train 51 624\n",
      "Phase: train. Epoch: 1. Loss: 0.20715205371379852\n",
      "1 train 52 636\n",
      "Phase: train. Epoch: 1. Loss: 0.22350716590881348\n",
      "1 train 53 648\n",
      "Phase: train. Epoch: 1. Loss: 0.21219170093536377\n",
      "1 train 54 660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 1. Loss: 0.21892988681793213\n",
      "1 train 55 672\n",
      "Phase: train. Epoch: 1. Loss: 0.20699678361415863\n",
      "1 train 56 684\n",
      "Phase: train. Epoch: 1. Loss: 0.22551020979881287\n",
      "1 train 57 696\n",
      "Phase: train. Epoch: 1. Loss: 0.1998796910047531\n",
      "1 train 58 708\n",
      "Phase: train. Epoch: 1. Loss: 0.20827381312847137\n",
      "1 train 59 720\n",
      "Phase: train. Epoch: 1. Loss: 0.2165856808423996\n",
      "1 train 60 732\n",
      "Phase: train. Epoch: 1. Loss: 0.20399639010429382\n",
      "1 train 61 744\n",
      "Phase: train. Epoch: 1. Loss: 0.20331081748008728\n",
      "1 train 62 751\n",
      "Phase: train. Epoch: 1. Loss: 0.20443664491176605\n",
      "1 val 0 763\n",
      "Phase: val. Epoch: 1. Loss: 0.21381554007530212\n",
      "1 val 1 775\n",
      "Phase: val. Epoch: 1. Loss: 0.21084623038768768\n",
      "1 val 2 787\n",
      "Phase: val. Epoch: 1. Loss: 0.193674236536026\n",
      "1 val 3 799\n",
      "Phase: val. Epoch: 1. Loss: 0.20598113536834717\n",
      "1 val 4 811\n",
      "Phase: val. Epoch: 1. Loss: 0.2049371600151062\n",
      "1 val 5 823\n",
      "Phase: val. Epoch: 1. Loss: 0.22174833714962006\n",
      "1 val 6 835\n",
      "Phase: val. Epoch: 1. Loss: 0.21890076994895935\n",
      "1 val 7 847\n",
      "Phase: val. Epoch: 1. Loss: 0.19898563623428345\n",
      "1 val 8 859\n",
      "Phase: val. Epoch: 1. Loss: 0.21304947137832642\n",
      "1 val 9 871\n",
      "Phase: val. Epoch: 1. Loss: 0.2217504233121872\n",
      "1 val 10 883\n",
      "Phase: val. Epoch: 1. Loss: 0.25105154514312744\n",
      "1 val 11 884\n",
      "Phase: val. Epoch: 1. Loss: 0.27223169803619385\n",
      "2 train 0 12\n",
      "Phase: train. Epoch: 2. Loss: 0.19352738559246063\n",
      "2 train 1 24\n",
      "Phase: train. Epoch: 2. Loss: 0.2166520208120346\n",
      "2 train 2 36\n",
      "Phase: train. Epoch: 2. Loss: 0.2299528568983078\n",
      "2 train 3 48\n",
      "Phase: train. Epoch: 2. Loss: 0.2065124213695526\n",
      "2 train 4 60\n",
      "Phase: train. Epoch: 2. Loss: 0.20923714339733124\n",
      "2 train 5 72\n",
      "Phase: train. Epoch: 2. Loss: 0.19525593519210815\n",
      "2 train 6 84\n",
      "Phase: train. Epoch: 2. Loss: 0.1952907145023346\n",
      "2 train 7 96\n",
      "Phase: train. Epoch: 2. Loss: 0.2099364995956421\n",
      "2 train 8 108\n",
      "Phase: train. Epoch: 2. Loss: 0.19050344824790955\n",
      "2 train 9 120\n",
      "Phase: train. Epoch: 2. Loss: 0.1987321376800537\n",
      "2 train 10 132\n",
      "Phase: train. Epoch: 2. Loss: 0.19917194545269012\n",
      "2 train 11 144\n",
      "Phase: train. Epoch: 2. Loss: 0.2270665466785431\n",
      "2 train 12 156\n",
      "Phase: train. Epoch: 2. Loss: 0.201149582862854\n",
      "2 train 13 168\n",
      "Phase: train. Epoch: 2. Loss: 0.20192909240722656\n",
      "2 train 14 180\n",
      "Phase: train. Epoch: 2. Loss: 0.2148354947566986\n",
      "2 train 15 192\n",
      "Phase: train. Epoch: 2. Loss: 0.20149479806423187\n",
      "2 train 16 204\n",
      "Phase: train. Epoch: 2. Loss: 0.1999584138393402\n",
      "2 train 17 216\n",
      "Phase: train. Epoch: 2. Loss: 0.2064555585384369\n",
      "2 train 18 228\n",
      "Phase: train. Epoch: 2. Loss: 0.2054252326488495\n",
      "2 train 19 240\n",
      "Phase: train. Epoch: 2. Loss: 0.19196584820747375\n",
      "2 train 20 252\n",
      "Phase: train. Epoch: 2. Loss: 0.1916312426328659\n",
      "2 train 21 264\n",
      "Phase: train. Epoch: 2. Loss: 0.20204715430736542\n",
      "2 train 22 276\n",
      "Phase: train. Epoch: 2. Loss: 0.19725143909454346\n",
      "2 train 23 288\n",
      "Phase: train. Epoch: 2. Loss: 0.20225009322166443\n",
      "2 train 24 300\n",
      "Phase: train. Epoch: 2. Loss: 0.1842992752790451\n",
      "2 train 25 312\n",
      "Phase: train. Epoch: 2. Loss: 0.21928708255290985\n",
      "2 train 26 324\n",
      "Phase: train. Epoch: 2. Loss: 0.200015589594841\n",
      "2 train 27 336\n",
      "Phase: train. Epoch: 2. Loss: 0.1889837235212326\n",
      "2 train 28 348\n",
      "Phase: train. Epoch: 2. Loss: 0.19125056266784668\n",
      "2 train 29 360\n",
      "Phase: train. Epoch: 2. Loss: 0.18344566226005554\n",
      "2 train 30 372\n",
      "Phase: train. Epoch: 2. Loss: 0.20795473456382751\n",
      "2 train 31 384\n",
      "Phase: train. Epoch: 2. Loss: 0.17457446455955505\n",
      "2 train 32 396\n",
      "Phase: train. Epoch: 2. Loss: 0.18182286620140076\n",
      "2 train 33 408\n",
      "Phase: train. Epoch: 2. Loss: 0.18934795260429382\n",
      "2 train 34 420\n",
      "Phase: train. Epoch: 2. Loss: 0.1941452920436859\n",
      "2 train 35 432\n",
      "Phase: train. Epoch: 2. Loss: 0.19061541557312012\n",
      "2 train 36 444\n",
      "Phase: train. Epoch: 2. Loss: 0.1951296031475067\n",
      "2 train 37 456\n",
      "Phase: train. Epoch: 2. Loss: 0.17093250155448914\n",
      "2 train 38 468\n",
      "Phase: train. Epoch: 2. Loss: 0.2002992331981659\n",
      "2 train 39 480\n",
      "Phase: train. Epoch: 2. Loss: 0.19874930381774902\n",
      "2 train 40 492\n",
      "Phase: train. Epoch: 2. Loss: 0.1778988540172577\n",
      "2 train 41 504\n",
      "Phase: train. Epoch: 2. Loss: 0.1846706122159958\n",
      "2 train 42 516\n",
      "Phase: train. Epoch: 2. Loss: 0.1815568506717682\n",
      "2 train 43 528\n",
      "Phase: train. Epoch: 2. Loss: 0.17864462733268738\n",
      "2 train 44 540\n",
      "Phase: train. Epoch: 2. Loss: 0.19780728220939636\n",
      "2 train 45 552\n",
      "Phase: train. Epoch: 2. Loss: 0.17147061228752136\n",
      "2 train 46 564\n",
      "Phase: train. Epoch: 2. Loss: 0.20374178886413574\n",
      "2 train 47 576\n",
      "Phase: train. Epoch: 2. Loss: 0.18006911873817444\n",
      "2 train 48 588\n",
      "Phase: train. Epoch: 2. Loss: 0.19369952380657196\n",
      "2 train 49 600\n",
      "Phase: train. Epoch: 2. Loss: 0.18404826521873474\n",
      "2 train 50 612\n",
      "Phase: train. Epoch: 2. Loss: 0.19275479018688202\n",
      "2 train 51 624\n",
      "Phase: train. Epoch: 2. Loss: 0.1715984344482422\n",
      "2 train 52 636\n",
      "Phase: train. Epoch: 2. Loss: 0.18962106108665466\n",
      "2 train 53 648\n",
      "Phase: train. Epoch: 2. Loss: 0.188192218542099\n",
      "2 train 54 660\n",
      "Phase: train. Epoch: 2. Loss: 0.19209489226341248\n",
      "2 train 55 672\n",
      "Phase: train. Epoch: 2. Loss: 0.19451956450939178\n",
      "2 train 56 684\n",
      "Phase: train. Epoch: 2. Loss: 0.1887395828962326\n",
      "2 train 57 696\n",
      "Phase: train. Epoch: 2. Loss: 0.17559108138084412\n",
      "2 train 58 708\n",
      "Phase: train. Epoch: 2. Loss: 0.1729629635810852\n",
      "2 train 59 720\n",
      "Phase: train. Epoch: 2. Loss: 0.17210452258586884\n",
      "2 train 60 732\n",
      "Phase: train. Epoch: 2. Loss: 0.16499052941799164\n",
      "2 train 61 744\n",
      "Phase: train. Epoch: 2. Loss: 0.17801880836486816\n",
      "2 train 62 751\n",
      "Phase: train. Epoch: 2. Loss: 0.18660883605480194\n",
      "2 val 0 763\n",
      "Phase: val. Epoch: 2. Loss: 0.17343521118164062\n",
      "2 val 1 775\n",
      "Phase: val. Epoch: 2. Loss: 0.20264552533626556\n",
      "2 val 2 787\n",
      "Phase: val. Epoch: 2. Loss: 0.16369637846946716\n",
      "2 val 3 799\n",
      "Phase: val. Epoch: 2. Loss: 0.18848121166229248\n",
      "2 val 4 811\n",
      "Phase: val. Epoch: 2. Loss: 0.19283726811408997\n",
      "2 val 5 823\n",
      "Phase: val. Epoch: 2. Loss: 0.22911927103996277\n",
      "2 val 6 835\n",
      "Phase: val. Epoch: 2. Loss: 0.18740765750408173\n",
      "2 val 7 847\n",
      "Phase: val. Epoch: 2. Loss: 0.15447711944580078\n",
      "2 val 8 859\n",
      "Phase: val. Epoch: 2. Loss: 0.17740845680236816\n",
      "2 val 9 871\n",
      "Phase: val. Epoch: 2. Loss: 0.16873197257518768\n",
      "2 val 10 883\n",
      "Phase: val. Epoch: 2. Loss: 0.16098414361476898\n",
      "2 val 11 884\n",
      "Phase: val. Epoch: 2. Loss: 0.17244580388069153\n",
      "3 train 0 12\n",
      "Phase: train. Epoch: 3. Loss: 0.15699732303619385\n",
      "3 train 1 24\n",
      "Phase: train. Epoch: 3. Loss: 0.16640286147594452\n",
      "3 train 2 36\n",
      "Phase: train. Epoch: 3. Loss: 0.17729899287223816\n",
      "3 train 3 48\n",
      "Phase: train. Epoch: 3. Loss: 0.18504993617534637\n",
      "3 train 4 60\n",
      "Phase: train. Epoch: 3. Loss: 0.18677374720573425\n",
      "3 train 5 72\n",
      "Phase: train. Epoch: 3. Loss: 0.16247370839118958\n",
      "3 train 6 84\n",
      "Phase: train. Epoch: 3. Loss: 0.18275193870067596\n",
      "3 train 7 96\n",
      "Phase: train. Epoch: 3. Loss: 0.1857571303844452\n",
      "3 train 8 108\n",
      "Phase: train. Epoch: 3. Loss: 0.1731986701488495\n",
      "3 train 9 120\n",
      "Phase: train. Epoch: 3. Loss: 0.16622453927993774\n",
      "3 train 10 132\n",
      "Phase: train. Epoch: 3. Loss: 0.17416688799858093\n",
      "3 train 11 144\n",
      "Phase: train. Epoch: 3. Loss: 0.19389721751213074\n",
      "3 train 12 156\n",
      "Phase: train. Epoch: 3. Loss: 0.17387789487838745\n",
      "3 train 13 168\n",
      "Phase: train. Epoch: 3. Loss: 0.18015019595623016\n",
      "3 train 14 180\n",
      "Phase: train. Epoch: 3. Loss: 0.19955691695213318\n",
      "3 train 15 192\n",
      "Phase: train. Epoch: 3. Loss: 0.1690131425857544\n",
      "3 train 16 204\n",
      "Phase: train. Epoch: 3. Loss: 0.16886620223522186\n",
      "3 train 17 216\n",
      "Phase: train. Epoch: 3. Loss: 0.16819855570793152\n",
      "3 train 18 228\n",
      "Phase: train. Epoch: 3. Loss: 0.17312732338905334\n",
      "3 train 19 240\n",
      "Phase: train. Epoch: 3. Loss: 0.17252883315086365\n",
      "3 train 20 252\n",
      "Phase: train. Epoch: 3. Loss: 0.18168042600154877\n",
      "3 train 21 264\n",
      "Phase: train. Epoch: 3. Loss: 0.1930953860282898\n",
      "3 train 22 276\n",
      "Phase: train. Epoch: 3. Loss: 0.1666342169046402\n",
      "3 train 23 288\n",
      "Phase: train. Epoch: 3. Loss: 0.17987647652626038\n",
      "3 train 24 300\n",
      "Phase: train. Epoch: 3. Loss: 0.17437201738357544\n",
      "3 train 25 312\n",
      "Phase: train. Epoch: 3. Loss: 0.16481377184391022\n",
      "3 train 26 324\n",
      "Phase: train. Epoch: 3. Loss: 0.17974552512168884\n",
      "3 train 27 336\n",
      "Phase: train. Epoch: 3. Loss: 0.17052923142910004\n",
      "3 train 28 348\n",
      "Phase: train. Epoch: 3. Loss: 0.17849338054656982\n",
      "3 train 29 360\n",
      "Phase: train. Epoch: 3. Loss: 0.15250888466835022\n",
      "3 train 30 372\n",
      "Phase: train. Epoch: 3. Loss: 0.17234985530376434\n",
      "3 train 31 384\n",
      "Phase: train. Epoch: 3. Loss: 0.15300726890563965\n",
      "3 train 32 396\n",
      "Phase: train. Epoch: 3. Loss: 0.14973017573356628\n",
      "3 train 33 408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 3. Loss: 0.15381458401679993\n",
      "3 train 34 420\n",
      "Phase: train. Epoch: 3. Loss: 0.16497547924518585\n",
      "3 train 35 432\n",
      "Phase: train. Epoch: 3. Loss: 0.20159554481506348\n",
      "3 train 36 444\n",
      "Phase: train. Epoch: 3. Loss: 0.15112578868865967\n",
      "3 train 37 456\n",
      "Phase: train. Epoch: 3. Loss: 0.16895195841789246\n",
      "3 train 38 468\n",
      "Phase: train. Epoch: 3. Loss: 0.16134345531463623\n",
      "3 train 39 480\n",
      "Phase: train. Epoch: 3. Loss: 0.18698863685131073\n",
      "3 train 40 492\n",
      "Phase: train. Epoch: 3. Loss: 0.1781396120786667\n",
      "3 train 41 504\n",
      "Phase: train. Epoch: 3. Loss: 0.17898428440093994\n",
      "3 train 42 516\n",
      "Phase: train. Epoch: 3. Loss: 0.16110122203826904\n",
      "3 train 43 528\n",
      "Phase: train. Epoch: 3. Loss: 0.1677739918231964\n",
      "3 train 44 540\n",
      "Phase: train. Epoch: 3. Loss: 0.1609068512916565\n",
      "3 train 45 552\n",
      "Phase: train. Epoch: 3. Loss: 0.16354387998580933\n",
      "3 train 46 564\n",
      "Phase: train. Epoch: 3. Loss: 0.15058687329292297\n",
      "3 train 47 576\n",
      "Phase: train. Epoch: 3. Loss: 0.15032994747161865\n",
      "3 train 48 588\n",
      "Phase: train. Epoch: 3. Loss: 0.15159645676612854\n",
      "3 train 49 600\n",
      "Phase: train. Epoch: 3. Loss: 0.15457305312156677\n",
      "3 train 50 612\n",
      "Phase: train. Epoch: 3. Loss: 0.14850091934204102\n",
      "3 train 51 624\n",
      "Phase: train. Epoch: 3. Loss: 0.15399861335754395\n",
      "3 train 52 636\n",
      "Phase: train. Epoch: 3. Loss: 0.13856953382492065\n",
      "3 train 53 648\n",
      "Phase: train. Epoch: 3. Loss: 0.1618468016386032\n",
      "3 train 54 660\n",
      "Phase: train. Epoch: 3. Loss: 0.16496321558952332\n",
      "3 train 55 672\n",
      "Phase: train. Epoch: 3. Loss: 0.15327875316143036\n",
      "3 train 56 684\n",
      "Phase: train. Epoch: 3. Loss: 0.1757262647151947\n",
      "3 train 57 696\n",
      "Phase: train. Epoch: 3. Loss: 0.14278745651245117\n",
      "3 train 58 708\n",
      "Phase: train. Epoch: 3. Loss: 0.15458951890468597\n",
      "3 train 59 720\n",
      "Phase: train. Epoch: 3. Loss: 0.14855021238327026\n",
      "3 train 60 732\n",
      "Phase: train. Epoch: 3. Loss: 0.14586284756660461\n",
      "3 train 61 744\n",
      "Phase: train. Epoch: 3. Loss: 0.19726574420928955\n",
      "3 train 62 751\n",
      "Phase: train. Epoch: 3. Loss: 0.16120216250419617\n",
      "3 val 0 763\n",
      "Phase: val. Epoch: 3. Loss: 0.15496093034744263\n",
      "3 val 1 775\n",
      "Phase: val. Epoch: 3. Loss: 0.18426458537578583\n",
      "3 val 2 787\n",
      "Phase: val. Epoch: 3. Loss: 0.1685234159231186\n",
      "3 val 3 799\n",
      "Phase: val. Epoch: 3. Loss: 0.13794264197349548\n",
      "3 val 4 811\n",
      "Phase: val. Epoch: 3. Loss: 0.15178070962429047\n",
      "3 val 5 823\n",
      "Phase: val. Epoch: 3. Loss: 0.16194039583206177\n",
      "3 val 6 835\n",
      "Phase: val. Epoch: 3. Loss: 0.15068396925926208\n",
      "3 val 7 847\n",
      "Phase: val. Epoch: 3. Loss: 0.14938919246196747\n",
      "3 val 8 859\n",
      "Phase: val. Epoch: 3. Loss: 0.1408289521932602\n",
      "3 val 9 871\n",
      "Phase: val. Epoch: 3. Loss: 0.1605161726474762\n",
      "3 val 10 883\n",
      "Phase: val. Epoch: 3. Loss: 0.15930044651031494\n",
      "3 val 11 884\n",
      "Phase: val. Epoch: 3. Loss: 0.13989196717739105\n",
      "4 train 0 12\n",
      "Phase: train. Epoch: 4. Loss: 0.15919920802116394\n",
      "4 train 1 24\n",
      "Phase: train. Epoch: 4. Loss: 0.1571277678012848\n",
      "4 train 2 36\n",
      "Phase: train. Epoch: 4. Loss: 0.15887194871902466\n",
      "4 train 3 48\n",
      "Phase: train. Epoch: 4. Loss: 0.1531752645969391\n",
      "4 train 4 60\n",
      "Phase: train. Epoch: 4. Loss: 0.15611258149147034\n",
      "4 train 5 72\n",
      "Phase: train. Epoch: 4. Loss: 0.1669175922870636\n",
      "4 train 6 84\n",
      "Phase: train. Epoch: 4. Loss: 0.14054416120052338\n",
      "4 train 7 96\n",
      "Phase: train. Epoch: 4. Loss: 0.140356183052063\n",
      "4 train 8 108\n",
      "Phase: train. Epoch: 4. Loss: 0.1381324976682663\n",
      "4 train 9 120\n",
      "Phase: train. Epoch: 4. Loss: 0.1460980623960495\n",
      "4 train 10 132\n",
      "Phase: train. Epoch: 4. Loss: 0.16680394113063812\n",
      "4 train 11 144\n",
      "Phase: train. Epoch: 4. Loss: 0.16625714302062988\n",
      "4 train 12 156\n",
      "Phase: train. Epoch: 4. Loss: 0.17606225609779358\n",
      "4 train 13 168\n",
      "Phase: train. Epoch: 4. Loss: 0.19227313995361328\n",
      "4 train 14 180\n",
      "Phase: train. Epoch: 4. Loss: 0.15469619631767273\n",
      "4 train 15 192\n",
      "Phase: train. Epoch: 4. Loss: 0.14516080915927887\n",
      "4 train 16 204\n",
      "Phase: train. Epoch: 4. Loss: 0.13369467854499817\n",
      "4 train 17 216\n",
      "Phase: train. Epoch: 4. Loss: 0.15622904896736145\n",
      "4 train 18 228\n",
      "Phase: train. Epoch: 4. Loss: 0.14016002416610718\n",
      "4 train 19 240\n",
      "Phase: train. Epoch: 4. Loss: 0.13100525736808777\n",
      "4 train 20 252\n",
      "Phase: train. Epoch: 4. Loss: 0.14693747460842133\n",
      "4 train 21 264\n",
      "Phase: train. Epoch: 4. Loss: 0.16438639163970947\n",
      "4 train 22 276\n",
      "Phase: train. Epoch: 4. Loss: 0.15579324960708618\n",
      "4 train 23 288\n",
      "Phase: train. Epoch: 4. Loss: 0.16991226375102997\n",
      "4 train 24 300\n",
      "Phase: train. Epoch: 4. Loss: 0.16954146325588226\n",
      "4 train 25 312\n",
      "Phase: train. Epoch: 4. Loss: 0.14050129055976868\n",
      "4 train 26 324\n",
      "Phase: train. Epoch: 4. Loss: 0.12935253977775574\n",
      "4 train 27 336\n",
      "Phase: train. Epoch: 4. Loss: 0.15562456846237183\n",
      "4 train 28 348\n",
      "Phase: train. Epoch: 4. Loss: 0.1578962355852127\n",
      "4 train 29 360\n",
      "Phase: train. Epoch: 4. Loss: 0.16357159614562988\n",
      "4 train 30 372\n",
      "Phase: train. Epoch: 4. Loss: 0.16019105911254883\n",
      "4 train 31 384\n",
      "Phase: train. Epoch: 4. Loss: 0.1488288938999176\n",
      "4 train 32 396\n",
      "Phase: train. Epoch: 4. Loss: 0.1583155393600464\n",
      "4 train 33 408\n",
      "Phase: train. Epoch: 4. Loss: 0.15781575441360474\n",
      "4 train 34 420\n",
      "Phase: train. Epoch: 4. Loss: 0.14968089759349823\n",
      "4 train 35 432\n",
      "Phase: train. Epoch: 4. Loss: 0.16824877262115479\n",
      "4 train 36 444\n",
      "Phase: train. Epoch: 4. Loss: 0.1375272125005722\n",
      "4 train 37 456\n",
      "Phase: train. Epoch: 4. Loss: 0.14579880237579346\n",
      "4 train 38 468\n",
      "Phase: train. Epoch: 4. Loss: 0.1402401328086853\n",
      "4 train 39 480\n",
      "Phase: train. Epoch: 4. Loss: 0.15029454231262207\n",
      "4 train 40 492\n",
      "Phase: train. Epoch: 4. Loss: 0.1645379364490509\n",
      "4 train 41 504\n",
      "Phase: train. Epoch: 4. Loss: 0.12450802326202393\n",
      "4 train 42 516\n",
      "Phase: train. Epoch: 4. Loss: 0.1435023993253708\n",
      "4 train 43 528\n",
      "Phase: train. Epoch: 4. Loss: 0.15724551677703857\n",
      "4 train 44 540\n",
      "Phase: train. Epoch: 4. Loss: 0.17054323852062225\n",
      "4 train 45 552\n",
      "Phase: train. Epoch: 4. Loss: 0.1329973042011261\n",
      "4 train 46 564\n",
      "Phase: train. Epoch: 4. Loss: 0.13092637062072754\n",
      "4 train 47 576\n",
      "Phase: train. Epoch: 4. Loss: 0.15384656190872192\n",
      "4 train 48 588\n",
      "Phase: train. Epoch: 4. Loss: 0.13693833351135254\n",
      "4 train 49 600\n",
      "Phase: train. Epoch: 4. Loss: 0.15427850186824799\n",
      "4 train 50 612\n",
      "Phase: train. Epoch: 4. Loss: 0.13775189220905304\n",
      "4 train 51 624\n",
      "Phase: train. Epoch: 4. Loss: 0.15294718742370605\n",
      "4 train 52 636\n",
      "Phase: train. Epoch: 4. Loss: 0.12358172982931137\n",
      "4 train 53 648\n",
      "Phase: train. Epoch: 4. Loss: 0.13300931453704834\n",
      "4 train 54 660\n",
      "Phase: train. Epoch: 4. Loss: 0.1760902851819992\n",
      "4 train 55 672\n",
      "Phase: train. Epoch: 4. Loss: 0.16632553935050964\n",
      "4 train 56 684\n",
      "Phase: train. Epoch: 4. Loss: 0.15405988693237305\n",
      "4 train 57 696\n",
      "Phase: train. Epoch: 4. Loss: 0.13160452246665955\n",
      "4 train 58 708\n",
      "Phase: train. Epoch: 4. Loss: 0.1355874389410019\n",
      "4 train 59 720\n",
      "Phase: train. Epoch: 4. Loss: 0.1404951810836792\n",
      "4 train 60 732\n",
      "Phase: train. Epoch: 4. Loss: 0.14823850989341736\n",
      "4 train 61 744\n",
      "Phase: train. Epoch: 4. Loss: 0.14347516000270844\n",
      "4 train 62 751\n",
      "Phase: train. Epoch: 4. Loss: 0.13084837794303894\n",
      "4 val 0 763\n",
      "Phase: val. Epoch: 4. Loss: 0.13635550439357758\n",
      "4 val 1 775\n",
      "Phase: val. Epoch: 4. Loss: 0.16242897510528564\n",
      "4 val 2 787\n",
      "Phase: val. Epoch: 4. Loss: 0.14389155805110931\n",
      "4 val 3 799\n",
      "Phase: val. Epoch: 4. Loss: 0.13644206523895264\n",
      "4 val 4 811\n",
      "Phase: val. Epoch: 4. Loss: 0.13364267349243164\n",
      "4 val 5 823\n",
      "Phase: val. Epoch: 4. Loss: 0.15768903493881226\n",
      "4 val 6 835\n",
      "Phase: val. Epoch: 4. Loss: 0.12739138305187225\n",
      "4 val 7 847\n",
      "Phase: val. Epoch: 4. Loss: 0.15458635985851288\n",
      "4 val 8 859\n",
      "Phase: val. Epoch: 4. Loss: 0.16308115422725677\n",
      "4 val 9 871\n",
      "Phase: val. Epoch: 4. Loss: 0.1423366516828537\n",
      "4 val 10 883\n",
      "Phase: val. Epoch: 4. Loss: 0.16252069175243378\n",
      "4 val 11 884\n",
      "Phase: val. Epoch: 4. Loss: 0.1600068360567093\n",
      "5 train 0 12\n",
      "Phase: train. Epoch: 5. Loss: 0.14588582515716553\n",
      "5 train 1 24\n",
      "Phase: train. Epoch: 5. Loss: 0.14898574352264404\n",
      "5 train 2 36\n",
      "Phase: train. Epoch: 5. Loss: 0.12400704622268677\n",
      "5 train 3 48\n",
      "Phase: train. Epoch: 5. Loss: 0.13342642784118652\n",
      "5 train 4 60\n",
      "Phase: train. Epoch: 5. Loss: 0.1588922142982483\n",
      "5 train 5 72\n",
      "Phase: train. Epoch: 5. Loss: 0.1475086510181427\n",
      "5 train 6 84\n",
      "Phase: train. Epoch: 5. Loss: 0.13400296866893768\n",
      "5 train 7 96\n",
      "Phase: train. Epoch: 5. Loss: 0.1261201947927475\n",
      "5 train 8 108\n",
      "Phase: train. Epoch: 5. Loss: 0.15434034168720245\n",
      "5 train 9 120\n",
      "Phase: train. Epoch: 5. Loss: 0.12500011920928955\n",
      "5 train 10 132\n",
      "Phase: train. Epoch: 5. Loss: 0.1384125053882599\n",
      "5 train 11 144\n",
      "Phase: train. Epoch: 5. Loss: 0.13147328794002533\n",
      "5 train 12 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 5. Loss: 0.18852321803569794\n",
      "5 train 13 168\n",
      "Phase: train. Epoch: 5. Loss: 0.14849211275577545\n",
      "5 train 14 180\n",
      "Phase: train. Epoch: 5. Loss: 0.1454683244228363\n",
      "5 train 15 192\n",
      "Phase: train. Epoch: 5. Loss: 0.12135422229766846\n",
      "5 train 16 204\n",
      "Phase: train. Epoch: 5. Loss: 0.1437118649482727\n",
      "5 train 17 216\n",
      "Phase: train. Epoch: 5. Loss: 0.13613013923168182\n",
      "5 train 18 228\n",
      "Phase: train. Epoch: 5. Loss: 0.16070708632469177\n",
      "5 train 19 240\n",
      "Phase: train. Epoch: 5. Loss: 0.13741159439086914\n",
      "5 train 20 252\n",
      "Phase: train. Epoch: 5. Loss: 0.1360650658607483\n",
      "5 train 21 264\n",
      "Phase: train. Epoch: 5. Loss: 0.12611286342144012\n",
      "5 train 22 276\n",
      "Phase: train. Epoch: 5. Loss: 0.1505991816520691\n",
      "5 train 23 288\n",
      "Phase: train. Epoch: 5. Loss: 0.16699472069740295\n",
      "5 train 24 300\n",
      "Phase: train. Epoch: 5. Loss: 0.1353302299976349\n",
      "5 train 25 312\n",
      "Phase: train. Epoch: 5. Loss: 0.13061532378196716\n",
      "5 train 26 324\n",
      "Phase: train. Epoch: 5. Loss: 0.11559176445007324\n",
      "5 train 27 336\n",
      "Phase: train. Epoch: 5. Loss: 0.12063375115394592\n",
      "5 train 28 348\n",
      "Phase: train. Epoch: 5. Loss: 0.16716627776622772\n",
      "5 train 29 360\n",
      "Phase: train. Epoch: 5. Loss: 0.14900843799114227\n",
      "5 train 30 372\n",
      "Phase: train. Epoch: 5. Loss: 0.13486270606517792\n",
      "5 train 31 384\n",
      "Phase: train. Epoch: 5. Loss: 0.12446281313896179\n",
      "5 train 32 396\n",
      "Phase: train. Epoch: 5. Loss: 0.149057075381279\n",
      "5 train 33 408\n",
      "Phase: train. Epoch: 5. Loss: 0.1255631297826767\n",
      "5 train 34 420\n",
      "Phase: train. Epoch: 5. Loss: 0.12139929831027985\n",
      "5 train 35 432\n",
      "Phase: train. Epoch: 5. Loss: 0.15701350569725037\n",
      "5 train 36 444\n",
      "Phase: train. Epoch: 5. Loss: 0.1184006929397583\n",
      "5 train 37 456\n",
      "Phase: train. Epoch: 5. Loss: 0.11014065146446228\n",
      "5 train 38 468\n",
      "Phase: train. Epoch: 5. Loss: 0.13289515674114227\n",
      "5 train 39 480\n",
      "Phase: train. Epoch: 5. Loss: 0.14999893307685852\n",
      "5 train 40 492\n",
      "Phase: train. Epoch: 5. Loss: 0.12387518584728241\n",
      "5 train 41 504\n",
      "Phase: train. Epoch: 5. Loss: 0.16037897765636444\n",
      "5 train 42 516\n",
      "Phase: train. Epoch: 5. Loss: 0.14830836653709412\n",
      "5 train 43 528\n",
      "Phase: train. Epoch: 5. Loss: 0.13363638520240784\n",
      "5 train 44 540\n",
      "Phase: train. Epoch: 5. Loss: 0.12910252809524536\n",
      "5 train 45 552\n",
      "Phase: train. Epoch: 5. Loss: 0.14176857471466064\n",
      "5 train 46 564\n",
      "Phase: train. Epoch: 5. Loss: 0.11429168283939362\n",
      "5 train 47 576\n",
      "Phase: train. Epoch: 5. Loss: 0.11530846357345581\n",
      "5 train 48 588\n",
      "Phase: train. Epoch: 5. Loss: 0.12244238704442978\n",
      "5 train 49 600\n",
      "Phase: train. Epoch: 5. Loss: 0.1382114291191101\n",
      "5 train 50 612\n",
      "Phase: train. Epoch: 5. Loss: 0.1067994087934494\n",
      "5 train 51 624\n",
      "Phase: train. Epoch: 5. Loss: 0.12175454944372177\n",
      "5 train 52 636\n",
      "Phase: train. Epoch: 5. Loss: 0.15181267261505127\n",
      "5 train 53 648\n",
      "Phase: train. Epoch: 5. Loss: 0.11722910404205322\n",
      "5 train 54 660\n",
      "Phase: train. Epoch: 5. Loss: 0.1391712874174118\n",
      "5 train 55 672\n",
      "Phase: train. Epoch: 5. Loss: 0.12698253989219666\n",
      "5 train 56 684\n",
      "Phase: train. Epoch: 5. Loss: 0.1475011110305786\n",
      "5 train 57 696\n",
      "Phase: train. Epoch: 5. Loss: 0.12768569588661194\n",
      "5 train 58 708\n",
      "Phase: train. Epoch: 5. Loss: 0.12187837809324265\n",
      "5 train 59 720\n",
      "Phase: train. Epoch: 5. Loss: 0.15091146528720856\n",
      "5 train 60 732\n",
      "Phase: train. Epoch: 5. Loss: 0.13151861727237701\n",
      "5 train 61 744\n",
      "Phase: train. Epoch: 5. Loss: 0.1253841519355774\n",
      "5 train 62 751\n",
      "Phase: train. Epoch: 5. Loss: 0.1330133080482483\n",
      "5 val 0 763\n",
      "Phase: val. Epoch: 5. Loss: 0.12825247645378113\n",
      "5 val 1 775\n",
      "Phase: val. Epoch: 5. Loss: 0.13336807489395142\n",
      "5 val 2 787\n",
      "Phase: val. Epoch: 5. Loss: 0.09654589742422104\n",
      "5 val 3 799\n",
      "Phase: val. Epoch: 5. Loss: 0.14203986525535583\n",
      "5 val 4 811\n",
      "Phase: val. Epoch: 5. Loss: 0.10146421194076538\n",
      "5 val 5 823\n",
      "Phase: val. Epoch: 5. Loss: 0.14810097217559814\n",
      "5 val 6 835\n",
      "Phase: val. Epoch: 5. Loss: 0.14724135398864746\n",
      "5 val 7 847\n",
      "Phase: val. Epoch: 5. Loss: 0.10818346589803696\n",
      "5 val 8 859\n",
      "Phase: val. Epoch: 5. Loss: 0.1440267413854599\n",
      "5 val 9 871\n",
      "Phase: val. Epoch: 5. Loss: 0.137015700340271\n",
      "5 val 10 883\n",
      "Phase: val. Epoch: 5. Loss: 0.12146779149770737\n",
      "5 val 11 884\n",
      "Phase: val. Epoch: 5. Loss: 0.11331051588058472\n",
      "6 train 0 12\n",
      "Phase: train. Epoch: 6. Loss: 0.13223254680633545\n",
      "6 train 1 24\n",
      "Phase: train. Epoch: 6. Loss: 0.1276993751525879\n",
      "6 train 2 36\n",
      "Phase: train. Epoch: 6. Loss: 0.1487722247838974\n",
      "6 train 3 48\n",
      "Phase: train. Epoch: 6. Loss: 0.12752807140350342\n",
      "6 train 4 60\n",
      "Phase: train. Epoch: 6. Loss: 0.11370798200368881\n",
      "6 train 5 72\n",
      "Phase: train. Epoch: 6. Loss: 0.11350390315055847\n",
      "6 train 6 84\n",
      "Phase: train. Epoch: 6. Loss: 0.13039763271808624\n",
      "6 train 7 96\n",
      "Phase: train. Epoch: 6. Loss: 0.13925954699516296\n",
      "6 train 8 108\n",
      "Phase: train. Epoch: 6. Loss: 0.11221438646316528\n",
      "6 train 9 120\n",
      "Phase: train. Epoch: 6. Loss: 0.11509333550930023\n",
      "6 train 10 132\n",
      "Phase: train. Epoch: 6. Loss: 0.1343129575252533\n",
      "6 train 11 144\n",
      "Phase: train. Epoch: 6. Loss: 0.13467061519622803\n",
      "6 train 12 156\n",
      "Phase: train. Epoch: 6. Loss: 0.11065205186605453\n",
      "6 train 13 168\n",
      "Phase: train. Epoch: 6. Loss: 0.12515437602996826\n",
      "6 train 14 180\n",
      "Phase: train. Epoch: 6. Loss: 0.13580942153930664\n",
      "6 train 15 192\n",
      "Phase: train. Epoch: 6. Loss: 0.11745692789554596\n",
      "6 train 16 204\n",
      "Phase: train. Epoch: 6. Loss: 0.12118233740329742\n",
      "6 train 17 216\n",
      "Phase: train. Epoch: 6. Loss: 0.12831619381904602\n",
      "6 train 18 228\n",
      "Phase: train. Epoch: 6. Loss: 0.1291048377752304\n",
      "6 train 19 240\n",
      "Phase: train. Epoch: 6. Loss: 0.15078683197498322\n",
      "6 train 20 252\n",
      "Phase: train. Epoch: 6. Loss: 0.14933733642101288\n",
      "6 train 21 264\n",
      "Phase: train. Epoch: 6. Loss: 0.13761596381664276\n",
      "6 train 22 276\n",
      "Phase: train. Epoch: 6. Loss: 0.14737677574157715\n",
      "6 train 23 288\n",
      "Phase: train. Epoch: 6. Loss: 0.12796643376350403\n",
      "6 train 24 300\n",
      "Phase: train. Epoch: 6. Loss: 0.14139054715633392\n",
      "6 train 25 312\n",
      "Phase: train. Epoch: 6. Loss: 0.10558882355690002\n",
      "6 train 26 324\n",
      "Phase: train. Epoch: 6. Loss: 0.11138248443603516\n",
      "6 train 27 336\n",
      "Phase: train. Epoch: 6. Loss: 0.11079120635986328\n",
      "6 train 28 348\n",
      "Phase: train. Epoch: 6. Loss: 0.13881815969944\n",
      "6 train 29 360\n",
      "Phase: train. Epoch: 6. Loss: 0.09917901456356049\n",
      "6 train 30 372\n",
      "Phase: train. Epoch: 6. Loss: 0.10798119008541107\n",
      "6 train 31 384\n",
      "Phase: train. Epoch: 6. Loss: 0.13133955001831055\n",
      "6 train 32 396\n",
      "Phase: train. Epoch: 6. Loss: 0.10706572234630585\n",
      "6 train 33 408\n",
      "Phase: train. Epoch: 6. Loss: 0.10477060079574585\n",
      "6 train 34 420\n",
      "Phase: train. Epoch: 6. Loss: 0.11694708466529846\n",
      "6 train 35 432\n",
      "Phase: train. Epoch: 6. Loss: 0.1137554869055748\n",
      "6 train 36 444\n",
      "Phase: train. Epoch: 6. Loss: 0.15122760832309723\n",
      "6 train 37 456\n",
      "Phase: train. Epoch: 6. Loss: 0.11822444200515747\n",
      "6 train 38 468\n",
      "Phase: train. Epoch: 6. Loss: 0.15831886231899261\n",
      "6 train 39 480\n",
      "Phase: train. Epoch: 6. Loss: 0.13294053077697754\n",
      "6 train 40 492\n",
      "Phase: train. Epoch: 6. Loss: 0.11761771142482758\n",
      "6 train 41 504\n",
      "Phase: train. Epoch: 6. Loss: 0.11192735284566879\n",
      "6 train 42 516\n",
      "Phase: train. Epoch: 6. Loss: 0.10925276577472687\n",
      "6 train 43 528\n",
      "Phase: train. Epoch: 6. Loss: 0.10292156785726547\n",
      "6 train 44 540\n",
      "Phase: train. Epoch: 6. Loss: 0.11369802802801132\n",
      "6 train 45 552\n",
      "Phase: train. Epoch: 6. Loss: 0.11171919107437134\n",
      "6 train 46 564\n",
      "Phase: train. Epoch: 6. Loss: 0.13205572962760925\n",
      "6 train 47 576\n",
      "Phase: train. Epoch: 6. Loss: 0.1199578195810318\n",
      "6 train 48 588\n",
      "Phase: train. Epoch: 6. Loss: 0.11867822706699371\n",
      "6 train 49 600\n",
      "Phase: train. Epoch: 6. Loss: 0.11244885623455048\n",
      "6 train 50 612\n",
      "Phase: train. Epoch: 6. Loss: 0.11339598894119263\n",
      "6 train 51 624\n",
      "Phase: train. Epoch: 6. Loss: 0.11861908435821533\n",
      "6 train 52 636\n",
      "Phase: train. Epoch: 6. Loss: 0.1339351385831833\n",
      "6 train 53 648\n",
      "Phase: train. Epoch: 6. Loss: 0.12625765800476074\n",
      "6 train 54 660\n",
      "Phase: train. Epoch: 6. Loss: 0.12324269860982895\n",
      "6 train 55 672\n",
      "Phase: train. Epoch: 6. Loss: 0.18391820788383484\n",
      "6 train 56 684\n",
      "Phase: train. Epoch: 6. Loss: 0.1286265105009079\n",
      "6 train 57 696\n",
      "Phase: train. Epoch: 6. Loss: 0.16088813543319702\n",
      "6 train 58 708\n",
      "Phase: train. Epoch: 6. Loss: 0.12539906799793243\n",
      "6 train 59 720\n",
      "Phase: train. Epoch: 6. Loss: 0.11373313516378403\n",
      "6 train 60 732\n",
      "Phase: train. Epoch: 6. Loss: 0.13864755630493164\n",
      "6 train 61 744\n",
      "Phase: train. Epoch: 6. Loss: 0.1373068392276764\n",
      "6 train 62 751\n",
      "Phase: train. Epoch: 6. Loss: 0.13674421608448029\n",
      "6 val 0 763\n",
      "Phase: val. Epoch: 6. Loss: 0.14690165221691132\n",
      "6 val 1 775\n",
      "Phase: val. Epoch: 6. Loss: 0.11446893215179443\n",
      "6 val 2 787\n",
      "Phase: val. Epoch: 6. Loss: 0.12780168652534485\n",
      "6 val 3 799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 6. Loss: 0.10449719429016113\n",
      "6 val 4 811\n",
      "Phase: val. Epoch: 6. Loss: 0.10815578699111938\n",
      "6 val 5 823\n",
      "Phase: val. Epoch: 6. Loss: 0.10588155686855316\n",
      "6 val 6 835\n",
      "Phase: val. Epoch: 6. Loss: 0.1228455901145935\n",
      "6 val 7 847\n",
      "Phase: val. Epoch: 6. Loss: 0.12695449590682983\n",
      "6 val 8 859\n",
      "Phase: val. Epoch: 6. Loss: 0.15566658973693848\n",
      "6 val 9 871\n",
      "Phase: val. Epoch: 6. Loss: 0.14526045322418213\n",
      "6 val 10 883\n",
      "Phase: val. Epoch: 6. Loss: 0.1327233612537384\n",
      "6 val 11 884\n",
      "Phase: val. Epoch: 6. Loss: 0.05823555588722229\n",
      "7 train 0 12\n",
      "Phase: train. Epoch: 7. Loss: 0.11677964776754379\n",
      "7 train 1 24\n",
      "Phase: train. Epoch: 7. Loss: 0.1450834721326828\n",
      "7 train 2 36\n",
      "Phase: train. Epoch: 7. Loss: 0.13148805499076843\n",
      "7 train 3 48\n",
      "Phase: train. Epoch: 7. Loss: 0.1284981667995453\n",
      "7 train 4 60\n",
      "Phase: train. Epoch: 7. Loss: 0.11752074211835861\n",
      "7 train 5 72\n",
      "Phase: train. Epoch: 7. Loss: 0.1227760910987854\n",
      "7 train 6 84\n",
      "Phase: train. Epoch: 7. Loss: 0.13153402507305145\n",
      "7 train 7 96\n",
      "Phase: train. Epoch: 7. Loss: 0.14088074862957\n",
      "7 train 8 108\n",
      "Phase: train. Epoch: 7. Loss: 0.14256227016448975\n",
      "7 train 9 120\n",
      "Phase: train. Epoch: 7. Loss: 0.10465727746486664\n",
      "7 train 10 132\n",
      "Phase: train. Epoch: 7. Loss: 0.11941859126091003\n",
      "7 train 11 144\n",
      "Phase: train. Epoch: 7. Loss: 0.11384443938732147\n",
      "7 train 12 156\n",
      "Phase: train. Epoch: 7. Loss: 0.10190614312887192\n",
      "7 train 13 168\n",
      "Phase: train. Epoch: 7. Loss: 0.1289897859096527\n",
      "7 train 14 180\n",
      "Phase: train. Epoch: 7. Loss: 0.13631872832775116\n",
      "7 train 15 192\n",
      "Phase: train. Epoch: 7. Loss: 0.12286343425512314\n",
      "7 train 16 204\n",
      "Phase: train. Epoch: 7. Loss: 0.09747018665075302\n",
      "7 train 17 216\n",
      "Phase: train. Epoch: 7. Loss: 0.09680746495723724\n",
      "7 train 18 228\n",
      "Phase: train. Epoch: 7. Loss: 0.1294858306646347\n",
      "7 train 19 240\n",
      "Phase: train. Epoch: 7. Loss: 0.12481270730495453\n",
      "7 train 20 252\n",
      "Phase: train. Epoch: 7. Loss: 0.10122880339622498\n",
      "7 train 21 264\n",
      "Phase: train. Epoch: 7. Loss: 0.138816699385643\n",
      "7 train 22 276\n",
      "Phase: train. Epoch: 7. Loss: 0.09462978690862656\n",
      "7 train 23 288\n",
      "Phase: train. Epoch: 7. Loss: 0.13873973488807678\n",
      "7 train 24 300\n",
      "Phase: train. Epoch: 7. Loss: 0.12496107816696167\n",
      "7 train 25 312\n",
      "Phase: train. Epoch: 7. Loss: 0.1263720840215683\n",
      "7 train 26 324\n",
      "Phase: train. Epoch: 7. Loss: 0.09530284255743027\n",
      "7 train 27 336\n",
      "Phase: train. Epoch: 7. Loss: 0.10332410037517548\n",
      "7 train 28 348\n",
      "Phase: train. Epoch: 7. Loss: 0.11199955642223358\n",
      "7 train 29 360\n",
      "Phase: train. Epoch: 7. Loss: 0.13528096675872803\n",
      "7 train 30 372\n",
      "Phase: train. Epoch: 7. Loss: 0.11261087656021118\n",
      "7 train 31 384\n",
      "Phase: train. Epoch: 7. Loss: 0.10076367110013962\n",
      "7 train 32 396\n",
      "Phase: train. Epoch: 7. Loss: 0.11804129928350449\n",
      "7 train 33 408\n",
      "Phase: train. Epoch: 7. Loss: 0.11921417713165283\n",
      "7 train 34 420\n",
      "Phase: train. Epoch: 7. Loss: 0.1483062505722046\n",
      "7 train 35 432\n",
      "Phase: train. Epoch: 7. Loss: 0.11108218878507614\n",
      "7 train 36 444\n",
      "Phase: train. Epoch: 7. Loss: 0.10180719196796417\n",
      "7 train 37 456\n",
      "Phase: train. Epoch: 7. Loss: 0.11419446766376495\n",
      "7 train 38 468\n",
      "Phase: train. Epoch: 7. Loss: 0.10354864597320557\n",
      "7 train 39 480\n",
      "Phase: train. Epoch: 7. Loss: 0.10210680961608887\n",
      "7 train 40 492\n",
      "Phase: train. Epoch: 7. Loss: 0.09998796880245209\n",
      "7 train 41 504\n",
      "Phase: train. Epoch: 7. Loss: 0.12527640163898468\n",
      "7 train 42 516\n",
      "Phase: train. Epoch: 7. Loss: 0.10150951147079468\n",
      "7 train 43 528\n",
      "Phase: train. Epoch: 7. Loss: 0.10411632061004639\n",
      "7 train 44 540\n",
      "Phase: train. Epoch: 7. Loss: 0.10435673594474792\n",
      "7 train 45 552\n",
      "Phase: train. Epoch: 7. Loss: 0.11380096524953842\n",
      "7 train 46 564\n",
      "Phase: train. Epoch: 7. Loss: 0.1398535966873169\n",
      "7 train 47 576\n",
      "Phase: train. Epoch: 7. Loss: 0.11488952487707138\n",
      "7 train 48 588\n",
      "Phase: train. Epoch: 7. Loss: 0.12039964646100998\n",
      "7 train 49 600\n",
      "Phase: train. Epoch: 7. Loss: 0.14276355504989624\n",
      "7 train 50 612\n",
      "Phase: train. Epoch: 7. Loss: 0.1249847412109375\n",
      "7 train 51 624\n",
      "Phase: train. Epoch: 7. Loss: 0.12056565284729004\n",
      "7 train 52 636\n",
      "Phase: train. Epoch: 7. Loss: 0.11577217280864716\n",
      "7 train 53 648\n",
      "Phase: train. Epoch: 7. Loss: 0.11579953879117966\n",
      "7 train 54 660\n",
      "Phase: train. Epoch: 7. Loss: 0.11193453520536423\n",
      "7 train 55 672\n",
      "Phase: train. Epoch: 7. Loss: 0.1319645643234253\n",
      "7 train 56 684\n",
      "Phase: train. Epoch: 7. Loss: 0.11266925930976868\n",
      "7 train 57 696\n",
      "Phase: train. Epoch: 7. Loss: 0.12199016660451889\n",
      "7 train 58 708\n",
      "Phase: train. Epoch: 7. Loss: 0.10855798423290253\n",
      "7 train 59 720\n",
      "Phase: train. Epoch: 7. Loss: 0.12028166651725769\n",
      "7 train 60 732\n",
      "Phase: train. Epoch: 7. Loss: 0.104929119348526\n",
      "7 train 61 744\n",
      "Phase: train. Epoch: 7. Loss: 0.10948272794485092\n",
      "7 train 62 751\n",
      "Phase: train. Epoch: 7. Loss: 0.09014885127544403\n",
      "7 val 0 763\n",
      "Phase: val. Epoch: 7. Loss: 0.08906663954257965\n",
      "7 val 1 775\n",
      "Phase: val. Epoch: 7. Loss: 0.13532057404518127\n",
      "7 val 2 787\n",
      "Phase: val. Epoch: 7. Loss: 0.10223637521266937\n",
      "7 val 3 799\n",
      "Phase: val. Epoch: 7. Loss: 0.12452034652233124\n",
      "7 val 4 811\n",
      "Phase: val. Epoch: 7. Loss: 0.10040228813886642\n",
      "7 val 5 823\n",
      "Phase: val. Epoch: 7. Loss: 0.0980599969625473\n",
      "7 val 6 835\n",
      "Phase: val. Epoch: 7. Loss: 0.09239564836025238\n",
      "7 val 7 847\n",
      "Phase: val. Epoch: 7. Loss: 0.10787749290466309\n",
      "7 val 8 859\n",
      "Phase: val. Epoch: 7. Loss: 0.11076013743877411\n",
      "7 val 9 871\n",
      "Phase: val. Epoch: 7. Loss: 0.1119089424610138\n",
      "7 val 10 883\n",
      "Phase: val. Epoch: 7. Loss: 0.10017480701208115\n",
      "7 val 11 884\n",
      "Phase: val. Epoch: 7. Loss: 0.14278343319892883\n",
      "8 train 0 12\n",
      "Phase: train. Epoch: 8. Loss: 0.11936530470848083\n",
      "8 train 1 24\n",
      "Phase: train. Epoch: 8. Loss: 0.09236781299114227\n",
      "8 train 2 36\n",
      "Phase: train. Epoch: 8. Loss: 0.09865039587020874\n",
      "8 train 3 48\n",
      "Phase: train. Epoch: 8. Loss: 0.1456204503774643\n",
      "8 train 4 60\n",
      "Phase: train. Epoch: 8. Loss: 0.11953612416982651\n",
      "8 train 5 72\n",
      "Phase: train. Epoch: 8. Loss: 0.1308351457118988\n",
      "8 train 6 84\n",
      "Phase: train. Epoch: 8. Loss: 0.14880171418190002\n",
      "8 train 7 96\n",
      "Phase: train. Epoch: 8. Loss: 0.10788973420858383\n",
      "8 train 8 108\n",
      "Phase: train. Epoch: 8. Loss: 0.1101304292678833\n",
      "8 train 9 120\n",
      "Phase: train. Epoch: 8. Loss: 0.08800370991230011\n",
      "8 train 10 132\n",
      "Phase: train. Epoch: 8. Loss: 0.09995843470096588\n",
      "8 train 11 144\n",
      "Phase: train. Epoch: 8. Loss: 0.09582747519016266\n",
      "8 train 12 156\n",
      "Phase: train. Epoch: 8. Loss: 0.10762598365545273\n",
      "8 train 13 168\n",
      "Phase: train. Epoch: 8. Loss: 0.09959938377141953\n",
      "8 train 14 180\n",
      "Phase: train. Epoch: 8. Loss: 0.11370325088500977\n",
      "8 train 15 192\n",
      "Phase: train. Epoch: 8. Loss: 0.11261212825775146\n",
      "8 train 16 204\n",
      "Phase: train. Epoch: 8. Loss: 0.10546104609966278\n",
      "8 train 17 216\n",
      "Phase: train. Epoch: 8. Loss: 0.11253812164068222\n",
      "8 train 18 228\n",
      "Phase: train. Epoch: 8. Loss: 0.11023672670125961\n",
      "8 train 19 240\n",
      "Phase: train. Epoch: 8. Loss: 0.09043608605861664\n",
      "8 train 20 252\n",
      "Phase: train. Epoch: 8. Loss: 0.0983472391963005\n",
      "8 train 21 264\n",
      "Phase: train. Epoch: 8. Loss: 0.13824628293514252\n",
      "8 train 22 276\n",
      "Phase: train. Epoch: 8. Loss: 0.09920438379049301\n",
      "8 train 23 288\n",
      "Phase: train. Epoch: 8. Loss: 0.10614576935768127\n",
      "8 train 24 300\n",
      "Phase: train. Epoch: 8. Loss: 0.11921299993991852\n",
      "8 train 25 312\n",
      "Phase: train. Epoch: 8. Loss: 0.10996150225400925\n",
      "8 train 26 324\n",
      "Phase: train. Epoch: 8. Loss: 0.09885804355144501\n",
      "8 train 27 336\n",
      "Phase: train. Epoch: 8. Loss: 0.10081087052822113\n",
      "8 train 28 348\n",
      "Phase: train. Epoch: 8. Loss: 0.10595858097076416\n",
      "8 train 29 360\n",
      "Phase: train. Epoch: 8. Loss: 0.09133579581975937\n",
      "8 train 30 372\n",
      "Phase: train. Epoch: 8. Loss: 0.11339449137449265\n",
      "8 train 31 384\n",
      "Phase: train. Epoch: 8. Loss: 0.10986459255218506\n",
      "8 train 32 396\n",
      "Phase: train. Epoch: 8. Loss: 0.11826191842556\n",
      "8 train 33 408\n",
      "Phase: train. Epoch: 8. Loss: 0.11261232942342758\n",
      "8 train 34 420\n",
      "Phase: train. Epoch: 8. Loss: 0.14203986525535583\n",
      "8 train 35 432\n",
      "Phase: train. Epoch: 8. Loss: 0.13238629698753357\n",
      "8 train 36 444\n",
      "Phase: train. Epoch: 8. Loss: 0.13521037995815277\n",
      "8 train 37 456\n",
      "Phase: train. Epoch: 8. Loss: 0.10013836622238159\n",
      "8 train 38 468\n",
      "Phase: train. Epoch: 8. Loss: 0.10765057802200317\n",
      "8 train 39 480\n",
      "Phase: train. Epoch: 8. Loss: 0.10632362216711044\n",
      "8 train 40 492\n",
      "Phase: train. Epoch: 8. Loss: 0.1231289952993393\n",
      "8 train 41 504\n",
      "Phase: train. Epoch: 8. Loss: 0.10910099744796753\n",
      "8 train 42 516\n",
      "Phase: train. Epoch: 8. Loss: 0.12328557670116425\n",
      "8 train 43 528\n",
      "Phase: train. Epoch: 8. Loss: 0.10415592044591904\n",
      "8 train 44 540\n",
      "Phase: train. Epoch: 8. Loss: 0.12185584008693695\n",
      "8 train 45 552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 8. Loss: 0.10034734010696411\n",
      "8 train 46 564\n",
      "Phase: train. Epoch: 8. Loss: 0.09197782725095749\n",
      "8 train 47 576\n",
      "Phase: train. Epoch: 8. Loss: 0.108925960958004\n",
      "8 train 48 588\n",
      "Phase: train. Epoch: 8. Loss: 0.08466269820928574\n",
      "8 train 49 600\n",
      "Phase: train. Epoch: 8. Loss: 0.10260337591171265\n",
      "8 train 50 612\n",
      "Phase: train. Epoch: 8. Loss: 0.08972756564617157\n",
      "8 train 51 624\n",
      "Phase: train. Epoch: 8. Loss: 0.08940895646810532\n",
      "8 train 52 636\n",
      "Phase: train. Epoch: 8. Loss: 0.10409371554851532\n",
      "8 train 53 648\n",
      "Phase: train. Epoch: 8. Loss: 0.10548095405101776\n",
      "8 train 54 660\n",
      "Phase: train. Epoch: 8. Loss: 0.0791209265589714\n",
      "8 train 55 672\n",
      "Phase: train. Epoch: 8. Loss: 0.09474962949752808\n",
      "8 train 56 684\n",
      "Phase: train. Epoch: 8. Loss: 0.1156793162226677\n",
      "8 train 57 696\n",
      "Phase: train. Epoch: 8. Loss: 0.13315819203853607\n",
      "8 train 58 708\n",
      "Phase: train. Epoch: 8. Loss: 0.08611683547496796\n",
      "8 train 59 720\n",
      "Phase: train. Epoch: 8. Loss: 0.10056643933057785\n",
      "8 train 60 732\n",
      "Phase: train. Epoch: 8. Loss: 0.1175367683172226\n",
      "8 train 61 744\n",
      "Phase: train. Epoch: 8. Loss: 0.1011529415845871\n",
      "8 train 62 751\n",
      "Phase: train. Epoch: 8. Loss: 0.08872819691896439\n",
      "8 val 0 763\n",
      "Phase: val. Epoch: 8. Loss: 0.13240213692188263\n",
      "8 val 1 775\n",
      "Phase: val. Epoch: 8. Loss: 0.06582416594028473\n",
      "8 val 2 787\n",
      "Phase: val. Epoch: 8. Loss: 0.10557691752910614\n",
      "8 val 3 799\n",
      "Phase: val. Epoch: 8. Loss: 0.10158269107341766\n",
      "8 val 4 811\n",
      "Phase: val. Epoch: 8. Loss: 0.10099097341299057\n",
      "8 val 5 823\n",
      "Phase: val. Epoch: 8. Loss: 0.13703596591949463\n",
      "8 val 6 835\n",
      "Phase: val. Epoch: 8. Loss: 0.08600962162017822\n",
      "8 val 7 847\n",
      "Phase: val. Epoch: 8. Loss: 0.12739908695220947\n",
      "8 val 8 859\n",
      "Phase: val. Epoch: 8. Loss: 0.1151275560259819\n",
      "8 val 9 871\n",
      "Phase: val. Epoch: 8. Loss: 0.09181100130081177\n",
      "8 val 10 883\n",
      "Phase: val. Epoch: 8. Loss: 0.12386151403188705\n",
      "8 val 11 884\n",
      "Phase: val. Epoch: 8. Loss: 0.062761090695858\n",
      "9 train 0 12\n",
      "Phase: train. Epoch: 9. Loss: 0.08243922889232635\n",
      "9 train 1 24\n",
      "Phase: train. Epoch: 9. Loss: 0.11060842126607895\n",
      "9 train 2 36\n",
      "Phase: train. Epoch: 9. Loss: 0.08914443850517273\n",
      "9 train 3 48\n",
      "Phase: train. Epoch: 9. Loss: 0.10011349618434906\n",
      "9 train 4 60\n",
      "Phase: train. Epoch: 9. Loss: 0.11192269623279572\n",
      "9 train 5 72\n",
      "Phase: train. Epoch: 9. Loss: 0.1110563725233078\n",
      "9 train 6 84\n",
      "Phase: train. Epoch: 9. Loss: 0.08029904961585999\n",
      "9 train 7 96\n",
      "Phase: train. Epoch: 9. Loss: 0.09076469391584396\n",
      "9 train 8 108\n",
      "Phase: train. Epoch: 9. Loss: 0.09137909859418869\n",
      "9 train 9 120\n",
      "Phase: train. Epoch: 9. Loss: 0.13045528531074524\n",
      "9 train 10 132\n",
      "Phase: train. Epoch: 9. Loss: 0.09905916452407837\n",
      "9 train 11 144\n",
      "Phase: train. Epoch: 9. Loss: 0.09389197826385498\n",
      "9 train 12 156\n",
      "Phase: train. Epoch: 9. Loss: 0.1142583116889\n",
      "9 train 13 168\n",
      "Phase: train. Epoch: 9. Loss: 0.11953523010015488\n",
      "9 train 14 180\n",
      "Phase: train. Epoch: 9. Loss: 0.0886213630437851\n",
      "9 train 15 192\n",
      "Phase: train. Epoch: 9. Loss: 0.10083343833684921\n",
      "9 train 16 204\n",
      "Phase: train. Epoch: 9. Loss: 0.12423795461654663\n",
      "9 train 17 216\n",
      "Phase: train. Epoch: 9. Loss: 0.11819444596767426\n",
      "9 train 18 228\n",
      "Phase: train. Epoch: 9. Loss: 0.12966956198215485\n",
      "9 train 19 240\n",
      "Phase: train. Epoch: 9. Loss: 0.11576835811138153\n",
      "9 train 20 252\n",
      "Phase: train. Epoch: 9. Loss: 0.09219958633184433\n",
      "9 train 21 264\n",
      "Phase: train. Epoch: 9. Loss: 0.11354538053274155\n",
      "9 train 22 276\n",
      "Phase: train. Epoch: 9. Loss: 0.10171221941709518\n",
      "9 train 23 288\n",
      "Phase: train. Epoch: 9. Loss: 0.10807524621486664\n",
      "9 train 24 300\n",
      "Phase: train. Epoch: 9. Loss: 0.09647064656019211\n",
      "9 train 25 312\n",
      "Phase: train. Epoch: 9. Loss: 0.0907253623008728\n",
      "9 train 26 324\n",
      "Phase: train. Epoch: 9. Loss: 0.10121260583400726\n",
      "9 train 27 336\n",
      "Phase: train. Epoch: 9. Loss: 0.10084136575460434\n",
      "9 train 28 348\n",
      "Phase: train. Epoch: 9. Loss: 0.10180144011974335\n",
      "9 train 29 360\n",
      "Phase: train. Epoch: 9. Loss: 0.1019066721200943\n",
      "9 train 30 372\n",
      "Phase: train. Epoch: 9. Loss: 0.12376352399587631\n",
      "9 train 31 384\n",
      "Phase: train. Epoch: 9. Loss: 0.10664763301610947\n",
      "9 train 32 396\n",
      "Phase: train. Epoch: 9. Loss: 0.10633425414562225\n",
      "9 train 33 408\n",
      "Phase: train. Epoch: 9. Loss: 0.11504898965358734\n",
      "9 train 34 420\n",
      "Phase: train. Epoch: 9. Loss: 0.0916045531630516\n",
      "9 train 35 432\n",
      "Phase: train. Epoch: 9. Loss: 0.09423508495092392\n",
      "9 train 36 444\n",
      "Phase: train. Epoch: 9. Loss: 0.08670272678136826\n",
      "9 train 37 456\n",
      "Phase: train. Epoch: 9. Loss: 0.09348352253437042\n",
      "9 train 38 468\n",
      "Phase: train. Epoch: 9. Loss: 0.12996727228164673\n",
      "9 train 39 480\n",
      "Phase: train. Epoch: 9. Loss: 0.1023513600230217\n",
      "9 train 40 492\n",
      "Phase: train. Epoch: 9. Loss: 0.09345089644193649\n",
      "9 train 41 504\n",
      "Phase: train. Epoch: 9. Loss: 0.09364275634288788\n",
      "9 train 42 516\n",
      "Phase: train. Epoch: 9. Loss: 0.10170774161815643\n",
      "9 train 43 528\n",
      "Phase: train. Epoch: 9. Loss: 0.10651443898677826\n",
      "9 train 44 540\n",
      "Phase: train. Epoch: 9. Loss: 0.10989303886890411\n",
      "9 train 45 552\n",
      "Phase: train. Epoch: 9. Loss: 0.10141117870807648\n",
      "9 train 46 564\n",
      "Phase: train. Epoch: 9. Loss: 0.1116318330168724\n",
      "9 train 47 576\n",
      "Phase: train. Epoch: 9. Loss: 0.09802701324224472\n",
      "9 train 48 588\n",
      "Phase: train. Epoch: 9. Loss: 0.10004584491252899\n",
      "9 train 49 600\n",
      "Phase: train. Epoch: 9. Loss: 0.14218920469284058\n",
      "9 train 50 612\n",
      "Phase: train. Epoch: 9. Loss: 0.08393347263336182\n",
      "9 train 51 624\n",
      "Phase: train. Epoch: 9. Loss: 0.09739828109741211\n",
      "9 train 52 636\n",
      "Phase: train. Epoch: 9. Loss: 0.10128878057003021\n",
      "9 train 53 648\n",
      "Phase: train. Epoch: 9. Loss: 0.13261833786964417\n",
      "9 train 54 660\n",
      "Phase: train. Epoch: 9. Loss: 0.08581037819385529\n",
      "9 train 55 672\n",
      "Phase: train. Epoch: 9. Loss: 0.10132476687431335\n",
      "9 train 56 684\n",
      "Phase: train. Epoch: 9. Loss: 0.08880949020385742\n",
      "9 train 57 696\n",
      "Phase: train. Epoch: 9. Loss: 0.1129671186208725\n",
      "9 train 58 708\n",
      "Phase: train. Epoch: 9. Loss: 0.09505113959312439\n",
      "9 train 59 720\n",
      "Phase: train. Epoch: 9. Loss: 0.11967351287603378\n",
      "9 train 60 732\n",
      "Phase: train. Epoch: 9. Loss: 0.08719369769096375\n",
      "9 train 61 744\n",
      "Phase: train. Epoch: 9. Loss: 0.07324205338954926\n",
      "9 train 62 751\n",
      "Phase: train. Epoch: 9. Loss: 0.10529688745737076\n",
      "9 val 0 763\n",
      "Phase: val. Epoch: 9. Loss: 0.0759553611278534\n",
      "9 val 1 775\n",
      "Phase: val. Epoch: 9. Loss: 0.06721240282058716\n",
      "9 val 2 787\n",
      "Phase: val. Epoch: 9. Loss: 0.09941216558218002\n",
      "9 val 3 799\n",
      "Phase: val. Epoch: 9. Loss: 0.0986347496509552\n",
      "9 val 4 811\n",
      "Phase: val. Epoch: 9. Loss: 0.11829160153865814\n",
      "9 val 5 823\n",
      "Phase: val. Epoch: 9. Loss: 0.09316923469305038\n",
      "9 val 6 835\n",
      "Phase: val. Epoch: 9. Loss: 0.11700472235679626\n",
      "9 val 7 847\n",
      "Phase: val. Epoch: 9. Loss: 0.08968858420848846\n",
      "9 val 8 859\n",
      "Phase: val. Epoch: 9. Loss: 0.09089887142181396\n",
      "9 val 9 871\n",
      "Phase: val. Epoch: 9. Loss: 0.07878460735082626\n",
      "9 val 10 883\n",
      "Phase: val. Epoch: 9. Loss: 0.1067507266998291\n",
      "9 val 11 884\n",
      "Phase: val. Epoch: 9. Loss: 0.07675334066152573\n",
      "10 train 0 12\n",
      "Phase: train. Epoch: 10. Loss: 0.1177121251821518\n",
      "10 train 1 24\n",
      "Phase: train. Epoch: 10. Loss: 0.10898163169622421\n",
      "10 train 2 36\n",
      "Phase: train. Epoch: 10. Loss: 0.08835365623235703\n",
      "10 train 3 48\n",
      "Phase: train. Epoch: 10. Loss: 0.10662321746349335\n",
      "10 train 4 60\n",
      "Phase: train. Epoch: 10. Loss: 0.10452496260404587\n",
      "10 train 5 72\n",
      "Phase: train. Epoch: 10. Loss: 0.10239174962043762\n",
      "10 train 6 84\n",
      "Phase: train. Epoch: 10. Loss: 0.12443425506353378\n",
      "10 train 7 96\n",
      "Phase: train. Epoch: 10. Loss: 0.12786665558815002\n",
      "10 train 8 108\n",
      "Phase: train. Epoch: 10. Loss: 0.09005918353796005\n",
      "10 train 9 120\n",
      "Phase: train. Epoch: 10. Loss: 0.0928758904337883\n",
      "10 train 10 132\n",
      "Phase: train. Epoch: 10. Loss: 0.11901529133319855\n",
      "10 train 11 144\n",
      "Phase: train. Epoch: 10. Loss: 0.14159874618053436\n",
      "10 train 12 156\n",
      "Phase: train. Epoch: 10. Loss: 0.10677336156368256\n",
      "10 train 13 168\n",
      "Phase: train. Epoch: 10. Loss: 0.0852946862578392\n",
      "10 train 14 180\n",
      "Phase: train. Epoch: 10. Loss: 0.09734819829463959\n",
      "10 train 15 192\n",
      "Phase: train. Epoch: 10. Loss: 0.0889153778553009\n",
      "10 train 16 204\n",
      "Phase: train. Epoch: 10. Loss: 0.09908922761678696\n",
      "10 train 17 216\n",
      "Phase: train. Epoch: 10. Loss: 0.11616458743810654\n",
      "10 train 18 228\n",
      "Phase: train. Epoch: 10. Loss: 0.08623254299163818\n",
      "10 train 19 240\n",
      "Phase: train. Epoch: 10. Loss: 0.0796741396188736\n",
      "10 train 20 252\n",
      "Phase: train. Epoch: 10. Loss: 0.08132316172122955\n",
      "10 train 21 264\n",
      "Phase: train. Epoch: 10. Loss: 0.09867014735937119\n",
      "10 train 22 276\n",
      "Phase: train. Epoch: 10. Loss: 0.0958237573504448\n",
      "10 train 23 288\n",
      "Phase: train. Epoch: 10. Loss: 0.11878975480794907\n",
      "10 train 24 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 10. Loss: 0.0946015864610672\n",
      "10 train 25 312\n",
      "Phase: train. Epoch: 10. Loss: 0.0994318351149559\n",
      "10 train 26 324\n",
      "Phase: train. Epoch: 10. Loss: 0.09586784243583679\n",
      "10 train 27 336\n",
      "Phase: train. Epoch: 10. Loss: 0.09830572456121445\n",
      "10 train 28 348\n",
      "Phase: train. Epoch: 10. Loss: 0.08067293465137482\n",
      "10 train 29 360\n",
      "Phase: train. Epoch: 10. Loss: 0.09832149744033813\n",
      "10 train 30 372\n",
      "Phase: train. Epoch: 10. Loss: 0.11838921904563904\n",
      "10 train 31 384\n",
      "Phase: train. Epoch: 10. Loss: 0.07444580644369125\n",
      "10 train 32 396\n",
      "Phase: train. Epoch: 10. Loss: 0.10481129586696625\n",
      "10 train 33 408\n",
      "Phase: train. Epoch: 10. Loss: 0.07632292807102203\n",
      "10 train 34 420\n",
      "Phase: train. Epoch: 10. Loss: 0.08529536426067352\n",
      "10 train 35 432\n",
      "Phase: train. Epoch: 10. Loss: 0.09264084696769714\n",
      "10 train 36 444\n",
      "Phase: train. Epoch: 10. Loss: 0.08996804803609848\n",
      "10 train 37 456\n",
      "Phase: train. Epoch: 10. Loss: 0.07857386767864227\n",
      "10 train 38 468\n",
      "Phase: train. Epoch: 10. Loss: 0.08008944988250732\n",
      "10 train 39 480\n",
      "Phase: train. Epoch: 10. Loss: 0.09734241664409637\n",
      "10 train 40 492\n",
      "Phase: train. Epoch: 10. Loss: 0.09209956228733063\n",
      "10 train 41 504\n",
      "Phase: train. Epoch: 10. Loss: 0.10701566934585571\n",
      "10 train 42 516\n",
      "Phase: train. Epoch: 10. Loss: 0.09360246360301971\n",
      "10 train 43 528\n",
      "Phase: train. Epoch: 10. Loss: 0.09649621695280075\n",
      "10 train 44 540\n",
      "Phase: train. Epoch: 10. Loss: 0.11580365151166916\n",
      "10 train 45 552\n",
      "Phase: train. Epoch: 10. Loss: 0.07747906446456909\n",
      "10 train 46 564\n",
      "Phase: train. Epoch: 10. Loss: 0.07504640519618988\n",
      "10 train 47 576\n",
      "Phase: train. Epoch: 10. Loss: 0.07697848975658417\n",
      "10 train 48 588\n",
      "Phase: train. Epoch: 10. Loss: 0.07552671432495117\n",
      "10 train 49 600\n",
      "Phase: train. Epoch: 10. Loss: 0.12126301229000092\n",
      "10 train 50 612\n",
      "Phase: train. Epoch: 10. Loss: 0.09254397451877594\n",
      "10 train 51 624\n",
      "Phase: train. Epoch: 10. Loss: 0.09264221042394638\n",
      "10 train 52 636\n",
      "Phase: train. Epoch: 10. Loss: 0.09539289772510529\n",
      "10 train 53 648\n",
      "Phase: train. Epoch: 10. Loss: 0.09212228655815125\n",
      "10 train 54 660\n",
      "Phase: train. Epoch: 10. Loss: 0.1556839495897293\n",
      "10 train 55 672\n",
      "Phase: train. Epoch: 10. Loss: 0.09912531077861786\n",
      "10 train 56 684\n",
      "Phase: train. Epoch: 10. Loss: 0.09603451192378998\n",
      "10 train 57 696\n",
      "Phase: train. Epoch: 10. Loss: 0.08753683418035507\n",
      "10 train 58 708\n",
      "Phase: train. Epoch: 10. Loss: 0.10591720044612885\n",
      "10 train 59 720\n",
      "Phase: train. Epoch: 10. Loss: 0.09071969985961914\n",
      "10 train 60 732\n",
      "Phase: train. Epoch: 10. Loss: 0.08989346027374268\n",
      "10 train 61 744\n",
      "Phase: train. Epoch: 10. Loss: 0.09100307524204254\n",
      "10 train 62 751\n",
      "Phase: train. Epoch: 10. Loss: 0.0821518525481224\n",
      "10 val 0 763\n",
      "Phase: val. Epoch: 10. Loss: 0.1274520754814148\n",
      "10 val 1 775\n",
      "Phase: val. Epoch: 10. Loss: 0.06895728409290314\n",
      "10 val 2 787\n",
      "Phase: val. Epoch: 10. Loss: 0.0728311762213707\n",
      "10 val 3 799\n",
      "Phase: val. Epoch: 10. Loss: 0.08487840741872787\n",
      "10 val 4 811\n",
      "Phase: val. Epoch: 10. Loss: 0.05874187499284744\n",
      "10 val 5 823\n",
      "Phase: val. Epoch: 10. Loss: 0.09965936839580536\n",
      "10 val 6 835\n",
      "Phase: val. Epoch: 10. Loss: 0.12474004924297333\n",
      "10 val 7 847\n",
      "Phase: val. Epoch: 10. Loss: 0.10205750167369843\n",
      "10 val 8 859\n",
      "Phase: val. Epoch: 10. Loss: 0.10364364832639694\n",
      "10 val 9 871\n",
      "Phase: val. Epoch: 10. Loss: 0.13683854043483734\n",
      "10 val 10 883\n",
      "Phase: val. Epoch: 10. Loss: 0.13034076988697052\n",
      "10 val 11 884\n",
      "Phase: val. Epoch: 10. Loss: 0.03192632645368576\n",
      "11 train 0 12\n",
      "Phase: train. Epoch: 11. Loss: 0.11253323405981064\n",
      "11 train 1 24\n",
      "Phase: train. Epoch: 11. Loss: 0.0981036052107811\n",
      "11 train 2 36\n",
      "Phase: train. Epoch: 11. Loss: 0.10605008900165558\n",
      "11 train 3 48\n",
      "Phase: train. Epoch: 11. Loss: 0.09903702139854431\n",
      "11 train 4 60\n",
      "Phase: train. Epoch: 11. Loss: 0.08058083057403564\n",
      "11 train 5 72\n",
      "Phase: train. Epoch: 11. Loss: 0.107021264731884\n",
      "11 train 6 84\n",
      "Phase: train. Epoch: 11. Loss: 0.10893221199512482\n",
      "11 train 7 96\n",
      "Phase: train. Epoch: 11. Loss: 0.13560625910758972\n",
      "11 train 8 108\n",
      "Phase: train. Epoch: 11. Loss: 0.10230650007724762\n",
      "11 train 9 120\n",
      "Phase: train. Epoch: 11. Loss: 0.07479327917098999\n",
      "11 train 10 132\n",
      "Phase: train. Epoch: 11. Loss: 0.07635542750358582\n",
      "11 train 11 144\n",
      "Phase: train. Epoch: 11. Loss: 0.09530516713857651\n",
      "11 train 12 156\n",
      "Phase: train. Epoch: 11. Loss: 0.08341493457555771\n",
      "11 train 13 168\n",
      "Phase: train. Epoch: 11. Loss: 0.0995093435049057\n",
      "11 train 14 180\n",
      "Phase: train. Epoch: 11. Loss: 0.07912667840719223\n",
      "11 train 15 192\n",
      "Phase: train. Epoch: 11. Loss: 0.1056542843580246\n",
      "11 train 16 204\n",
      "Phase: train. Epoch: 11. Loss: 0.07254215329885483\n",
      "11 train 17 216\n",
      "Phase: train. Epoch: 11. Loss: 0.08741472661495209\n",
      "11 train 18 228\n",
      "Phase: train. Epoch: 11. Loss: 0.09794698655605316\n",
      "11 train 19 240\n",
      "Phase: train. Epoch: 11. Loss: 0.10583466291427612\n",
      "11 train 20 252\n",
      "Phase: train. Epoch: 11. Loss: 0.08537419140338898\n",
      "11 train 21 264\n",
      "Phase: train. Epoch: 11. Loss: 0.08786380290985107\n",
      "11 train 22 276\n",
      "Phase: train. Epoch: 11. Loss: 0.11971467733383179\n",
      "11 train 23 288\n",
      "Phase: train. Epoch: 11. Loss: 0.1027212142944336\n",
      "11 train 24 300\n",
      "Phase: train. Epoch: 11. Loss: 0.09794239699840546\n",
      "11 train 25 312\n",
      "Phase: train. Epoch: 11. Loss: 0.09996162354946136\n",
      "11 train 26 324\n",
      "Phase: train. Epoch: 11. Loss: 0.09955447912216187\n",
      "11 train 27 336\n",
      "Phase: train. Epoch: 11. Loss: 0.07526606321334839\n",
      "11 train 28 348\n",
      "Phase: train. Epoch: 11. Loss: 0.08893382549285889\n",
      "11 train 29 360\n",
      "Phase: train. Epoch: 11. Loss: 0.07039239257574081\n",
      "11 train 30 372\n",
      "Phase: train. Epoch: 11. Loss: 0.10851699113845825\n",
      "11 train 31 384\n",
      "Phase: train. Epoch: 11. Loss: 0.07142038643360138\n",
      "11 train 32 396\n",
      "Phase: train. Epoch: 11. Loss: 0.1080981120467186\n",
      "11 train 33 408\n",
      "Phase: train. Epoch: 11. Loss: 0.08851595968008041\n",
      "11 train 34 420\n",
      "Phase: train. Epoch: 11. Loss: 0.08875316381454468\n",
      "11 train 35 432\n",
      "Phase: train. Epoch: 11. Loss: 0.11079391837120056\n",
      "11 train 36 444\n",
      "Phase: train. Epoch: 11. Loss: 0.09163989126682281\n",
      "11 train 37 456\n",
      "Phase: train. Epoch: 11. Loss: 0.10125689953565598\n",
      "11 train 38 468\n",
      "Phase: train. Epoch: 11. Loss: 0.11412335932254791\n",
      "11 train 39 480\n",
      "Phase: train. Epoch: 11. Loss: 0.10722440481185913\n",
      "11 train 40 492\n",
      "Phase: train. Epoch: 11. Loss: 0.09576349705457687\n",
      "11 train 41 504\n",
      "Phase: train. Epoch: 11. Loss: 0.11253132671117783\n",
      "11 train 42 516\n",
      "Phase: train. Epoch: 11. Loss: 0.10249774158000946\n",
      "11 train 43 528\n",
      "Phase: train. Epoch: 11. Loss: 0.11887278407812119\n",
      "11 train 44 540\n",
      "Phase: train. Epoch: 11. Loss: 0.07428940385580063\n",
      "11 train 45 552\n",
      "Phase: train. Epoch: 11. Loss: 0.08887626975774765\n",
      "11 train 46 564\n",
      "Phase: train. Epoch: 11. Loss: 0.1068694144487381\n",
      "11 train 47 576\n",
      "Phase: train. Epoch: 11. Loss: 0.08770841360092163\n",
      "11 train 48 588\n",
      "Phase: train. Epoch: 11. Loss: 0.12446729838848114\n",
      "11 train 49 600\n",
      "Phase: train. Epoch: 11. Loss: 0.09797787666320801\n",
      "11 train 50 612\n",
      "Phase: train. Epoch: 11. Loss: 0.07898691296577454\n",
      "11 train 51 624\n",
      "Phase: train. Epoch: 11. Loss: 0.10111483186483383\n",
      "11 train 52 636\n",
      "Phase: train. Epoch: 11. Loss: 0.08712786436080933\n",
      "11 train 53 648\n",
      "Phase: train. Epoch: 11. Loss: 0.08954000473022461\n",
      "11 train 54 660\n",
      "Phase: train. Epoch: 11. Loss: 0.08017455786466599\n",
      "11 train 55 672\n",
      "Phase: train. Epoch: 11. Loss: 0.0921485424041748\n",
      "11 train 56 684\n",
      "Phase: train. Epoch: 11. Loss: 0.1006443127989769\n",
      "11 train 57 696\n",
      "Phase: train. Epoch: 11. Loss: 0.1017838567495346\n",
      "11 train 58 708\n",
      "Phase: train. Epoch: 11. Loss: 0.07440382242202759\n",
      "11 train 59 720\n",
      "Phase: train. Epoch: 11. Loss: 0.08533772081136703\n",
      "11 train 60 732\n",
      "Phase: train. Epoch: 11. Loss: 0.08629828691482544\n",
      "11 train 61 744\n",
      "Phase: train. Epoch: 11. Loss: 0.0914270207285881\n",
      "11 train 62 751\n",
      "Phase: train. Epoch: 11. Loss: 0.0784752145409584\n",
      "11 val 0 763\n",
      "Phase: val. Epoch: 11. Loss: 0.0992765873670578\n",
      "11 val 1 775\n",
      "Phase: val. Epoch: 11. Loss: 0.1053948849439621\n",
      "11 val 2 787\n",
      "Phase: val. Epoch: 11. Loss: 0.09595157951116562\n",
      "11 val 3 799\n",
      "Phase: val. Epoch: 11. Loss: 0.07002480328083038\n",
      "11 val 4 811\n",
      "Phase: val. Epoch: 11. Loss: 0.07258208841085434\n",
      "11 val 5 823\n",
      "Phase: val. Epoch: 11. Loss: 0.07922078669071198\n",
      "11 val 6 835\n",
      "Phase: val. Epoch: 11. Loss: 0.15125206112861633\n",
      "11 val 7 847\n",
      "Phase: val. Epoch: 11. Loss: 0.0735456570982933\n",
      "11 val 8 859\n",
      "Phase: val. Epoch: 11. Loss: 0.04818432033061981\n",
      "11 val 9 871\n",
      "Phase: val. Epoch: 11. Loss: 0.07065882533788681\n",
      "11 val 10 883\n",
      "Phase: val. Epoch: 11. Loss: 0.118211530148983\n",
      "11 val 11 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 11. Loss: 0.0510709211230278\n",
      "12 train 0 12\n",
      "Phase: train. Epoch: 12. Loss: 0.0706530436873436\n",
      "12 train 1 24\n",
      "Phase: train. Epoch: 12. Loss: 0.07077252119779587\n",
      "12 train 2 36\n",
      "Phase: train. Epoch: 12. Loss: 0.08627833425998688\n",
      "12 train 3 48\n",
      "Phase: train. Epoch: 12. Loss: 0.07879377901554108\n",
      "12 train 4 60\n",
      "Phase: train. Epoch: 12. Loss: 0.09174623340368271\n",
      "12 train 5 72\n",
      "Phase: train. Epoch: 12. Loss: 0.09373399615287781\n",
      "12 train 6 84\n",
      "Phase: train. Epoch: 12. Loss: 0.09814491122961044\n",
      "12 train 7 96\n",
      "Phase: train. Epoch: 12. Loss: 0.0889204740524292\n",
      "12 train 8 108\n",
      "Phase: train. Epoch: 12. Loss: 0.09272518008947372\n",
      "12 train 9 120\n",
      "Phase: train. Epoch: 12. Loss: 0.09166025370359421\n",
      "12 train 10 132\n",
      "Phase: train. Epoch: 12. Loss: 0.09704907983541489\n",
      "12 train 11 144\n",
      "Phase: train. Epoch: 12. Loss: 0.08257710933685303\n",
      "12 train 12 156\n",
      "Phase: train. Epoch: 12. Loss: 0.08598970621824265\n",
      "12 train 13 168\n",
      "Phase: train. Epoch: 12. Loss: 0.09630225598812103\n",
      "12 train 14 180\n",
      "Phase: train. Epoch: 12. Loss: 0.07796473801136017\n",
      "12 train 15 192\n",
      "Phase: train. Epoch: 12. Loss: 0.10338398069143295\n",
      "12 train 16 204\n",
      "Phase: train. Epoch: 12. Loss: 0.10580436885356903\n",
      "12 train 17 216\n",
      "Phase: train. Epoch: 12. Loss: 0.07706975936889648\n",
      "12 train 18 228\n",
      "Phase: train. Epoch: 12. Loss: 0.07877607643604279\n",
      "12 train 19 240\n",
      "Phase: train. Epoch: 12. Loss: 0.11251787841320038\n",
      "12 train 20 252\n",
      "Phase: train. Epoch: 12. Loss: 0.0807218849658966\n",
      "12 train 21 264\n",
      "Phase: train. Epoch: 12. Loss: 0.07178810238838196\n",
      "12 train 22 276\n",
      "Phase: train. Epoch: 12. Loss: 0.10080792754888535\n",
      "12 train 23 288\n",
      "Phase: train. Epoch: 12. Loss: 0.10695785284042358\n",
      "12 train 24 300\n",
      "Phase: train. Epoch: 12. Loss: 0.12456848472356796\n",
      "12 train 25 312\n",
      "Phase: train. Epoch: 12. Loss: 0.12631481885910034\n",
      "12 train 26 324\n",
      "Phase: train. Epoch: 12. Loss: 0.07535987347364426\n",
      "12 train 27 336\n",
      "Phase: train. Epoch: 12. Loss: 0.11728173494338989\n",
      "12 train 28 348\n",
      "Phase: train. Epoch: 12. Loss: 0.07856795191764832\n",
      "12 train 29 360\n",
      "Phase: train. Epoch: 12. Loss: 0.07587265968322754\n",
      "12 train 30 372\n",
      "Phase: train. Epoch: 12. Loss: 0.08960073441267014\n",
      "12 train 31 384\n",
      "Phase: train. Epoch: 12. Loss: 0.07901052385568619\n",
      "12 train 32 396\n",
      "Phase: train. Epoch: 12. Loss: 0.09115833044052124\n",
      "12 train 33 408\n",
      "Phase: train. Epoch: 12. Loss: 0.08213189989328384\n",
      "12 train 34 420\n",
      "Phase: train. Epoch: 12. Loss: 0.0951315388083458\n",
      "12 train 35 432\n",
      "Phase: train. Epoch: 12. Loss: 0.07957496494054794\n",
      "12 train 36 444\n",
      "Phase: train. Epoch: 12. Loss: 0.07693277299404144\n",
      "12 train 37 456\n",
      "Phase: train. Epoch: 12. Loss: 0.09546931087970734\n",
      "12 train 38 468\n",
      "Phase: train. Epoch: 12. Loss: 0.08238068222999573\n",
      "12 train 39 480\n",
      "Phase: train. Epoch: 12. Loss: 0.0864410251379013\n",
      "12 train 40 492\n",
      "Phase: train. Epoch: 12. Loss: 0.0960136353969574\n",
      "12 train 41 504\n",
      "Phase: train. Epoch: 12. Loss: 0.11997245252132416\n",
      "12 train 42 516\n",
      "Phase: train. Epoch: 12. Loss: 0.09848448634147644\n",
      "12 train 43 528\n",
      "Phase: train. Epoch: 12. Loss: 0.07344171404838562\n",
      "12 train 44 540\n",
      "Phase: train. Epoch: 12. Loss: 0.125102236866951\n",
      "12 train 45 552\n",
      "Phase: train. Epoch: 12. Loss: 0.09869472682476044\n",
      "12 train 46 564\n",
      "Phase: train. Epoch: 12. Loss: 0.11107753217220306\n",
      "12 train 47 576\n",
      "Phase: train. Epoch: 12. Loss: 0.07611971348524094\n",
      "12 train 48 588\n",
      "Phase: train. Epoch: 12. Loss: 0.1137046068906784\n",
      "12 train 49 600\n",
      "Phase: train. Epoch: 12. Loss: 0.09145327657461166\n",
      "12 train 50 612\n",
      "Phase: train. Epoch: 12. Loss: 0.09911994636058807\n",
      "12 train 51 624\n",
      "Phase: train. Epoch: 12. Loss: 0.10377340018749237\n",
      "12 train 52 636\n",
      "Phase: train. Epoch: 12. Loss: 0.08639442175626755\n",
      "12 train 53 648\n",
      "Phase: train. Epoch: 12. Loss: 0.09991504997015\n",
      "12 train 54 660\n",
      "Phase: train. Epoch: 12. Loss: 0.07976068556308746\n",
      "12 train 55 672\n",
      "Phase: train. Epoch: 12. Loss: 0.08831145614385605\n",
      "12 train 56 684\n",
      "Phase: train. Epoch: 12. Loss: 0.105364590883255\n",
      "12 train 57 696\n",
      "Phase: train. Epoch: 12. Loss: 0.07578060775995255\n",
      "12 train 58 708\n",
      "Phase: train. Epoch: 12. Loss: 0.09172488749027252\n",
      "12 train 59 720\n",
      "Phase: train. Epoch: 12. Loss: 0.08660736680030823\n",
      "12 train 60 732\n",
      "Phase: train. Epoch: 12. Loss: 0.10077526420354843\n",
      "12 train 61 744\n",
      "Phase: train. Epoch: 12. Loss: 0.10464368760585785\n",
      "12 train 62 751\n",
      "Phase: train. Epoch: 12. Loss: 0.08567371219396591\n",
      "12 val 0 763\n",
      "Phase: val. Epoch: 12. Loss: 0.09850244224071503\n",
      "12 val 1 775\n",
      "Phase: val. Epoch: 12. Loss: 0.09545397013425827\n",
      "12 val 2 787\n",
      "Phase: val. Epoch: 12. Loss: 0.102442167699337\n",
      "12 val 3 799\n",
      "Phase: val. Epoch: 12. Loss: 0.09328695386648178\n",
      "12 val 4 811\n",
      "Phase: val. Epoch: 12. Loss: 0.0728011429309845\n",
      "12 val 5 823\n",
      "Phase: val. Epoch: 12. Loss: 0.07418989390134811\n",
      "12 val 6 835\n",
      "Phase: val. Epoch: 12. Loss: 0.11439897865056992\n",
      "12 val 7 847\n",
      "Phase: val. Epoch: 12. Loss: 0.09887081384658813\n",
      "12 val 8 859\n",
      "Phase: val. Epoch: 12. Loss: 0.08330664783716202\n",
      "12 val 9 871\n",
      "Phase: val. Epoch: 12. Loss: 0.07262866199016571\n",
      "12 val 10 883\n",
      "Phase: val. Epoch: 12. Loss: 0.08147108554840088\n",
      "12 val 11 884\n",
      "Phase: val. Epoch: 12. Loss: 0.03230322152376175\n",
      "13 train 0 12\n",
      "Phase: train. Epoch: 13. Loss: 0.11876416206359863\n",
      "13 train 1 24\n",
      "Phase: train. Epoch: 13. Loss: 0.09402233362197876\n",
      "13 train 2 36\n",
      "Phase: train. Epoch: 13. Loss: 0.07821200788021088\n",
      "13 train 3 48\n",
      "Phase: train. Epoch: 13. Loss: 0.08800879120826721\n",
      "13 train 4 60\n",
      "Phase: train. Epoch: 13. Loss: 0.09242729842662811\n",
      "13 train 5 72\n",
      "Phase: train. Epoch: 13. Loss: 0.08006764203310013\n",
      "13 train 6 84\n",
      "Phase: train. Epoch: 13. Loss: 0.07328939437866211\n",
      "13 train 7 96\n",
      "Phase: train. Epoch: 13. Loss: 0.1210426539182663\n",
      "13 train 8 108\n",
      "Phase: train. Epoch: 13. Loss: 0.08356510102748871\n",
      "13 train 9 120\n",
      "Phase: train. Epoch: 13. Loss: 0.09524848312139511\n",
      "13 train 10 132\n",
      "Phase: train. Epoch: 13. Loss: 0.10159403085708618\n",
      "13 train 11 144\n",
      "Phase: train. Epoch: 13. Loss: 0.10793542116880417\n",
      "13 train 12 156\n",
      "Phase: train. Epoch: 13. Loss: 0.0695849359035492\n",
      "13 train 13 168\n",
      "Phase: train. Epoch: 13. Loss: 0.07575566321611404\n",
      "13 train 14 180\n",
      "Phase: train. Epoch: 13. Loss: 0.08319947123527527\n",
      "13 train 15 192\n",
      "Phase: train. Epoch: 13. Loss: 0.0969676822423935\n",
      "13 train 16 204\n",
      "Phase: train. Epoch: 13. Loss: 0.07112947106361389\n",
      "13 train 17 216\n",
      "Phase: train. Epoch: 13. Loss: 0.08309765160083771\n",
      "13 train 18 228\n",
      "Phase: train. Epoch: 13. Loss: 0.143617644906044\n",
      "13 train 19 240\n",
      "Phase: train. Epoch: 13. Loss: 0.08843176066875458\n",
      "13 train 20 252\n",
      "Phase: train. Epoch: 13. Loss: 0.08019384741783142\n",
      "13 train 21 264\n",
      "Phase: train. Epoch: 13. Loss: 0.08213505148887634\n",
      "13 train 22 276\n",
      "Phase: train. Epoch: 13. Loss: 0.10755004733800888\n",
      "13 train 23 288\n",
      "Phase: train. Epoch: 13. Loss: 0.09831591695547104\n",
      "13 train 24 300\n",
      "Phase: train. Epoch: 13. Loss: 0.1002299040555954\n",
      "13 train 25 312\n",
      "Phase: train. Epoch: 13. Loss: 0.07815803587436676\n",
      "13 train 26 324\n",
      "Phase: train. Epoch: 13. Loss: 0.10972757637500763\n",
      "13 train 27 336\n",
      "Phase: train. Epoch: 13. Loss: 0.08927784860134125\n",
      "13 train 28 348\n",
      "Phase: train. Epoch: 13. Loss: 0.10181348770856857\n",
      "13 train 29 360\n",
      "Phase: train. Epoch: 13. Loss: 0.09504272043704987\n",
      "13 train 30 372\n",
      "Phase: train. Epoch: 13. Loss: 0.08698822557926178\n",
      "13 train 31 384\n",
      "Phase: train. Epoch: 13. Loss: 0.13106045126914978\n",
      "13 train 32 396\n",
      "Phase: train. Epoch: 13. Loss: 0.09724104404449463\n",
      "13 train 33 408\n",
      "Phase: train. Epoch: 13. Loss: 0.07722605764865875\n",
      "13 train 34 420\n",
      "Phase: train. Epoch: 13. Loss: 0.07016389071941376\n",
      "13 train 35 432\n",
      "Phase: train. Epoch: 13. Loss: 0.08027039468288422\n",
      "13 train 36 444\n",
      "Phase: train. Epoch: 13. Loss: 0.0927254781126976\n",
      "13 train 37 456\n",
      "Phase: train. Epoch: 13. Loss: 0.06614493578672409\n",
      "13 train 38 468\n",
      "Phase: train. Epoch: 13. Loss: 0.09287788718938828\n",
      "13 train 39 480\n",
      "Phase: train. Epoch: 13. Loss: 0.09704837203025818\n",
      "13 train 40 492\n",
      "Phase: train. Epoch: 13. Loss: 0.08890905231237411\n",
      "13 train 41 504\n",
      "Phase: train. Epoch: 13. Loss: 0.06721381843090057\n",
      "13 train 42 516\n",
      "Phase: train. Epoch: 13. Loss: 0.09583503007888794\n",
      "13 train 43 528\n",
      "Phase: train. Epoch: 13. Loss: 0.08106094598770142\n",
      "13 train 44 540\n",
      "Phase: train. Epoch: 13. Loss: 0.06918543577194214\n",
      "13 train 45 552\n",
      "Phase: train. Epoch: 13. Loss: 0.08305993676185608\n",
      "13 train 46 564\n",
      "Phase: train. Epoch: 13. Loss: 0.09173803776502609\n",
      "13 train 47 576\n",
      "Phase: train. Epoch: 13. Loss: 0.08530713617801666\n",
      "13 train 48 588\n",
      "Phase: train. Epoch: 13. Loss: 0.0718701034784317\n",
      "13 train 49 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 13. Loss: 0.09907182306051254\n",
      "13 train 50 612\n",
      "Phase: train. Epoch: 13. Loss: 0.09542672336101532\n",
      "13 train 51 624\n",
      "Phase: train. Epoch: 13. Loss: 0.09267684072256088\n",
      "13 train 52 636\n",
      "Phase: train. Epoch: 13. Loss: 0.07160232961177826\n",
      "13 train 53 648\n",
      "Phase: train. Epoch: 13. Loss: 0.07894881814718246\n",
      "13 train 54 660\n",
      "Phase: train. Epoch: 13. Loss: 0.05717840790748596\n",
      "13 train 55 672\n",
      "Phase: train. Epoch: 13. Loss: 0.08714759349822998\n",
      "13 train 56 684\n",
      "Phase: train. Epoch: 13. Loss: 0.05988205596804619\n",
      "13 train 57 696\n",
      "Phase: train. Epoch: 13. Loss: 0.06138843297958374\n",
      "13 train 58 708\n",
      "Phase: train. Epoch: 13. Loss: 0.09535382688045502\n",
      "13 train 59 720\n",
      "Phase: train. Epoch: 13. Loss: 0.09770777076482773\n",
      "13 train 60 732\n",
      "Phase: train. Epoch: 13. Loss: 0.08969095349311829\n",
      "13 train 61 744\n",
      "Phase: train. Epoch: 13. Loss: 0.09547704458236694\n",
      "13 train 62 751\n",
      "Phase: train. Epoch: 13. Loss: 0.10963637381792068\n",
      "13 val 0 763\n",
      "Phase: val. Epoch: 13. Loss: 0.07882718741893768\n",
      "13 val 1 775\n",
      "Phase: val. Epoch: 13. Loss: 0.07188799977302551\n",
      "13 val 2 787\n",
      "Phase: val. Epoch: 13. Loss: 0.07677806168794632\n",
      "13 val 3 799\n",
      "Phase: val. Epoch: 13. Loss: 0.0880654975771904\n",
      "13 val 4 811\n",
      "Phase: val. Epoch: 13. Loss: 0.05358865112066269\n",
      "13 val 5 823\n",
      "Phase: val. Epoch: 13. Loss: 0.09079483151435852\n",
      "13 val 6 835\n",
      "Phase: val. Epoch: 13. Loss: 0.04617520421743393\n",
      "13 val 7 847\n",
      "Phase: val. Epoch: 13. Loss: 0.09231562912464142\n",
      "13 val 8 859\n",
      "Phase: val. Epoch: 13. Loss: 0.10495414584875107\n",
      "13 val 9 871\n",
      "Phase: val. Epoch: 13. Loss: 0.0942390188574791\n",
      "13 val 10 883\n",
      "Phase: val. Epoch: 13. Loss: 0.1169208437204361\n",
      "13 val 11 884\n",
      "Phase: val. Epoch: 13. Loss: 0.05029132217168808\n",
      "14 train 0 12\n",
      "Phase: train. Epoch: 14. Loss: 0.10164198279380798\n",
      "14 train 1 24\n",
      "Phase: train. Epoch: 14. Loss: 0.07553239911794662\n",
      "14 train 2 36\n",
      "Phase: train. Epoch: 14. Loss: 0.08424290269613266\n",
      "14 train 3 48\n",
      "Phase: train. Epoch: 14. Loss: 0.09220733493566513\n",
      "14 train 4 60\n",
      "Phase: train. Epoch: 14. Loss: 0.1236879974603653\n",
      "14 train 5 72\n",
      "Phase: train. Epoch: 14. Loss: 0.08734302222728729\n",
      "14 train 6 84\n",
      "Phase: train. Epoch: 14. Loss: 0.08540649712085724\n",
      "14 train 7 96\n",
      "Phase: train. Epoch: 14. Loss: 0.0960354208946228\n",
      "14 train 8 108\n",
      "Phase: train. Epoch: 14. Loss: 0.09069713205099106\n",
      "14 train 9 120\n",
      "Phase: train. Epoch: 14. Loss: 0.09999781847000122\n",
      "14 train 10 132\n",
      "Phase: train. Epoch: 14. Loss: 0.08576437830924988\n",
      "14 train 11 144\n",
      "Phase: train. Epoch: 14. Loss: 0.06914988160133362\n",
      "14 train 12 156\n",
      "Phase: train. Epoch: 14. Loss: 0.12965883314609528\n",
      "14 train 13 168\n",
      "Phase: train. Epoch: 14. Loss: 0.0982147753238678\n",
      "14 train 14 180\n",
      "Phase: train. Epoch: 14. Loss: 0.09628163278102875\n",
      "14 train 15 192\n",
      "Phase: train. Epoch: 14. Loss: 0.06930139660835266\n",
      "14 train 16 204\n",
      "Phase: train. Epoch: 14. Loss: 0.08933045715093613\n",
      "14 train 17 216\n",
      "Phase: train. Epoch: 14. Loss: 0.08558410406112671\n",
      "14 train 18 228\n",
      "Phase: train. Epoch: 14. Loss: 0.09226638078689575\n",
      "14 train 19 240\n",
      "Phase: train. Epoch: 14. Loss: 0.07118003815412521\n",
      "14 train 20 252\n",
      "Phase: train. Epoch: 14. Loss: 0.11238203942775726\n",
      "14 train 21 264\n",
      "Phase: train. Epoch: 14. Loss: 0.09470798075199127\n",
      "14 train 22 276\n",
      "Phase: train. Epoch: 14. Loss: 0.08024459332227707\n",
      "14 train 23 288\n",
      "Phase: train. Epoch: 14. Loss: 0.09498997777700424\n",
      "14 train 24 300\n",
      "Phase: train. Epoch: 14. Loss: 0.08273961395025253\n",
      "14 train 25 312\n",
      "Phase: train. Epoch: 14. Loss: 0.10574537515640259\n",
      "14 train 26 324\n",
      "Phase: train. Epoch: 14. Loss: 0.10154670476913452\n",
      "14 train 27 336\n",
      "Phase: train. Epoch: 14. Loss: 0.07468269765377045\n",
      "14 train 28 348\n",
      "Phase: train. Epoch: 14. Loss: 0.05693953484296799\n",
      "14 train 29 360\n",
      "Phase: train. Epoch: 14. Loss: 0.08399668335914612\n",
      "14 train 30 372\n",
      "Phase: train. Epoch: 14. Loss: 0.09081779420375824\n",
      "14 train 31 384\n",
      "Phase: train. Epoch: 14. Loss: 0.07866939157247543\n",
      "14 train 32 396\n",
      "Phase: train. Epoch: 14. Loss: 0.09110143035650253\n",
      "14 train 33 408\n",
      "Phase: train. Epoch: 14. Loss: 0.08843424171209335\n",
      "14 train 34 420\n",
      "Phase: train. Epoch: 14. Loss: 0.11689567565917969\n",
      "14 train 35 432\n",
      "Phase: train. Epoch: 14. Loss: 0.08746292442083359\n",
      "14 train 36 444\n",
      "Phase: train. Epoch: 14. Loss: 0.10154800862073898\n",
      "14 train 37 456\n",
      "Phase: train. Epoch: 14. Loss: 0.08443211764097214\n",
      "14 train 38 468\n",
      "Phase: train. Epoch: 14. Loss: 0.11137368530035019\n",
      "14 train 39 480\n",
      "Phase: train. Epoch: 14. Loss: 0.08185025304555893\n",
      "14 train 40 492\n",
      "Phase: train. Epoch: 14. Loss: 0.06908612698316574\n",
      "14 train 41 504\n",
      "Phase: train. Epoch: 14. Loss: 0.07344236969947815\n",
      "14 train 42 516\n",
      "Phase: train. Epoch: 14. Loss: 0.099449023604393\n",
      "14 train 43 528\n",
      "Phase: train. Epoch: 14. Loss: 0.0726318210363388\n",
      "14 train 44 540\n",
      "Phase: train. Epoch: 14. Loss: 0.06785555928945541\n",
      "14 train 45 552\n",
      "Phase: train. Epoch: 14. Loss: 0.06199100241065025\n",
      "14 train 46 564\n",
      "Phase: train. Epoch: 14. Loss: 0.06811045855283737\n",
      "14 train 47 576\n",
      "Phase: train. Epoch: 14. Loss: 0.05420375615358353\n",
      "14 train 48 588\n",
      "Phase: train. Epoch: 14. Loss: 0.05507928133010864\n",
      "14 train 49 600\n",
      "Phase: train. Epoch: 14. Loss: 0.07780934125185013\n",
      "14 train 50 612\n",
      "Phase: train. Epoch: 14. Loss: 0.08194570988416672\n",
      "14 train 51 624\n",
      "Phase: train. Epoch: 14. Loss: 0.07450626790523529\n",
      "14 train 52 636\n",
      "Phase: train. Epoch: 14. Loss: 0.08545047044754028\n",
      "14 train 53 648\n",
      "Phase: train. Epoch: 14. Loss: 0.06264961510896683\n",
      "14 train 54 660\n",
      "Phase: train. Epoch: 14. Loss: 0.07693376392126083\n",
      "14 train 55 672\n",
      "Phase: train. Epoch: 14. Loss: 0.11138703674077988\n",
      "14 train 56 684\n",
      "Phase: train. Epoch: 14. Loss: 0.06502941250801086\n",
      "14 train 57 696\n",
      "Phase: train. Epoch: 14. Loss: 0.0823763981461525\n",
      "14 train 58 708\n",
      "Phase: train. Epoch: 14. Loss: 0.06531848013401031\n",
      "14 train 59 720\n",
      "Phase: train. Epoch: 14. Loss: 0.06949581950902939\n",
      "14 train 60 732\n",
      "Phase: train. Epoch: 14. Loss: 0.09700974076986313\n",
      "14 train 61 744\n",
      "Phase: train. Epoch: 14. Loss: 0.0859132930636406\n",
      "14 train 62 751\n",
      "Phase: train. Epoch: 14. Loss: 0.12631164491176605\n",
      "14 val 0 763\n",
      "Phase: val. Epoch: 14. Loss: 0.07328493893146515\n",
      "14 val 1 775\n",
      "Phase: val. Epoch: 14. Loss: 0.1029556542634964\n",
      "14 val 2 787\n",
      "Phase: val. Epoch: 14. Loss: 0.09541629254817963\n",
      "14 val 3 799\n",
      "Phase: val. Epoch: 14. Loss: 0.06328451633453369\n",
      "14 val 4 811\n",
      "Phase: val. Epoch: 14. Loss: 0.07697034627199173\n",
      "14 val 5 823\n",
      "Phase: val. Epoch: 14. Loss: 0.08968474715948105\n",
      "14 val 6 835\n",
      "Phase: val. Epoch: 14. Loss: 0.0842660665512085\n",
      "14 val 7 847\n",
      "Phase: val. Epoch: 14. Loss: 0.06488244235515594\n",
      "14 val 8 859\n",
      "Phase: val. Epoch: 14. Loss: 0.10992912948131561\n",
      "14 val 9 871\n",
      "Phase: val. Epoch: 14. Loss: 0.10174831002950668\n",
      "14 val 10 883\n",
      "Phase: val. Epoch: 14. Loss: 0.0840054377913475\n",
      "14 val 11 884\n",
      "Phase: val. Epoch: 14. Loss: 0.056019533425569534\n",
      "15 train 0 12\n",
      "Phase: train. Epoch: 15. Loss: 0.07792790979146957\n",
      "15 train 1 24\n",
      "Phase: train. Epoch: 15. Loss: 0.10872164368629456\n",
      "15 train 2 36\n",
      "Phase: train. Epoch: 15. Loss: 0.06657490134239197\n",
      "15 train 3 48\n",
      "Phase: train. Epoch: 15. Loss: 0.07944343239068985\n",
      "15 train 4 60\n",
      "Phase: train. Epoch: 15. Loss: 0.0665082335472107\n",
      "15 train 5 72\n",
      "Phase: train. Epoch: 15. Loss: 0.09757870435714722\n",
      "15 train 6 84\n",
      "Phase: train. Epoch: 15. Loss: 0.0878991112112999\n",
      "15 train 7 96\n",
      "Phase: train. Epoch: 15. Loss: 0.10952573269605637\n",
      "15 train 8 108\n",
      "Phase: train. Epoch: 15. Loss: 0.0657820850610733\n",
      "15 train 9 120\n",
      "Phase: train. Epoch: 15. Loss: 0.0714147537946701\n",
      "15 train 10 132\n",
      "Phase: train. Epoch: 15. Loss: 0.07034359872341156\n",
      "15 train 11 144\n",
      "Phase: train. Epoch: 15. Loss: 0.10736002773046494\n",
      "15 train 12 156\n",
      "Phase: train. Epoch: 15. Loss: 0.08893638104200363\n",
      "15 train 13 168\n",
      "Phase: train. Epoch: 15. Loss: 0.0903552994132042\n",
      "15 train 14 180\n",
      "Phase: train. Epoch: 15. Loss: 0.07354997098445892\n",
      "15 train 15 192\n",
      "Phase: train. Epoch: 15. Loss: 0.06997676193714142\n",
      "15 train 16 204\n",
      "Phase: train. Epoch: 15. Loss: 0.0781625509262085\n",
      "15 train 17 216\n",
      "Phase: train. Epoch: 15. Loss: 0.08989264816045761\n",
      "15 train 18 228\n",
      "Phase: train. Epoch: 15. Loss: 0.11911573261022568\n",
      "15 train 19 240\n",
      "Phase: train. Epoch: 15. Loss: 0.07293462753295898\n",
      "15 train 20 252\n",
      "Phase: train. Epoch: 15. Loss: 0.10428955405950546\n",
      "15 train 21 264\n",
      "Phase: train. Epoch: 15. Loss: 0.07534442842006683\n",
      "15 train 22 276\n",
      "Phase: train. Epoch: 15. Loss: 0.06386792659759521\n",
      "15 train 23 288\n",
      "Phase: train. Epoch: 15. Loss: 0.07456539571285248\n",
      "15 train 24 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 15. Loss: 0.0957966074347496\n",
      "15 train 25 312\n",
      "Phase: train. Epoch: 15. Loss: 0.07786507159471512\n",
      "15 train 26 324\n",
      "Phase: train. Epoch: 15. Loss: 0.07908541709184647\n",
      "15 train 27 336\n",
      "Phase: train. Epoch: 15. Loss: 0.06784699857234955\n",
      "15 train 28 348\n",
      "Phase: train. Epoch: 15. Loss: 0.0692693442106247\n",
      "15 train 29 360\n",
      "Phase: train. Epoch: 15. Loss: 0.09516193717718124\n",
      "15 train 30 372\n",
      "Phase: train. Epoch: 15. Loss: 0.10446366667747498\n",
      "15 train 31 384\n",
      "Phase: train. Epoch: 15. Loss: 0.07699649035930634\n",
      "15 train 32 396\n",
      "Phase: train. Epoch: 15. Loss: 0.06594058871269226\n",
      "15 train 33 408\n",
      "Phase: train. Epoch: 15. Loss: 0.06270332634449005\n",
      "15 train 34 420\n",
      "Phase: train. Epoch: 15. Loss: 0.06336662173271179\n",
      "15 train 35 432\n",
      "Phase: train. Epoch: 15. Loss: 0.10717034339904785\n",
      "15 train 36 444\n",
      "Phase: train. Epoch: 15. Loss: 0.09942500293254852\n",
      "15 train 37 456\n",
      "Phase: train. Epoch: 15. Loss: 0.10823886096477509\n",
      "15 train 38 468\n",
      "Phase: train. Epoch: 15. Loss: 0.08441287279129028\n",
      "15 train 39 480\n",
      "Phase: train. Epoch: 15. Loss: 0.0932517871260643\n",
      "15 train 40 492\n",
      "Phase: train. Epoch: 15. Loss: 0.08086943626403809\n",
      "15 train 41 504\n",
      "Phase: train. Epoch: 15. Loss: 0.09511967748403549\n",
      "15 train 42 516\n",
      "Phase: train. Epoch: 15. Loss: 0.11857068538665771\n",
      "15 train 43 528\n",
      "Phase: train. Epoch: 15. Loss: 0.09998971223831177\n",
      "15 train 44 540\n",
      "Phase: train. Epoch: 15. Loss: 0.09609496593475342\n",
      "15 train 45 552\n",
      "Phase: train. Epoch: 15. Loss: 0.08163890242576599\n",
      "15 train 46 564\n",
      "Phase: train. Epoch: 15. Loss: 0.07200604677200317\n",
      "15 train 47 576\n",
      "Phase: train. Epoch: 15. Loss: 0.0766647681593895\n",
      "15 train 48 588\n",
      "Phase: train. Epoch: 15. Loss: 0.08814139664173126\n",
      "15 train 49 600\n",
      "Phase: train. Epoch: 15. Loss: 0.08163650333881378\n",
      "15 train 50 612\n",
      "Phase: train. Epoch: 15. Loss: 0.0682486742734909\n",
      "15 train 51 624\n",
      "Phase: train. Epoch: 15. Loss: 0.06078355014324188\n",
      "15 train 52 636\n",
      "Phase: train. Epoch: 15. Loss: 0.07398027926683426\n",
      "15 train 53 648\n",
      "Phase: train. Epoch: 15. Loss: 0.08778675645589828\n",
      "15 train 54 660\n",
      "Phase: train. Epoch: 15. Loss: 0.08311909437179565\n",
      "15 train 55 672\n",
      "Phase: train. Epoch: 15. Loss: 0.06402904540300369\n",
      "15 train 56 684\n",
      "Phase: train. Epoch: 15. Loss: 0.06631452590227127\n",
      "15 train 57 696\n",
      "Phase: train. Epoch: 15. Loss: 0.061233170330524445\n",
      "15 train 58 708\n",
      "Phase: train. Epoch: 15. Loss: 0.08619476854801178\n",
      "15 train 59 720\n",
      "Phase: train. Epoch: 15. Loss: 0.07556596398353577\n",
      "15 train 60 732\n",
      "Phase: train. Epoch: 15. Loss: 0.09978131949901581\n",
      "15 train 61 744\n",
      "Phase: train. Epoch: 15. Loss: 0.062292974442243576\n",
      "15 train 62 751\n",
      "Phase: train. Epoch: 15. Loss: 0.06441522389650345\n",
      "15 val 0 763\n",
      "Phase: val. Epoch: 15. Loss: 0.09205174446105957\n",
      "15 val 1 775\n",
      "Phase: val. Epoch: 15. Loss: 0.0980539619922638\n",
      "15 val 2 787\n",
      "Phase: val. Epoch: 15. Loss: 0.05778729170560837\n",
      "15 val 3 799\n",
      "Phase: val. Epoch: 15. Loss: 0.06533116102218628\n",
      "15 val 4 811\n",
      "Phase: val. Epoch: 15. Loss: 0.060144051909446716\n",
      "15 val 5 823\n",
      "Phase: val. Epoch: 15. Loss: 0.1004512757062912\n",
      "15 val 6 835\n",
      "Phase: val. Epoch: 15. Loss: 0.0732831060886383\n",
      "15 val 7 847\n",
      "Phase: val. Epoch: 15. Loss: 0.1043454110622406\n",
      "15 val 8 859\n",
      "Phase: val. Epoch: 15. Loss: 0.08566810935735703\n",
      "15 val 9 871\n",
      "Phase: val. Epoch: 15. Loss: 0.05313846468925476\n",
      "15 val 10 883\n",
      "Phase: val. Epoch: 15. Loss: 0.05617144703865051\n",
      "15 val 11 884\n",
      "Phase: val. Epoch: 15. Loss: 0.12462213635444641\n",
      "16 train 0 12\n",
      "Phase: train. Epoch: 16. Loss: 0.09220906347036362\n",
      "16 train 1 24\n",
      "Phase: train. Epoch: 16. Loss: 0.08878526091575623\n",
      "16 train 2 36\n",
      "Phase: train. Epoch: 16. Loss: 0.06876610219478607\n",
      "16 train 3 48\n",
      "Phase: train. Epoch: 16. Loss: 0.1043604165315628\n",
      "16 train 4 60\n",
      "Phase: train. Epoch: 16. Loss: 0.09023413062095642\n",
      "16 train 5 72\n",
      "Phase: train. Epoch: 16. Loss: 0.08641594648361206\n",
      "16 train 6 84\n",
      "Phase: train. Epoch: 16. Loss: 0.07793411612510681\n",
      "16 train 7 96\n",
      "Phase: train. Epoch: 16. Loss: 0.09809901565313339\n",
      "16 train 8 108\n",
      "Phase: train. Epoch: 16. Loss: 0.11057402193546295\n",
      "16 train 9 120\n",
      "Phase: train. Epoch: 16. Loss: 0.06878551840782166\n",
      "16 train 10 132\n",
      "Phase: train. Epoch: 16. Loss: 0.06882928311824799\n",
      "16 train 11 144\n",
      "Phase: train. Epoch: 16. Loss: 0.07786163687705994\n",
      "16 train 12 156\n",
      "Phase: train. Epoch: 16. Loss: 0.06612043082714081\n",
      "16 train 13 168\n",
      "Phase: train. Epoch: 16. Loss: 0.082182377576828\n",
      "16 train 14 180\n",
      "Phase: train. Epoch: 16. Loss: 0.11715632677078247\n",
      "16 train 15 192\n",
      "Phase: train. Epoch: 16. Loss: 0.07170196622610092\n",
      "16 train 16 204\n",
      "Phase: train. Epoch: 16. Loss: 0.07493659108877182\n",
      "16 train 17 216\n",
      "Phase: train. Epoch: 16. Loss: 0.0752754658460617\n",
      "16 train 18 228\n",
      "Phase: train. Epoch: 16. Loss: 0.06484341621398926\n",
      "16 train 19 240\n",
      "Phase: train. Epoch: 16. Loss: 0.059082381427288055\n",
      "16 train 20 252\n",
      "Phase: train. Epoch: 16. Loss: 0.06718973815441132\n",
      "16 train 21 264\n",
      "Phase: train. Epoch: 16. Loss: 0.08840072154998779\n",
      "16 train 22 276\n",
      "Phase: train. Epoch: 16. Loss: 0.06439733505249023\n",
      "16 train 23 288\n",
      "Phase: train. Epoch: 16. Loss: 0.05605052039027214\n",
      "16 train 24 300\n",
      "Phase: train. Epoch: 16. Loss: 0.088304303586483\n",
      "16 train 25 312\n",
      "Phase: train. Epoch: 16. Loss: 0.0631205141544342\n",
      "16 train 26 324\n",
      "Phase: train. Epoch: 16. Loss: 0.13633938133716583\n",
      "16 train 27 336\n",
      "Phase: train. Epoch: 16. Loss: 0.08340513706207275\n",
      "16 train 28 348\n",
      "Phase: train. Epoch: 16. Loss: 0.050278615206480026\n",
      "16 train 29 360\n",
      "Phase: train. Epoch: 16. Loss: 0.08598288148641586\n",
      "16 train 30 372\n",
      "Phase: train. Epoch: 16. Loss: 0.0710102766752243\n",
      "16 train 31 384\n",
      "Phase: train. Epoch: 16. Loss: 0.07238967716693878\n",
      "16 train 32 396\n",
      "Phase: train. Epoch: 16. Loss: 0.08465562760829926\n",
      "16 train 33 408\n",
      "Phase: train. Epoch: 16. Loss: 0.06948951631784439\n",
      "16 train 34 420\n",
      "Phase: train. Epoch: 16. Loss: 0.0842401385307312\n",
      "16 train 35 432\n",
      "Phase: train. Epoch: 16. Loss: 0.07020287960767746\n",
      "16 train 36 444\n",
      "Phase: train. Epoch: 16. Loss: 0.06426038593053818\n",
      "16 train 37 456\n",
      "Phase: train. Epoch: 16. Loss: 0.08774596452713013\n",
      "16 train 38 468\n",
      "Phase: train. Epoch: 16. Loss: 0.10631566494703293\n",
      "16 train 39 480\n",
      "Phase: train. Epoch: 16. Loss: 0.07220359146595001\n",
      "16 train 40 492\n",
      "Phase: train. Epoch: 16. Loss: 0.12156244367361069\n",
      "16 train 41 504\n",
      "Phase: train. Epoch: 16. Loss: 0.07737399637699127\n",
      "16 train 42 516\n",
      "Phase: train. Epoch: 16. Loss: 0.11274826526641846\n",
      "16 train 43 528\n",
      "Phase: train. Epoch: 16. Loss: 0.07334166020154953\n",
      "16 train 44 540\n",
      "Phase: train. Epoch: 16. Loss: 0.07585244625806808\n",
      "16 train 45 552\n",
      "Phase: train. Epoch: 16. Loss: 0.07380901277065277\n",
      "16 train 46 564\n",
      "Phase: train. Epoch: 16. Loss: 0.08315830677747726\n",
      "16 train 47 576\n",
      "Phase: train. Epoch: 16. Loss: 0.08474364876747131\n",
      "16 train 48 588\n",
      "Phase: train. Epoch: 16. Loss: 0.0627569854259491\n",
      "16 train 49 600\n",
      "Phase: train. Epoch: 16. Loss: 0.07230404019355774\n",
      "16 train 50 612\n",
      "Phase: train. Epoch: 16. Loss: 0.06637929379940033\n",
      "16 train 51 624\n",
      "Phase: train. Epoch: 16. Loss: 0.09005376696586609\n",
      "16 train 52 636\n",
      "Phase: train. Epoch: 16. Loss: 0.08429624885320663\n",
      "16 train 53 648\n",
      "Phase: train. Epoch: 16. Loss: 0.0753125250339508\n",
      "16 train 54 660\n",
      "Phase: train. Epoch: 16. Loss: 0.0964251458644867\n",
      "16 train 55 672\n",
      "Phase: train. Epoch: 16. Loss: 0.1030241847038269\n",
      "16 train 56 684\n",
      "Phase: train. Epoch: 16. Loss: 0.06406426429748535\n",
      "16 train 57 696\n",
      "Phase: train. Epoch: 16. Loss: 0.060794807970523834\n",
      "16 train 58 708\n",
      "Phase: train. Epoch: 16. Loss: 0.06323780119419098\n",
      "16 train 59 720\n",
      "Phase: train. Epoch: 16. Loss: 0.0655205175280571\n",
      "16 train 60 732\n",
      "Phase: train. Epoch: 16. Loss: 0.07520821690559387\n",
      "16 train 61 744\n",
      "Phase: train. Epoch: 16. Loss: 0.08883929252624512\n",
      "16 train 62 751\n",
      "Phase: train. Epoch: 16. Loss: 0.0635915994644165\n",
      "16 val 0 763\n",
      "Phase: val. Epoch: 16. Loss: 0.07749881595373154\n",
      "16 val 1 775\n",
      "Phase: val. Epoch: 16. Loss: 0.10540954768657684\n",
      "16 val 2 787\n",
      "Phase: val. Epoch: 16. Loss: 0.05508830398321152\n",
      "16 val 3 799\n",
      "Phase: val. Epoch: 16. Loss: 0.04670102149248123\n",
      "16 val 4 811\n",
      "Phase: val. Epoch: 16. Loss: 0.12282636016607285\n",
      "16 val 5 823\n",
      "Phase: val. Epoch: 16. Loss: 0.0533454529941082\n",
      "16 val 6 835\n",
      "Phase: val. Epoch: 16. Loss: 0.08261355012655258\n",
      "16 val 7 847\n",
      "Phase: val. Epoch: 16. Loss: 0.07138222455978394\n",
      "16 val 8 859\n",
      "Phase: val. Epoch: 16. Loss: 0.06634359061717987\n",
      "16 val 9 871\n",
      "Phase: val. Epoch: 16. Loss: 0.10350936651229858\n",
      "16 val 10 883\n",
      "Phase: val. Epoch: 16. Loss: 0.09008540213108063\n",
      "16 val 11 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 16. Loss: 0.06662819534540176\n",
      "17 train 0 12\n",
      "Phase: train. Epoch: 17. Loss: 0.07038307189941406\n",
      "17 train 1 24\n",
      "Phase: train. Epoch: 17. Loss: 0.1404549479484558\n",
      "17 train 2 36\n",
      "Phase: train. Epoch: 17. Loss: 0.0832005962729454\n",
      "17 train 3 48\n",
      "Phase: train. Epoch: 17. Loss: 0.07532255351543427\n",
      "17 train 4 60\n",
      "Phase: train. Epoch: 17. Loss: 0.05078176409006119\n",
      "17 train 5 72\n",
      "Phase: train. Epoch: 17. Loss: 0.08934202790260315\n",
      "17 train 6 84\n",
      "Phase: train. Epoch: 17. Loss: 0.056511662900447845\n",
      "17 train 7 96\n",
      "Phase: train. Epoch: 17. Loss: 0.06508456915616989\n",
      "17 train 8 108\n",
      "Phase: train. Epoch: 17. Loss: 0.07425297796726227\n",
      "17 train 9 120\n",
      "Phase: train. Epoch: 17. Loss: 0.09078162908554077\n",
      "17 train 10 132\n",
      "Phase: train. Epoch: 17. Loss: 0.061427365988492966\n",
      "17 train 11 144\n",
      "Phase: train. Epoch: 17. Loss: 0.08948874473571777\n",
      "17 train 12 156\n",
      "Phase: train. Epoch: 17. Loss: 0.061167921870946884\n",
      "17 train 13 168\n",
      "Phase: train. Epoch: 17. Loss: 0.08987831324338913\n",
      "17 train 14 180\n",
      "Phase: train. Epoch: 17. Loss: 0.09550909698009491\n",
      "17 train 15 192\n",
      "Phase: train. Epoch: 17. Loss: 0.06206117570400238\n",
      "17 train 16 204\n",
      "Phase: train. Epoch: 17. Loss: 0.08263452351093292\n",
      "17 train 17 216\n",
      "Phase: train. Epoch: 17. Loss: 0.07973363250494003\n",
      "17 train 18 228\n",
      "Phase: train. Epoch: 17. Loss: 0.08845441043376923\n",
      "17 train 19 240\n",
      "Phase: train. Epoch: 17. Loss: 0.0737605020403862\n",
      "17 train 20 252\n",
      "Phase: train. Epoch: 17. Loss: 0.05801945552229881\n",
      "17 train 21 264\n",
      "Phase: train. Epoch: 17. Loss: 0.06542915105819702\n",
      "17 train 22 276\n",
      "Phase: train. Epoch: 17. Loss: 0.05959516763687134\n",
      "17 train 23 288\n",
      "Phase: train. Epoch: 17. Loss: 0.09885060042142868\n",
      "17 train 24 300\n",
      "Phase: train. Epoch: 17. Loss: 0.06581059098243713\n",
      "17 train 25 312\n",
      "Phase: train. Epoch: 17. Loss: 0.06634216755628586\n",
      "17 train 26 324\n",
      "Phase: train. Epoch: 17. Loss: 0.10463279485702515\n",
      "17 train 27 336\n",
      "Phase: train. Epoch: 17. Loss: 0.07511113584041595\n",
      "17 train 28 348\n",
      "Phase: train. Epoch: 17. Loss: 0.10379312932491302\n",
      "17 train 29 360\n",
      "Phase: train. Epoch: 17. Loss: 0.08064215630292892\n",
      "17 train 30 372\n",
      "Phase: train. Epoch: 17. Loss: 0.06277666985988617\n",
      "17 train 31 384\n",
      "Phase: train. Epoch: 17. Loss: 0.07761061191558838\n",
      "17 train 32 396\n",
      "Phase: train. Epoch: 17. Loss: 0.06555140018463135\n",
      "17 train 33 408\n",
      "Phase: train. Epoch: 17. Loss: 0.0598701648414135\n",
      "17 train 34 420\n",
      "Phase: train. Epoch: 17. Loss: 0.05533807724714279\n",
      "17 train 35 432\n",
      "Phase: train. Epoch: 17. Loss: 0.10780620574951172\n",
      "17 train 36 444\n",
      "Phase: train. Epoch: 17. Loss: 0.074976347386837\n",
      "17 train 37 456\n",
      "Phase: train. Epoch: 17. Loss: 0.12068009376525879\n",
      "17 train 38 468\n",
      "Phase: train. Epoch: 17. Loss: 0.07951630651950836\n",
      "17 train 39 480\n",
      "Phase: train. Epoch: 17. Loss: 0.0667758360505104\n",
      "17 train 40 492\n",
      "Phase: train. Epoch: 17. Loss: 0.06312417984008789\n",
      "17 train 41 504\n",
      "Phase: train. Epoch: 17. Loss: 0.08198290318250656\n",
      "17 train 42 516\n",
      "Phase: train. Epoch: 17. Loss: 0.07184117287397385\n",
      "17 train 43 528\n",
      "Phase: train. Epoch: 17. Loss: 0.06391271948814392\n",
      "17 train 44 540\n",
      "Phase: train. Epoch: 17. Loss: 0.07061012089252472\n",
      "17 train 45 552\n",
      "Phase: train. Epoch: 17. Loss: 0.06152399629354477\n",
      "17 train 46 564\n",
      "Phase: train. Epoch: 17. Loss: 0.09080766141414642\n",
      "17 train 47 576\n",
      "Phase: train. Epoch: 17. Loss: 0.08796635270118713\n",
      "17 train 48 588\n",
      "Phase: train. Epoch: 17. Loss: 0.11258681863546371\n",
      "17 train 49 600\n",
      "Phase: train. Epoch: 17. Loss: 0.052385684102773666\n",
      "17 train 50 612\n",
      "Phase: train. Epoch: 17. Loss: 0.09598829597234726\n",
      "17 train 51 624\n",
      "Phase: train. Epoch: 17. Loss: 0.11828529834747314\n",
      "17 train 52 636\n",
      "Phase: train. Epoch: 17. Loss: 0.08548659086227417\n",
      "17 train 53 648\n",
      "Phase: train. Epoch: 17. Loss: 0.0968482494354248\n",
      "17 train 54 660\n",
      "Phase: train. Epoch: 17. Loss: 0.05779748782515526\n",
      "17 train 55 672\n",
      "Phase: train. Epoch: 17. Loss: 0.06909973919391632\n",
      "17 train 56 684\n",
      "Phase: train. Epoch: 17. Loss: 0.08062934130430222\n",
      "17 train 57 696\n",
      "Phase: train. Epoch: 17. Loss: 0.07140597701072693\n",
      "17 train 58 708\n",
      "Phase: train. Epoch: 17. Loss: 0.06749168038368225\n",
      "17 train 59 720\n",
      "Phase: train. Epoch: 17. Loss: 0.07235640287399292\n",
      "17 train 60 732\n",
      "Phase: train. Epoch: 17. Loss: 0.0822850689291954\n",
      "17 train 61 744\n",
      "Phase: train. Epoch: 17. Loss: 0.06501083821058273\n",
      "17 train 62 751\n",
      "Phase: train. Epoch: 17. Loss: 0.0520440973341465\n",
      "17 val 0 763\n",
      "Phase: val. Epoch: 17. Loss: 0.06429851055145264\n",
      "17 val 1 775\n",
      "Phase: val. Epoch: 17. Loss: 0.09453749656677246\n",
      "17 val 2 787\n",
      "Phase: val. Epoch: 17. Loss: 0.07960455119609833\n",
      "17 val 3 799\n",
      "Phase: val. Epoch: 17. Loss: 0.06425036489963531\n",
      "17 val 4 811\n",
      "Phase: val. Epoch: 17. Loss: 0.046751078218221664\n",
      "17 val 5 823\n",
      "Phase: val. Epoch: 17. Loss: 0.07500340789556503\n",
      "17 val 6 835\n",
      "Phase: val. Epoch: 17. Loss: 0.06332635879516602\n",
      "17 val 7 847\n",
      "Phase: val. Epoch: 17. Loss: 0.08031526207923889\n",
      "17 val 8 859\n",
      "Phase: val. Epoch: 17. Loss: 0.08808343857526779\n",
      "17 val 9 871\n",
      "Phase: val. Epoch: 17. Loss: 0.12022444605827332\n",
      "17 val 10 883\n",
      "Phase: val. Epoch: 17. Loss: 0.05161340534687042\n",
      "17 val 11 884\n",
      "Phase: val. Epoch: 17. Loss: 0.02132546901702881\n",
      "18 train 0 12\n",
      "Phase: train. Epoch: 18. Loss: 0.07703584432601929\n",
      "18 train 1 24\n",
      "Phase: train. Epoch: 18. Loss: 0.07907755672931671\n",
      "18 train 2 36\n",
      "Phase: train. Epoch: 18. Loss: 0.08461840450763702\n",
      "18 train 3 48\n",
      "Phase: train. Epoch: 18. Loss: 0.08660577237606049\n",
      "18 train 4 60\n",
      "Phase: train. Epoch: 18. Loss: 0.08254506438970566\n",
      "18 train 5 72\n",
      "Phase: train. Epoch: 18. Loss: 0.06847883015871048\n",
      "18 train 6 84\n",
      "Phase: train. Epoch: 18. Loss: 0.056324176490306854\n",
      "18 train 7 96\n",
      "Phase: train. Epoch: 18. Loss: 0.06638427078723907\n",
      "18 train 8 108\n",
      "Phase: train. Epoch: 18. Loss: 0.05984792113304138\n",
      "18 train 9 120\n",
      "Phase: train. Epoch: 18. Loss: 0.08988728374242783\n",
      "18 train 10 132\n",
      "Phase: train. Epoch: 18. Loss: 0.05251239985227585\n",
      "18 train 11 144\n",
      "Phase: train. Epoch: 18. Loss: 0.0636100098490715\n",
      "18 train 12 156\n",
      "Phase: train. Epoch: 18. Loss: 0.06101632118225098\n",
      "18 train 13 168\n",
      "Phase: train. Epoch: 18. Loss: 0.0863194391131401\n",
      "18 train 14 180\n",
      "Phase: train. Epoch: 18. Loss: 0.06517845392227173\n",
      "18 train 15 192\n",
      "Phase: train. Epoch: 18. Loss: 0.08632104098796844\n",
      "18 train 16 204\n",
      "Phase: train. Epoch: 18. Loss: 0.07234769314527512\n",
      "18 train 17 216\n",
      "Phase: train. Epoch: 18. Loss: 0.0861123651266098\n",
      "18 train 18 228\n",
      "Phase: train. Epoch: 18. Loss: 0.0676582008600235\n",
      "18 train 19 240\n",
      "Phase: train. Epoch: 18. Loss: 0.06747841835021973\n",
      "18 train 20 252\n",
      "Phase: train. Epoch: 18. Loss: 0.07706049084663391\n",
      "18 train 21 264\n",
      "Phase: train. Epoch: 18. Loss: 0.05251869559288025\n",
      "18 train 22 276\n",
      "Phase: train. Epoch: 18. Loss: 0.06726032495498657\n",
      "18 train 23 288\n",
      "Phase: train. Epoch: 18. Loss: 0.10788950324058533\n",
      "18 train 24 300\n",
      "Phase: train. Epoch: 18. Loss: 0.0678846687078476\n",
      "18 train 25 312\n",
      "Phase: train. Epoch: 18. Loss: 0.06751160323619843\n",
      "18 train 26 324\n",
      "Phase: train. Epoch: 18. Loss: 0.08682011067867279\n",
      "18 train 27 336\n",
      "Phase: train. Epoch: 18. Loss: 0.08202957361936569\n",
      "18 train 28 348\n",
      "Phase: train. Epoch: 18. Loss: 0.07064800709486008\n",
      "18 train 29 360\n",
      "Phase: train. Epoch: 18. Loss: 0.07191845029592514\n",
      "18 train 30 372\n",
      "Phase: train. Epoch: 18. Loss: 0.07222750782966614\n",
      "18 train 31 384\n",
      "Phase: train. Epoch: 18. Loss: 0.10111507028341293\n",
      "18 train 32 396\n",
      "Phase: train. Epoch: 18. Loss: 0.101934514939785\n",
      "18 train 33 408\n",
      "Phase: train. Epoch: 18. Loss: 0.06588089466094971\n",
      "18 train 34 420\n",
      "Phase: train. Epoch: 18. Loss: 0.07397010922431946\n",
      "18 train 35 432\n",
      "Phase: train. Epoch: 18. Loss: 0.08947689831256866\n",
      "18 train 36 444\n",
      "Phase: train. Epoch: 18. Loss: 0.07247138023376465\n",
      "18 train 37 456\n",
      "Phase: train. Epoch: 18. Loss: 0.07124115526676178\n",
      "18 train 38 468\n",
      "Phase: train. Epoch: 18. Loss: 0.08756866306066513\n",
      "18 train 39 480\n",
      "Phase: train. Epoch: 18. Loss: 0.07953439652919769\n",
      "18 train 40 492\n",
      "Phase: train. Epoch: 18. Loss: 0.08450692892074585\n",
      "18 train 41 504\n",
      "Phase: train. Epoch: 18. Loss: 0.053720176219940186\n",
      "18 train 42 516\n",
      "Phase: train. Epoch: 18. Loss: 0.08073289692401886\n",
      "18 train 43 528\n",
      "Phase: train. Epoch: 18. Loss: 0.09482189267873764\n",
      "18 train 44 540\n",
      "Phase: train. Epoch: 18. Loss: 0.065411776304245\n",
      "18 train 45 552\n",
      "Phase: train. Epoch: 18. Loss: 0.07941348850727081\n",
      "18 train 46 564\n",
      "Phase: train. Epoch: 18. Loss: 0.07036915421485901\n",
      "18 train 47 576\n",
      "Phase: train. Epoch: 18. Loss: 0.09429828822612762\n",
      "18 train 48 588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 18. Loss: 0.07854683697223663\n",
      "18 train 49 600\n",
      "Phase: train. Epoch: 18. Loss: 0.08986242860555649\n",
      "18 train 50 612\n",
      "Phase: train. Epoch: 18. Loss: 0.11455553770065308\n",
      "18 train 51 624\n",
      "Phase: train. Epoch: 18. Loss: 0.0645563006401062\n",
      "18 train 52 636\n",
      "Phase: train. Epoch: 18. Loss: 0.12217243015766144\n",
      "18 train 53 648\n",
      "Phase: train. Epoch: 18. Loss: 0.09711172431707382\n",
      "18 train 54 660\n",
      "Phase: train. Epoch: 18. Loss: 0.05525942146778107\n",
      "18 train 55 672\n",
      "Phase: train. Epoch: 18. Loss: 0.06582721322774887\n",
      "18 train 56 684\n",
      "Phase: train. Epoch: 18. Loss: 0.08684800565242767\n",
      "18 train 57 696\n",
      "Phase: train. Epoch: 18. Loss: 0.06052510067820549\n",
      "18 train 58 708\n",
      "Phase: train. Epoch: 18. Loss: 0.08311015367507935\n",
      "18 train 59 720\n",
      "Phase: train. Epoch: 18. Loss: 0.07963049411773682\n",
      "18 train 60 732\n",
      "Phase: train. Epoch: 18. Loss: 0.06678616255521774\n",
      "18 train 61 744\n",
      "Phase: train. Epoch: 18. Loss: 0.04365856945514679\n",
      "18 train 62 751\n",
      "Phase: train. Epoch: 18. Loss: 0.0672721341252327\n",
      "18 val 0 763\n",
      "Phase: val. Epoch: 18. Loss: 0.10018110275268555\n",
      "18 val 1 775\n",
      "Phase: val. Epoch: 18. Loss: 0.06251363456249237\n",
      "18 val 2 787\n",
      "Phase: val. Epoch: 18. Loss: 0.06854551285505295\n",
      "18 val 3 799\n",
      "Phase: val. Epoch: 18. Loss: 0.1323503702878952\n",
      "18 val 4 811\n",
      "Phase: val. Epoch: 18. Loss: 0.05488535761833191\n",
      "18 val 5 823\n",
      "Phase: val. Epoch: 18. Loss: 0.09016671031713486\n",
      "18 val 6 835\n",
      "Phase: val. Epoch: 18. Loss: 0.06047583743929863\n",
      "18 val 7 847\n",
      "Phase: val. Epoch: 18. Loss: 0.07746587693691254\n",
      "18 val 8 859\n",
      "Phase: val. Epoch: 18. Loss: 0.06838227808475494\n",
      "18 val 9 871\n",
      "Phase: val. Epoch: 18. Loss: 0.08160965889692307\n",
      "18 val 10 883\n",
      "Phase: val. Epoch: 18. Loss: 0.07487481832504272\n",
      "18 val 11 884\n",
      "Phase: val. Epoch: 18. Loss: 0.16513076424598694\n",
      "19 train 0 12\n",
      "Phase: train. Epoch: 19. Loss: 0.07947918772697449\n",
      "19 train 1 24\n",
      "Phase: train. Epoch: 19. Loss: 0.061746031045913696\n",
      "19 train 2 36\n",
      "Phase: train. Epoch: 19. Loss: 0.09592711925506592\n",
      "19 train 3 48\n",
      "Phase: train. Epoch: 19. Loss: 0.08637166023254395\n",
      "19 train 4 60\n",
      "Phase: train. Epoch: 19. Loss: 0.06669854372739792\n",
      "19 train 5 72\n",
      "Phase: train. Epoch: 19. Loss: 0.0784955769777298\n",
      "19 train 6 84\n",
      "Phase: train. Epoch: 19. Loss: 0.11327141523361206\n",
      "19 train 7 96\n",
      "Phase: train. Epoch: 19. Loss: 0.07952812314033508\n",
      "19 train 8 108\n",
      "Phase: train. Epoch: 19. Loss: 0.06791229546070099\n",
      "19 train 9 120\n",
      "Phase: train. Epoch: 19. Loss: 0.06025223061442375\n",
      "19 train 10 132\n",
      "Phase: train. Epoch: 19. Loss: 0.058602482080459595\n",
      "19 train 11 144\n",
      "Phase: train. Epoch: 19. Loss: 0.06194755434989929\n",
      "19 train 12 156\n",
      "Phase: train. Epoch: 19. Loss: 0.11782346665859222\n",
      "19 train 13 168\n",
      "Phase: train. Epoch: 19. Loss: 0.06761741638183594\n",
      "19 train 14 180\n",
      "Phase: train. Epoch: 19. Loss: 0.1128002405166626\n",
      "19 train 15 192\n",
      "Phase: train. Epoch: 19. Loss: 0.07124434411525726\n",
      "19 train 16 204\n",
      "Phase: train. Epoch: 19. Loss: 0.08143073320388794\n",
      "19 train 17 216\n",
      "Phase: train. Epoch: 19. Loss: 0.06417946517467499\n",
      "19 train 18 228\n",
      "Phase: train. Epoch: 19. Loss: 0.05683686584234238\n",
      "19 train 19 240\n",
      "Phase: train. Epoch: 19. Loss: 0.06471145153045654\n",
      "19 train 20 252\n",
      "Phase: train. Epoch: 19. Loss: 0.09143532812595367\n",
      "19 train 21 264\n",
      "Phase: train. Epoch: 19. Loss: 0.06896992772817612\n",
      "19 train 22 276\n",
      "Phase: train. Epoch: 19. Loss: 0.10578237473964691\n",
      "19 train 23 288\n",
      "Phase: train. Epoch: 19. Loss: 0.09742790460586548\n",
      "19 train 24 300\n",
      "Phase: train. Epoch: 19. Loss: 0.06303173303604126\n",
      "19 train 25 312\n",
      "Phase: train. Epoch: 19. Loss: 0.0702386125922203\n",
      "19 train 26 324\n",
      "Phase: train. Epoch: 19. Loss: 0.0815572664141655\n",
      "19 train 27 336\n",
      "Phase: train. Epoch: 19. Loss: 0.06606437265872955\n",
      "19 train 28 348\n",
      "Phase: train. Epoch: 19. Loss: 0.07926668971776962\n",
      "19 train 29 360\n",
      "Phase: train. Epoch: 19. Loss: 0.05969782918691635\n",
      "19 train 30 372\n",
      "Phase: train. Epoch: 19. Loss: 0.0597207248210907\n",
      "19 train 31 384\n",
      "Phase: train. Epoch: 19. Loss: 0.07571472227573395\n",
      "19 train 32 396\n",
      "Phase: train. Epoch: 19. Loss: 0.0876888856291771\n",
      "19 train 33 408\n",
      "Phase: train. Epoch: 19. Loss: 0.09358695149421692\n",
      "19 train 34 420\n",
      "Phase: train. Epoch: 19. Loss: 0.07025520503520966\n",
      "19 train 35 432\n",
      "Phase: train. Epoch: 19. Loss: 0.095264732837677\n",
      "19 train 36 444\n",
      "Phase: train. Epoch: 19. Loss: 0.0658470019698143\n",
      "19 train 37 456\n",
      "Phase: train. Epoch: 19. Loss: 0.05192431062459946\n",
      "19 train 38 468\n",
      "Phase: train. Epoch: 19. Loss: 0.08947211503982544\n",
      "19 train 39 480\n",
      "Phase: train. Epoch: 19. Loss: 0.07777661830186844\n",
      "19 train 40 492\n",
      "Phase: train. Epoch: 19. Loss: 0.0564626082777977\n",
      "19 train 41 504\n",
      "Phase: train. Epoch: 19. Loss: 0.07148125767707825\n",
      "19 train 42 516\n",
      "Phase: train. Epoch: 19. Loss: 0.07257160544395447\n",
      "19 train 43 528\n",
      "Phase: train. Epoch: 19. Loss: 0.09111146628856659\n",
      "19 train 44 540\n",
      "Phase: train. Epoch: 19. Loss: 0.10457204282283783\n",
      "19 train 45 552\n",
      "Phase: train. Epoch: 19. Loss: 0.08704453706741333\n",
      "19 train 46 564\n",
      "Phase: train. Epoch: 19. Loss: 0.07055511325597763\n",
      "19 train 47 576\n",
      "Phase: train. Epoch: 19. Loss: 0.09578092396259308\n",
      "19 train 48 588\n",
      "Phase: train. Epoch: 19. Loss: 0.06256763637065887\n",
      "19 train 49 600\n",
      "Phase: train. Epoch: 19. Loss: 0.06698071211576462\n",
      "19 train 50 612\n",
      "Phase: train. Epoch: 19. Loss: 0.08070603013038635\n",
      "19 train 51 624\n",
      "Phase: train. Epoch: 19. Loss: 0.09574990719556808\n",
      "19 train 52 636\n",
      "Phase: train. Epoch: 19. Loss: 0.06335002928972244\n",
      "19 train 53 648\n",
      "Phase: train. Epoch: 19. Loss: 0.07406701892614365\n",
      "19 train 54 660\n",
      "Phase: train. Epoch: 19. Loss: 0.05312022566795349\n",
      "19 train 55 672\n",
      "Phase: train. Epoch: 19. Loss: 0.051983218640089035\n",
      "19 train 56 684\n",
      "Phase: train. Epoch: 19. Loss: 0.06752994656562805\n",
      "19 train 57 696\n",
      "Phase: train. Epoch: 19. Loss: 0.09068267047405243\n",
      "19 train 58 708\n",
      "Phase: train. Epoch: 19. Loss: 0.0813153088092804\n",
      "19 train 59 720\n",
      "Phase: train. Epoch: 19. Loss: 0.08039280027151108\n",
      "19 train 60 732\n",
      "Phase: train. Epoch: 19. Loss: 0.07031190395355225\n",
      "19 train 61 744\n",
      "Phase: train. Epoch: 19. Loss: 0.11190448701381683\n",
      "19 train 62 751\n",
      "Phase: train. Epoch: 19. Loss: 0.07925780117511749\n",
      "19 val 0 763\n",
      "Phase: val. Epoch: 19. Loss: 0.05638188123703003\n",
      "19 val 1 775\n",
      "Phase: val. Epoch: 19. Loss: 0.09640449285507202\n",
      "19 val 2 787\n",
      "Phase: val. Epoch: 19. Loss: 0.07320176810026169\n",
      "19 val 3 799\n",
      "Phase: val. Epoch: 19. Loss: 0.05124424025416374\n",
      "19 val 4 811\n",
      "Phase: val. Epoch: 19. Loss: 0.053699396550655365\n",
      "19 val 5 823\n",
      "Phase: val. Epoch: 19. Loss: 0.0700942799448967\n",
      "19 val 6 835\n",
      "Phase: val. Epoch: 19. Loss: 0.07370758056640625\n",
      "19 val 7 847\n",
      "Phase: val. Epoch: 19. Loss: 0.052974849939346313\n",
      "19 val 8 859\n",
      "Phase: val. Epoch: 19. Loss: 0.057357292622327805\n",
      "19 val 9 871\n",
      "Phase: val. Epoch: 19. Loss: 0.0654677003622055\n",
      "19 val 10 883\n",
      "Phase: val. Epoch: 19. Loss: 0.08024841547012329\n",
      "19 val 11 884\n",
      "Phase: val. Epoch: 19. Loss: 0.02010533958673477\n",
      "20 train 0 12\n",
      "Phase: train. Epoch: 20. Loss: 0.0716095119714737\n",
      "20 train 1 24\n",
      "Phase: train. Epoch: 20. Loss: 0.0679459497332573\n",
      "20 train 2 36\n",
      "Phase: train. Epoch: 20. Loss: 0.10875003784894943\n",
      "20 train 3 48\n",
      "Phase: train. Epoch: 20. Loss: 0.08186018466949463\n",
      "20 train 4 60\n",
      "Phase: train. Epoch: 20. Loss: 0.10011114925146103\n",
      "20 train 5 72\n",
      "Phase: train. Epoch: 20. Loss: 0.06726757436990738\n",
      "20 train 6 84\n",
      "Phase: train. Epoch: 20. Loss: 0.0757439061999321\n",
      "20 train 7 96\n",
      "Phase: train. Epoch: 20. Loss: 0.06725972890853882\n",
      "20 train 8 108\n",
      "Phase: train. Epoch: 20. Loss: 0.06536820530891418\n",
      "20 train 9 120\n",
      "Phase: train. Epoch: 20. Loss: 0.0746883898973465\n",
      "20 train 10 132\n",
      "Phase: train. Epoch: 20. Loss: 0.07433073222637177\n",
      "20 train 11 144\n",
      "Phase: train. Epoch: 20. Loss: 0.08081811666488647\n",
      "20 train 12 156\n",
      "Phase: train. Epoch: 20. Loss: 0.09182082116603851\n",
      "20 train 13 168\n",
      "Phase: train. Epoch: 20. Loss: 0.07417496293783188\n",
      "20 train 14 180\n",
      "Phase: train. Epoch: 20. Loss: 0.06450025737285614\n",
      "20 train 15 192\n",
      "Phase: train. Epoch: 20. Loss: 0.08453451097011566\n",
      "20 train 16 204\n",
      "Phase: train. Epoch: 20. Loss: 0.0632798969745636\n",
      "20 train 17 216\n",
      "Phase: train. Epoch: 20. Loss: 0.045223891735076904\n",
      "20 train 18 228\n",
      "Phase: train. Epoch: 20. Loss: 0.046546854078769684\n",
      "20 train 19 240\n",
      "Phase: train. Epoch: 20. Loss: 0.07260643690824509\n",
      "20 train 20 252\n",
      "Phase: train. Epoch: 20. Loss: 0.16369380056858063\n",
      "20 train 21 264\n",
      "Phase: train. Epoch: 20. Loss: 0.05270438641309738\n",
      "20 train 22 276\n",
      "Phase: train. Epoch: 20. Loss: 0.07728643715381622\n",
      "20 train 23 288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 20. Loss: 0.07661047577857971\n",
      "20 train 24 300\n",
      "Phase: train. Epoch: 20. Loss: 0.07760684937238693\n",
      "20 train 25 312\n",
      "Phase: train. Epoch: 20. Loss: 0.0741170272231102\n",
      "20 train 26 324\n",
      "Phase: train. Epoch: 20. Loss: 0.06729710102081299\n",
      "20 train 27 336\n",
      "Phase: train. Epoch: 20. Loss: 0.06265486776828766\n",
      "20 train 28 348\n",
      "Phase: train. Epoch: 20. Loss: 0.07803111523389816\n",
      "20 train 29 360\n",
      "Phase: train. Epoch: 20. Loss: 0.08283299952745438\n",
      "20 train 30 372\n",
      "Phase: train. Epoch: 20. Loss: 0.09882616251707077\n",
      "20 train 31 384\n",
      "Phase: train. Epoch: 20. Loss: 0.05615910887718201\n",
      "20 train 32 396\n",
      "Phase: train. Epoch: 20. Loss: 0.07446075230836868\n",
      "20 train 33 408\n",
      "Phase: train. Epoch: 20. Loss: 0.07502295076847076\n",
      "20 train 34 420\n",
      "Phase: train. Epoch: 20. Loss: 0.08112847059965134\n",
      "20 train 35 432\n",
      "Phase: train. Epoch: 20. Loss: 0.08516594767570496\n",
      "20 train 36 444\n",
      "Phase: train. Epoch: 20. Loss: 0.06183148920536041\n",
      "20 train 37 456\n",
      "Phase: train. Epoch: 20. Loss: 0.05321764945983887\n",
      "20 train 38 468\n",
      "Phase: train. Epoch: 20. Loss: 0.0881989449262619\n",
      "20 train 39 480\n",
      "Phase: train. Epoch: 20. Loss: 0.050158701837062836\n",
      "20 train 40 492\n",
      "Phase: train. Epoch: 20. Loss: 0.05583510920405388\n",
      "20 train 41 504\n",
      "Phase: train. Epoch: 20. Loss: 0.05746787041425705\n",
      "20 train 42 516\n",
      "Phase: train. Epoch: 20. Loss: 0.07065748423337936\n",
      "20 train 43 528\n",
      "Phase: train. Epoch: 20. Loss: 0.10084116458892822\n",
      "20 train 44 540\n",
      "Phase: train. Epoch: 20. Loss: 0.08234807848930359\n",
      "20 train 45 552\n",
      "Phase: train. Epoch: 20. Loss: 0.05313599854707718\n",
      "20 train 46 564\n",
      "Phase: train. Epoch: 20. Loss: 0.08646275848150253\n",
      "20 train 47 576\n",
      "Phase: train. Epoch: 20. Loss: 0.13722029328346252\n",
      "20 train 48 588\n",
      "Phase: train. Epoch: 20. Loss: 0.09754054993391037\n",
      "20 train 49 600\n",
      "Phase: train. Epoch: 20. Loss: 0.05203300341963768\n",
      "20 train 50 612\n",
      "Phase: train. Epoch: 20. Loss: 0.05975283309817314\n",
      "20 train 51 624\n",
      "Phase: train. Epoch: 20. Loss: 0.0871092677116394\n",
      "20 train 52 636\n",
      "Phase: train. Epoch: 20. Loss: 0.07919106632471085\n",
      "20 train 53 648\n",
      "Phase: train. Epoch: 20. Loss: 0.06470610946416855\n",
      "20 train 54 660\n",
      "Phase: train. Epoch: 20. Loss: 0.06981554627418518\n",
      "20 train 55 672\n",
      "Phase: train. Epoch: 20. Loss: 0.05411438271403313\n",
      "20 train 56 684\n",
      "Phase: train. Epoch: 20. Loss: 0.0628192275762558\n",
      "20 train 57 696\n",
      "Phase: train. Epoch: 20. Loss: 0.09442746639251709\n",
      "20 train 58 708\n",
      "Phase: train. Epoch: 20. Loss: 0.08131500333547592\n",
      "20 train 59 720\n",
      "Phase: train. Epoch: 20. Loss: 0.05547991022467613\n",
      "20 train 60 732\n",
      "Phase: train. Epoch: 20. Loss: 0.10772740095853806\n",
      "20 train 61 744\n",
      "Phase: train. Epoch: 20. Loss: 0.06754516810178757\n",
      "20 train 62 751\n",
      "Phase: train. Epoch: 20. Loss: 0.08155465126037598\n",
      "20 val 0 763\n",
      "Phase: val. Epoch: 20. Loss: 0.11229966580867767\n",
      "20 val 1 775\n",
      "Phase: val. Epoch: 20. Loss: 0.07997837662696838\n",
      "20 val 2 787\n",
      "Phase: val. Epoch: 20. Loss: 0.05090458318591118\n",
      "20 val 3 799\n",
      "Phase: val. Epoch: 20. Loss: 0.05367076396942139\n",
      "20 val 4 811\n",
      "Phase: val. Epoch: 20. Loss: 0.05732521414756775\n",
      "20 val 5 823\n",
      "Phase: val. Epoch: 20. Loss: 0.04493173211812973\n",
      "20 val 6 835\n",
      "Phase: val. Epoch: 20. Loss: 0.07608974725008011\n",
      "20 val 7 847\n",
      "Phase: val. Epoch: 20. Loss: 0.07829588651657104\n",
      "20 val 8 859\n",
      "Phase: val. Epoch: 20. Loss: 0.10790248960256577\n",
      "20 val 9 871\n",
      "Phase: val. Epoch: 20. Loss: 0.05473221465945244\n",
      "20 val 10 883\n",
      "Phase: val. Epoch: 20. Loss: 0.09274067729711533\n",
      "20 val 11 884\n",
      "Phase: val. Epoch: 20. Loss: 0.07607657462358475\n",
      "21 train 0 12\n",
      "Phase: train. Epoch: 21. Loss: 0.07083773612976074\n",
      "21 train 1 24\n",
      "Phase: train. Epoch: 21. Loss: 0.06232244893908501\n",
      "21 train 2 36\n",
      "Phase: train. Epoch: 21. Loss: 0.04746643826365471\n",
      "21 train 3 48\n",
      "Phase: train. Epoch: 21. Loss: 0.07292364537715912\n",
      "21 train 4 60\n",
      "Phase: train. Epoch: 21. Loss: 0.07757466286420822\n",
      "21 train 5 72\n",
      "Phase: train. Epoch: 21. Loss: 0.043438225984573364\n",
      "21 train 6 84\n",
      "Phase: train. Epoch: 21. Loss: 0.06473305821418762\n",
      "21 train 7 96\n",
      "Phase: train. Epoch: 21. Loss: 0.0606984905898571\n",
      "21 train 8 108\n",
      "Phase: train. Epoch: 21. Loss: 0.054970066994428635\n",
      "21 train 9 120\n",
      "Phase: train. Epoch: 21. Loss: 0.06953708827495575\n",
      "21 train 10 132\n",
      "Phase: train. Epoch: 21. Loss: 0.09303390979766846\n",
      "21 train 11 144\n",
      "Phase: train. Epoch: 21. Loss: 0.07121944427490234\n",
      "21 train 12 156\n",
      "Phase: train. Epoch: 21. Loss: 0.10489027947187424\n",
      "21 train 13 168\n",
      "Phase: train. Epoch: 21. Loss: 0.07687307894229889\n",
      "21 train 14 180\n",
      "Phase: train. Epoch: 21. Loss: 0.03732966631650925\n",
      "21 train 15 192\n",
      "Phase: train. Epoch: 21. Loss: 0.08421122282743454\n",
      "21 train 16 204\n",
      "Phase: train. Epoch: 21. Loss: 0.06296248733997345\n",
      "21 train 17 216\n",
      "Phase: train. Epoch: 21. Loss: 0.0697053074836731\n",
      "21 train 18 228\n",
      "Phase: train. Epoch: 21. Loss: 0.06968816369771957\n",
      "21 train 19 240\n",
      "Phase: train. Epoch: 21. Loss: 0.06002029776573181\n",
      "21 train 20 252\n",
      "Phase: train. Epoch: 21. Loss: 0.06017760559916496\n",
      "21 train 21 264\n",
      "Phase: train. Epoch: 21. Loss: 0.07502198219299316\n",
      "21 train 22 276\n",
      "Phase: train. Epoch: 21. Loss: 0.06917881965637207\n",
      "21 train 23 288\n",
      "Phase: train. Epoch: 21. Loss: 0.076783686876297\n",
      "21 train 24 300\n",
      "Phase: train. Epoch: 21. Loss: 0.10306524485349655\n",
      "21 train 25 312\n",
      "Phase: train. Epoch: 21. Loss: 0.13476064801216125\n",
      "21 train 26 324\n",
      "Phase: train. Epoch: 21. Loss: 0.07953953742980957\n",
      "21 train 27 336\n",
      "Phase: train. Epoch: 21. Loss: 0.12405746430158615\n",
      "21 train 28 348\n",
      "Phase: train. Epoch: 21. Loss: 0.07618142664432526\n",
      "21 train 29 360\n",
      "Phase: train. Epoch: 21. Loss: 0.06169099733233452\n",
      "21 train 30 372\n",
      "Phase: train. Epoch: 21. Loss: 0.06067551299929619\n",
      "21 train 31 384\n",
      "Phase: train. Epoch: 21. Loss: 0.058450158685445786\n",
      "21 train 32 396\n",
      "Phase: train. Epoch: 21. Loss: 0.06298281252384186\n",
      "21 train 33 408\n",
      "Phase: train. Epoch: 21. Loss: 0.08350040763616562\n",
      "21 train 34 420\n",
      "Phase: train. Epoch: 21. Loss: 0.061220765113830566\n",
      "21 train 35 432\n",
      "Phase: train. Epoch: 21. Loss: 0.09899134933948517\n",
      "21 train 36 444\n",
      "Phase: train. Epoch: 21. Loss: 0.06544017791748047\n",
      "21 train 37 456\n",
      "Phase: train. Epoch: 21. Loss: 0.05797061324119568\n",
      "21 train 38 468\n",
      "Phase: train. Epoch: 21. Loss: 0.10973042249679565\n",
      "21 train 39 480\n",
      "Phase: train. Epoch: 21. Loss: 0.05098193511366844\n",
      "21 train 40 492\n",
      "Phase: train. Epoch: 21. Loss: 0.046803124248981476\n",
      "21 train 41 504\n",
      "Phase: train. Epoch: 21. Loss: 0.07656306028366089\n",
      "21 train 42 516\n",
      "Phase: train. Epoch: 21. Loss: 0.05992088466882706\n",
      "21 train 43 528\n",
      "Phase: train. Epoch: 21. Loss: 0.08472424000501633\n",
      "21 train 44 540\n",
      "Phase: train. Epoch: 21. Loss: 0.03207671642303467\n",
      "21 train 45 552\n",
      "Phase: train. Epoch: 21. Loss: 0.04892731457948685\n",
      "21 train 46 564\n",
      "Phase: train. Epoch: 21. Loss: 0.08928130567073822\n",
      "21 train 47 576\n",
      "Phase: train. Epoch: 21. Loss: 0.06587792187929153\n",
      "21 train 48 588\n",
      "Phase: train. Epoch: 21. Loss: 0.07174329459667206\n",
      "21 train 49 600\n",
      "Phase: train. Epoch: 21. Loss: 0.07663169503211975\n",
      "21 train 50 612\n",
      "Phase: train. Epoch: 21. Loss: 0.06939730048179626\n",
      "21 train 51 624\n",
      "Phase: train. Epoch: 21. Loss: 0.12679341435432434\n",
      "21 train 52 636\n",
      "Phase: train. Epoch: 21. Loss: 0.06125744432210922\n",
      "21 train 53 648\n",
      "Phase: train. Epoch: 21. Loss: 0.05354272574186325\n",
      "21 train 54 660\n",
      "Phase: train. Epoch: 21. Loss: 0.05255744606256485\n",
      "21 train 55 672\n",
      "Phase: train. Epoch: 21. Loss: 0.08107984066009521\n",
      "21 train 56 684\n",
      "Phase: train. Epoch: 21. Loss: 0.0932176411151886\n",
      "21 train 57 696\n",
      "Phase: train. Epoch: 21. Loss: 0.07036775350570679\n",
      "21 train 58 708\n",
      "Phase: train. Epoch: 21. Loss: 0.07319025695323944\n",
      "21 train 59 720\n",
      "Phase: train. Epoch: 21. Loss: 0.0724174752831459\n",
      "21 train 60 732\n",
      "Phase: train. Epoch: 21. Loss: 0.10228046774864197\n",
      "21 train 61 744\n",
      "Phase: train. Epoch: 21. Loss: 0.0896461233496666\n",
      "21 train 62 751\n",
      "Phase: train. Epoch: 21. Loss: 0.07180318236351013\n",
      "21 val 0 763\n",
      "Phase: val. Epoch: 21. Loss: 0.09177635610103607\n",
      "21 val 1 775\n",
      "Phase: val. Epoch: 21. Loss: 0.07603943347930908\n",
      "21 val 2 787\n",
      "Phase: val. Epoch: 21. Loss: 0.06687301397323608\n",
      "21 val 3 799\n",
      "Phase: val. Epoch: 21. Loss: 0.06272676587104797\n",
      "21 val 4 811\n",
      "Phase: val. Epoch: 21. Loss: 0.08358608186244965\n",
      "21 val 5 823\n",
      "Phase: val. Epoch: 21. Loss: 0.06103929877281189\n",
      "21 val 6 835\n",
      "Phase: val. Epoch: 21. Loss: 0.07463284581899643\n",
      "21 val 7 847\n",
      "Phase: val. Epoch: 21. Loss: 0.06863638013601303\n",
      "21 val 8 859\n",
      "Phase: val. Epoch: 21. Loss: 0.06748916953802109\n",
      "21 val 9 871\n",
      "Phase: val. Epoch: 21. Loss: 0.09317445755004883\n",
      "21 val 10 883\n",
      "Phase: val. Epoch: 21. Loss: 0.06221386417746544\n",
      "21 val 11 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 21. Loss: 0.09492500126361847\n",
      "22 train 0 12\n",
      "Phase: train. Epoch: 22. Loss: 0.07212337106466293\n",
      "22 train 1 24\n",
      "Phase: train. Epoch: 22. Loss: 0.06383131444454193\n",
      "22 train 2 36\n",
      "Phase: train. Epoch: 22. Loss: 0.11531217396259308\n",
      "22 train 3 48\n",
      "Phase: train. Epoch: 22. Loss: 0.08707763999700546\n",
      "22 train 4 60\n",
      "Phase: train. Epoch: 22. Loss: 0.0762607529759407\n",
      "22 train 5 72\n",
      "Phase: train. Epoch: 22. Loss: 0.07713806629180908\n",
      "22 train 6 84\n",
      "Phase: train. Epoch: 22. Loss: 0.06438934803009033\n",
      "22 train 7 96\n",
      "Phase: train. Epoch: 22. Loss: 0.06190779060125351\n",
      "22 train 8 108\n",
      "Phase: train. Epoch: 22. Loss: 0.09173904359340668\n",
      "22 train 9 120\n",
      "Phase: train. Epoch: 22. Loss: 0.07895557582378387\n",
      "22 train 10 132\n",
      "Phase: train. Epoch: 22. Loss: 0.07478605210781097\n",
      "22 train 11 144\n",
      "Phase: train. Epoch: 22. Loss: 0.06022302433848381\n",
      "22 train 12 156\n",
      "Phase: train. Epoch: 22. Loss: 0.05082499235868454\n",
      "22 train 13 168\n",
      "Phase: train. Epoch: 22. Loss: 0.08111940324306488\n",
      "22 train 14 180\n",
      "Phase: train. Epoch: 22. Loss: 0.059438206255435944\n",
      "22 train 15 192\n",
      "Phase: train. Epoch: 22. Loss: 0.10159742832183838\n",
      "22 train 16 204\n",
      "Phase: train. Epoch: 22. Loss: 0.04706565663218498\n",
      "22 train 17 216\n",
      "Phase: train. Epoch: 22. Loss: 0.061176326125860214\n",
      "22 train 18 228\n",
      "Phase: train. Epoch: 22. Loss: 0.09697919338941574\n",
      "22 train 19 240\n",
      "Phase: train. Epoch: 22. Loss: 0.07617993652820587\n",
      "22 train 20 252\n",
      "Phase: train. Epoch: 22. Loss: 0.10122831910848618\n",
      "22 train 21 264\n",
      "Phase: train. Epoch: 22. Loss: 0.07646134495735168\n",
      "22 train 22 276\n",
      "Phase: train. Epoch: 22. Loss: 0.09174754470586777\n",
      "22 train 23 288\n",
      "Phase: train. Epoch: 22. Loss: 0.06373003870248795\n",
      "22 train 24 300\n",
      "Phase: train. Epoch: 22. Loss: 0.05833900719881058\n",
      "22 train 25 312\n",
      "Phase: train. Epoch: 22. Loss: 0.049164123833179474\n",
      "22 train 26 324\n",
      "Phase: train. Epoch: 22. Loss: 0.06638489663600922\n",
      "22 train 27 336\n",
      "Phase: train. Epoch: 22. Loss: 0.08376891911029816\n",
      "22 train 28 348\n",
      "Phase: train. Epoch: 22. Loss: 0.06225830316543579\n",
      "22 train 29 360\n",
      "Phase: train. Epoch: 22. Loss: 0.06695427000522614\n",
      "22 train 30 372\n",
      "Phase: train. Epoch: 22. Loss: 0.08661063760519028\n",
      "22 train 31 384\n",
      "Phase: train. Epoch: 22. Loss: 0.09793748706579208\n",
      "22 train 32 396\n",
      "Phase: train. Epoch: 22. Loss: 0.04571673274040222\n",
      "22 train 33 408\n",
      "Phase: train. Epoch: 22. Loss: 0.0532369427382946\n",
      "22 train 34 420\n",
      "Phase: train. Epoch: 22. Loss: 0.07789421081542969\n",
      "22 train 35 432\n",
      "Phase: train. Epoch: 22. Loss: 0.055474769324064255\n",
      "22 train 36 444\n",
      "Phase: train. Epoch: 22. Loss: 0.07239247113466263\n",
      "22 train 37 456\n",
      "Phase: train. Epoch: 22. Loss: 0.03907281905412674\n",
      "22 train 38 468\n",
      "Phase: train. Epoch: 22. Loss: 0.09927459806203842\n",
      "22 train 39 480\n",
      "Phase: train. Epoch: 22. Loss: 0.0426885150372982\n",
      "22 train 40 492\n",
      "Phase: train. Epoch: 22. Loss: 0.10630285739898682\n",
      "22 train 41 504\n",
      "Phase: train. Epoch: 22. Loss: 0.06264221668243408\n",
      "22 train 42 516\n",
      "Phase: train. Epoch: 22. Loss: 0.06941220164299011\n",
      "22 train 43 528\n",
      "Phase: train. Epoch: 22. Loss: 0.06421152502298355\n",
      "22 train 44 540\n",
      "Phase: train. Epoch: 22. Loss: 0.0778416246175766\n",
      "22 train 45 552\n",
      "Phase: train. Epoch: 22. Loss: 0.06079412251710892\n",
      "22 train 46 564\n",
      "Phase: train. Epoch: 22. Loss: 0.07718940079212189\n",
      "22 train 47 576\n",
      "Phase: train. Epoch: 22. Loss: 0.08711721003055573\n",
      "22 train 48 588\n",
      "Phase: train. Epoch: 22. Loss: 0.08315297961235046\n",
      "22 train 49 600\n",
      "Phase: train. Epoch: 22. Loss: 0.044825293123722076\n",
      "22 train 50 612\n",
      "Phase: train. Epoch: 22. Loss: 0.08927427232265472\n",
      "22 train 51 624\n",
      "Phase: train. Epoch: 22. Loss: 0.06985544413328171\n",
      "22 train 52 636\n",
      "Phase: train. Epoch: 22. Loss: 0.07668255269527435\n",
      "22 train 53 648\n",
      "Phase: train. Epoch: 22. Loss: 0.08521628379821777\n",
      "22 train 54 660\n",
      "Phase: train. Epoch: 22. Loss: 0.06317746639251709\n",
      "22 train 55 672\n",
      "Phase: train. Epoch: 22. Loss: 0.06507271528244019\n",
      "22 train 56 684\n",
      "Phase: train. Epoch: 22. Loss: 0.06406930088996887\n",
      "22 train 57 696\n",
      "Phase: train. Epoch: 22. Loss: 0.07154712826013565\n",
      "22 train 58 708\n",
      "Phase: train. Epoch: 22. Loss: 0.06080034002661705\n",
      "22 train 59 720\n",
      "Phase: train. Epoch: 22. Loss: 0.0706712156534195\n",
      "22 train 60 732\n",
      "Phase: train. Epoch: 22. Loss: 0.0873965471982956\n",
      "22 train 61 744\n",
      "Phase: train. Epoch: 22. Loss: 0.05053526163101196\n",
      "22 train 62 751\n",
      "Phase: train. Epoch: 22. Loss: 0.09837140142917633\n",
      "22 val 0 763\n",
      "Phase: val. Epoch: 22. Loss: 0.07447054982185364\n",
      "22 val 1 775\n",
      "Phase: val. Epoch: 22. Loss: 0.05265500396490097\n",
      "22 val 2 787\n",
      "Phase: val. Epoch: 22. Loss: 0.06531428545713425\n",
      "22 val 3 799\n",
      "Phase: val. Epoch: 22. Loss: 0.060728974640369415\n",
      "22 val 4 811\n",
      "Phase: val. Epoch: 22. Loss: 0.05175720155239105\n",
      "22 val 5 823\n",
      "Phase: val. Epoch: 22. Loss: 0.0895417332649231\n",
      "22 val 6 835\n",
      "Phase: val. Epoch: 22. Loss: 0.04489244148135185\n",
      "22 val 7 847\n",
      "Phase: val. Epoch: 22. Loss: 0.057584576308727264\n",
      "22 val 8 859\n",
      "Phase: val. Epoch: 22. Loss: 0.05577873811125755\n",
      "22 val 9 871\n",
      "Phase: val. Epoch: 22. Loss: 0.05890852212905884\n",
      "22 val 10 883\n",
      "Phase: val. Epoch: 22. Loss: 0.07138034701347351\n",
      "22 val 11 884\n",
      "Phase: val. Epoch: 22. Loss: 0.06288381665945053\n",
      "23 train 0 12\n",
      "Phase: train. Epoch: 23. Loss: 0.08826296031475067\n",
      "23 train 1 24\n",
      "Phase: train. Epoch: 23. Loss: 0.1031092032790184\n",
      "23 train 2 36\n",
      "Phase: train. Epoch: 23. Loss: 0.04636937379837036\n",
      "23 train 3 48\n",
      "Phase: train. Epoch: 23. Loss: 0.07332290709018707\n",
      "23 train 4 60\n",
      "Phase: train. Epoch: 23. Loss: 0.06805962324142456\n",
      "23 train 5 72\n",
      "Phase: train. Epoch: 23. Loss: 0.06864047050476074\n",
      "23 train 6 84\n",
      "Phase: train. Epoch: 23. Loss: 0.0559961162507534\n",
      "23 train 7 96\n",
      "Phase: train. Epoch: 23. Loss: 0.06790381669998169\n",
      "23 train 8 108\n",
      "Phase: train. Epoch: 23. Loss: 0.10464676469564438\n",
      "23 train 9 120\n",
      "Phase: train. Epoch: 23. Loss: 0.07456278800964355\n",
      "23 train 10 132\n",
      "Phase: train. Epoch: 23. Loss: 0.0675443634390831\n",
      "23 train 11 144\n",
      "Phase: train. Epoch: 23. Loss: 0.07924306392669678\n",
      "23 train 12 156\n",
      "Phase: train. Epoch: 23. Loss: 0.09110185503959656\n",
      "23 train 13 168\n",
      "Phase: train. Epoch: 23. Loss: 0.0882335901260376\n",
      "23 train 14 180\n",
      "Phase: train. Epoch: 23. Loss: 0.06304226815700531\n",
      "23 train 15 192\n",
      "Phase: train. Epoch: 23. Loss: 0.08744435012340546\n",
      "23 train 16 204\n",
      "Phase: train. Epoch: 23. Loss: 0.05148177593946457\n",
      "23 train 17 216\n",
      "Phase: train. Epoch: 23. Loss: 0.058053724467754364\n",
      "23 train 18 228\n",
      "Phase: train. Epoch: 23. Loss: 0.047210805118083954\n",
      "23 train 19 240\n",
      "Phase: train. Epoch: 23. Loss: 0.05679621547460556\n",
      "23 train 20 252\n",
      "Phase: train. Epoch: 23. Loss: 0.05023927241563797\n",
      "23 train 21 264\n",
      "Phase: train. Epoch: 23. Loss: 0.04485086724162102\n",
      "23 train 22 276\n",
      "Phase: train. Epoch: 23. Loss: 0.08077727258205414\n",
      "23 train 23 288\n",
      "Phase: train. Epoch: 23. Loss: 0.1023288369178772\n",
      "23 train 24 300\n",
      "Phase: train. Epoch: 23. Loss: 0.0631074458360672\n",
      "23 train 25 312\n",
      "Phase: train. Epoch: 23. Loss: 0.09797793626785278\n",
      "23 train 26 324\n",
      "Phase: train. Epoch: 23. Loss: 0.06818030774593353\n",
      "23 train 27 336\n",
      "Phase: train. Epoch: 23. Loss: 0.04448086768388748\n",
      "23 train 28 348\n",
      "Phase: train. Epoch: 23. Loss: 0.058872561901807785\n",
      "23 train 29 360\n",
      "Phase: train. Epoch: 23. Loss: 0.05915134772658348\n",
      "23 train 30 372\n",
      "Phase: train. Epoch: 23. Loss: 0.04898418113589287\n",
      "23 train 31 384\n",
      "Phase: train. Epoch: 23. Loss: 0.05284640192985535\n",
      "23 train 32 396\n",
      "Phase: train. Epoch: 23. Loss: 0.062197472900152206\n",
      "23 train 33 408\n",
      "Phase: train. Epoch: 23. Loss: 0.06082920357584953\n",
      "23 train 34 420\n",
      "Phase: train. Epoch: 23. Loss: 0.06060986965894699\n",
      "23 train 35 432\n",
      "Phase: train. Epoch: 23. Loss: 0.08565011620521545\n",
      "23 train 36 444\n",
      "Phase: train. Epoch: 23. Loss: 0.06309408694505692\n",
      "23 train 37 456\n",
      "Phase: train. Epoch: 23. Loss: 0.10233854502439499\n",
      "23 train 38 468\n",
      "Phase: train. Epoch: 23. Loss: 0.06649746745824814\n",
      "23 train 39 480\n",
      "Phase: train. Epoch: 23. Loss: 0.0640745759010315\n",
      "23 train 40 492\n",
      "Phase: train. Epoch: 23. Loss: 0.06732803583145142\n",
      "23 train 41 504\n",
      "Phase: train. Epoch: 23. Loss: 0.05678492784500122\n",
      "23 train 42 516\n",
      "Phase: train. Epoch: 23. Loss: 0.11191606521606445\n",
      "23 train 43 528\n",
      "Phase: train. Epoch: 23. Loss: 0.10563382506370544\n",
      "23 train 44 540\n",
      "Phase: train. Epoch: 23. Loss: 0.05342617630958557\n",
      "23 train 45 552\n",
      "Phase: train. Epoch: 23. Loss: 0.05403532832860947\n",
      "23 train 46 564\n",
      "Phase: train. Epoch: 23. Loss: 0.06047661229968071\n",
      "23 train 47 576\n",
      "Phase: train. Epoch: 23. Loss: 0.09017473459243774\n",
      "23 train 48 588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 23. Loss: 0.07367417216300964\n",
      "23 train 49 600\n",
      "Phase: train. Epoch: 23. Loss: 0.06575965136289597\n",
      "23 train 50 612\n",
      "Phase: train. Epoch: 23. Loss: 0.044133029878139496\n",
      "23 train 51 624\n",
      "Phase: train. Epoch: 23. Loss: 0.09801918268203735\n",
      "23 train 52 636\n",
      "Phase: train. Epoch: 23. Loss: 0.07655416429042816\n",
      "23 train 53 648\n",
      "Phase: train. Epoch: 23. Loss: 0.06680287420749664\n",
      "23 train 54 660\n",
      "Phase: train. Epoch: 23. Loss: 0.07746591418981552\n",
      "23 train 55 672\n",
      "Phase: train. Epoch: 23. Loss: 0.07234137505292892\n",
      "23 train 56 684\n",
      "Phase: train. Epoch: 23. Loss: 0.05418648570775986\n",
      "23 train 57 696\n",
      "Phase: train. Epoch: 23. Loss: 0.09850291907787323\n",
      "23 train 58 708\n",
      "Phase: train. Epoch: 23. Loss: 0.0716552585363388\n",
      "23 train 59 720\n",
      "Phase: train. Epoch: 23. Loss: 0.06018826365470886\n",
      "23 train 60 732\n",
      "Phase: train. Epoch: 23. Loss: 0.0490746796131134\n",
      "23 train 61 744\n",
      "Phase: train. Epoch: 23. Loss: 0.07389553636312485\n",
      "23 train 62 751\n",
      "Phase: train. Epoch: 23. Loss: 0.05391673371195793\n",
      "23 val 0 763\n",
      "Phase: val. Epoch: 23. Loss: 0.08214190602302551\n",
      "23 val 1 775\n",
      "Phase: val. Epoch: 23. Loss: 0.050511788576841354\n",
      "23 val 2 787\n",
      "Phase: val. Epoch: 23. Loss: 0.06479310989379883\n",
      "23 val 3 799\n",
      "Phase: val. Epoch: 23. Loss: 0.06043218821287155\n",
      "23 val 4 811\n",
      "Phase: val. Epoch: 23. Loss: 0.04948008060455322\n",
      "23 val 5 823\n",
      "Phase: val. Epoch: 23. Loss: 0.0608069971203804\n",
      "23 val 6 835\n",
      "Phase: val. Epoch: 23. Loss: 0.07063178718090057\n",
      "23 val 7 847\n",
      "Phase: val. Epoch: 23. Loss: 0.06204521656036377\n",
      "23 val 8 859\n",
      "Phase: val. Epoch: 23. Loss: 0.10096728801727295\n",
      "23 val 9 871\n",
      "Phase: val. Epoch: 23. Loss: 0.058078519999980927\n",
      "23 val 10 883\n",
      "Phase: val. Epoch: 23. Loss: 0.07120811939239502\n",
      "23 val 11 884\n",
      "Phase: val. Epoch: 23. Loss: 0.06272698938846588\n",
      "24 train 0 12\n",
      "Phase: train. Epoch: 24. Loss: 0.03844916820526123\n",
      "24 train 1 24\n",
      "Phase: train. Epoch: 24. Loss: 0.06707347929477692\n",
      "24 train 2 36\n",
      "Phase: train. Epoch: 24. Loss: 0.05608358234167099\n",
      "24 train 3 48\n",
      "Phase: train. Epoch: 24. Loss: 0.06091376766562462\n",
      "24 train 4 60\n",
      "Phase: train. Epoch: 24. Loss: 0.0893578976392746\n",
      "24 train 5 72\n",
      "Phase: train. Epoch: 24. Loss: 0.05048133060336113\n",
      "24 train 6 84\n",
      "Phase: train. Epoch: 24. Loss: 0.054465871304273605\n",
      "24 train 7 96\n",
      "Phase: train. Epoch: 24. Loss: 0.11129920184612274\n",
      "24 train 8 108\n",
      "Phase: train. Epoch: 24. Loss: 0.05876946076750755\n",
      "24 train 9 120\n",
      "Phase: train. Epoch: 24. Loss: 0.06717603653669357\n",
      "24 train 10 132\n",
      "Phase: train. Epoch: 24. Loss: 0.0778314396739006\n",
      "24 train 11 144\n",
      "Phase: train. Epoch: 24. Loss: 0.09271716326475143\n",
      "24 train 12 156\n",
      "Phase: train. Epoch: 24. Loss: 0.09131728857755661\n",
      "24 train 13 168\n",
      "Phase: train. Epoch: 24. Loss: 0.0703711062669754\n",
      "24 train 14 180\n",
      "Phase: train. Epoch: 24. Loss: 0.0878647193312645\n",
      "24 train 15 192\n",
      "Phase: train. Epoch: 24. Loss: 0.04754951596260071\n",
      "24 train 16 204\n",
      "Phase: train. Epoch: 24. Loss: 0.04461335390806198\n",
      "24 train 17 216\n",
      "Phase: train. Epoch: 24. Loss: 0.06462536752223969\n",
      "24 train 18 228\n",
      "Phase: train. Epoch: 24. Loss: 0.060792431235313416\n",
      "24 train 19 240\n",
      "Phase: train. Epoch: 24. Loss: 0.10861925780773163\n",
      "24 train 20 252\n",
      "Phase: train. Epoch: 24. Loss: 0.05396738648414612\n",
      "24 train 21 264\n",
      "Phase: train. Epoch: 24. Loss: 0.07818985730409622\n",
      "24 train 22 276\n",
      "Phase: train. Epoch: 24. Loss: 0.06641246378421783\n",
      "24 train 23 288\n",
      "Phase: train. Epoch: 24. Loss: 0.07070992887020111\n",
      "24 train 24 300\n",
      "Phase: train. Epoch: 24. Loss: 0.06163570284843445\n",
      "24 train 25 312\n",
      "Phase: train. Epoch: 24. Loss: 0.05453965812921524\n",
      "24 train 26 324\n",
      "Phase: train. Epoch: 24. Loss: 0.07846903055906296\n",
      "24 train 27 336\n",
      "Phase: train. Epoch: 24. Loss: 0.06497962772846222\n",
      "24 train 28 348\n",
      "Phase: train. Epoch: 24. Loss: 0.08928226679563522\n",
      "24 train 29 360\n",
      "Phase: train. Epoch: 24. Loss: 0.08809532225131989\n",
      "24 train 30 372\n",
      "Phase: train. Epoch: 24. Loss: 0.08491992950439453\n",
      "24 train 31 384\n",
      "Phase: train. Epoch: 24. Loss: 0.06404407322406769\n",
      "24 train 32 396\n",
      "Phase: train. Epoch: 24. Loss: 0.04414014145731926\n",
      "24 train 33 408\n",
      "Phase: train. Epoch: 24. Loss: 0.06928373873233795\n",
      "24 train 34 420\n",
      "Phase: train. Epoch: 24. Loss: 0.06137601286172867\n",
      "24 train 35 432\n",
      "Phase: train. Epoch: 24. Loss: 0.08470141142606735\n",
      "24 train 36 444\n",
      "Phase: train. Epoch: 24. Loss: 0.04660199582576752\n",
      "24 train 37 456\n",
      "Phase: train. Epoch: 24. Loss: 0.06722103804349899\n",
      "24 train 38 468\n",
      "Phase: train. Epoch: 24. Loss: 0.0720185935497284\n",
      "24 train 39 480\n",
      "Phase: train. Epoch: 24. Loss: 0.06826889514923096\n",
      "24 train 40 492\n",
      "Phase: train. Epoch: 24. Loss: 0.09857916831970215\n",
      "24 train 41 504\n",
      "Phase: train. Epoch: 24. Loss: 0.05814535915851593\n",
      "24 train 42 516\n",
      "Phase: train. Epoch: 24. Loss: 0.09358017146587372\n",
      "24 train 43 528\n",
      "Phase: train. Epoch: 24. Loss: 0.07755251228809357\n",
      "24 train 44 540\n",
      "Phase: train. Epoch: 24. Loss: 0.05167129635810852\n",
      "24 train 45 552\n",
      "Phase: train. Epoch: 24. Loss: 0.10213354229927063\n",
      "24 train 46 564\n",
      "Phase: train. Epoch: 24. Loss: 0.07201331853866577\n",
      "24 train 47 576\n",
      "Phase: train. Epoch: 24. Loss: 0.06345672905445099\n",
      "24 train 48 588\n",
      "Phase: train. Epoch: 24. Loss: 0.08264952898025513\n",
      "24 train 49 600\n",
      "Phase: train. Epoch: 24. Loss: 0.047970741987228394\n",
      "24 train 50 612\n",
      "Phase: train. Epoch: 24. Loss: 0.07982929050922394\n",
      "24 train 51 624\n",
      "Phase: train. Epoch: 24. Loss: 0.07066436111927032\n",
      "24 train 52 636\n",
      "Phase: train. Epoch: 24. Loss: 0.085874542593956\n",
      "24 train 53 648\n",
      "Phase: train. Epoch: 24. Loss: 0.09588887542486191\n",
      "24 train 54 660\n",
      "Phase: train. Epoch: 24. Loss: 0.05664897710084915\n",
      "24 train 55 672\n",
      "Phase: train. Epoch: 24. Loss: 0.04201593995094299\n",
      "24 train 56 684\n",
      "Phase: train. Epoch: 24. Loss: 0.09083060920238495\n",
      "24 train 57 696\n",
      "Phase: train. Epoch: 24. Loss: 0.06218259036540985\n",
      "24 train 58 708\n",
      "Phase: train. Epoch: 24. Loss: 0.07434049248695374\n",
      "24 train 59 720\n",
      "Phase: train. Epoch: 24. Loss: 0.06314585357904434\n",
      "24 train 60 732\n",
      "Phase: train. Epoch: 24. Loss: 0.09121830761432648\n",
      "24 train 61 744\n",
      "Phase: train. Epoch: 24. Loss: 0.07368438690900803\n",
      "24 train 62 751\n",
      "Phase: train. Epoch: 24. Loss: 0.04734902083873749\n",
      "24 val 0 763\n",
      "Phase: val. Epoch: 24. Loss: 0.04802742600440979\n",
      "24 val 1 775\n",
      "Phase: val. Epoch: 24. Loss: 0.07122880220413208\n",
      "24 val 2 787\n",
      "Phase: val. Epoch: 24. Loss: 0.07406337559223175\n",
      "24 val 3 799\n",
      "Phase: val. Epoch: 24. Loss: 0.046844255179166794\n",
      "24 val 4 811\n",
      "Phase: val. Epoch: 24. Loss: 0.08653008937835693\n",
      "24 val 5 823\n",
      "Phase: val. Epoch: 24. Loss: 0.04768037796020508\n",
      "24 val 6 835\n",
      "Phase: val. Epoch: 24. Loss: 0.0887155756354332\n",
      "24 val 7 847\n",
      "Phase: val. Epoch: 24. Loss: 0.04804859310388565\n",
      "24 val 8 859\n",
      "Phase: val. Epoch: 24. Loss: 0.0691528245806694\n",
      "24 val 9 871\n",
      "Phase: val. Epoch: 24. Loss: 0.06670864671468735\n",
      "24 val 10 883\n",
      "Phase: val. Epoch: 24. Loss: 0.06835062801837921\n",
      "24 val 11 884\n",
      "Phase: val. Epoch: 24. Loss: 0.022785700857639313\n",
      "25 train 0 12\n",
      "Phase: train. Epoch: 25. Loss: 0.08873041719198227\n",
      "25 train 1 24\n",
      "Phase: train. Epoch: 25. Loss: 0.07241211831569672\n",
      "25 train 2 36\n",
      "Phase: train. Epoch: 25. Loss: 0.06183250993490219\n",
      "25 train 3 48\n",
      "Phase: train. Epoch: 25. Loss: 0.08969680964946747\n",
      "25 train 4 60\n",
      "Phase: train. Epoch: 25. Loss: 0.0954793244600296\n",
      "25 train 5 72\n",
      "Phase: train. Epoch: 25. Loss: 0.08251487463712692\n",
      "25 train 6 84\n",
      "Phase: train. Epoch: 25. Loss: 0.09006066620349884\n",
      "25 train 7 96\n",
      "Phase: train. Epoch: 25. Loss: 0.060408663004636765\n",
      "25 train 8 108\n",
      "Phase: train. Epoch: 25. Loss: 0.07980228960514069\n",
      "25 train 9 120\n",
      "Phase: train. Epoch: 25. Loss: 0.05569864809513092\n",
      "25 train 10 132\n",
      "Phase: train. Epoch: 25. Loss: 0.07510001212358475\n",
      "25 train 11 144\n",
      "Phase: train. Epoch: 25. Loss: 0.06423868238925934\n",
      "25 train 12 156\n",
      "Phase: train. Epoch: 25. Loss: 0.05233606696128845\n",
      "25 train 13 168\n",
      "Phase: train. Epoch: 25. Loss: 0.08417601883411407\n",
      "25 train 14 180\n",
      "Phase: train. Epoch: 25. Loss: 0.06777320802211761\n",
      "25 train 15 192\n",
      "Phase: train. Epoch: 25. Loss: 0.08022309094667435\n",
      "25 train 16 204\n",
      "Phase: train. Epoch: 25. Loss: 0.06609465181827545\n",
      "25 train 17 216\n",
      "Phase: train. Epoch: 25. Loss: 0.06284814327955246\n",
      "25 train 18 228\n",
      "Phase: train. Epoch: 25. Loss: 0.07896211743354797\n",
      "25 train 19 240\n",
      "Phase: train. Epoch: 25. Loss: 0.06737911701202393\n",
      "25 train 20 252\n",
      "Phase: train. Epoch: 25. Loss: 0.06325370073318481\n",
      "25 train 21 264\n",
      "Phase: train. Epoch: 25. Loss: 0.09156762063503265\n",
      "25 train 22 276\n",
      "Phase: train. Epoch: 25. Loss: 0.09209266304969788\n",
      "25 train 23 288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 25. Loss: 0.04807230085134506\n",
      "25 train 24 300\n",
      "Phase: train. Epoch: 25. Loss: 0.06763383746147156\n",
      "25 train 25 312\n",
      "Phase: train. Epoch: 25. Loss: 0.06303407996892929\n",
      "25 train 26 324\n",
      "Phase: train. Epoch: 25. Loss: 0.058296628296375275\n",
      "25 train 27 336\n",
      "Phase: train. Epoch: 25. Loss: 0.08223018050193787\n",
      "25 train 28 348\n",
      "Phase: train. Epoch: 25. Loss: 0.06716524064540863\n",
      "25 train 29 360\n",
      "Phase: train. Epoch: 25. Loss: 0.061940357089042664\n",
      "25 train 30 372\n",
      "Phase: train. Epoch: 25. Loss: 0.041821204125881195\n",
      "25 train 31 384\n",
      "Phase: train. Epoch: 25. Loss: 0.047463178634643555\n",
      "25 train 32 396\n",
      "Phase: train. Epoch: 25. Loss: 0.06976010650396347\n",
      "25 train 33 408\n",
      "Phase: train. Epoch: 25. Loss: 0.059627145528793335\n",
      "25 train 34 420\n",
      "Phase: train. Epoch: 25. Loss: 0.053575873374938965\n",
      "25 train 35 432\n",
      "Phase: train. Epoch: 25. Loss: 0.10460972040891647\n",
      "25 train 36 444\n",
      "Phase: train. Epoch: 25. Loss: 0.09314149618148804\n",
      "25 train 37 456\n",
      "Phase: train. Epoch: 25. Loss: 0.04855339229106903\n",
      "25 train 38 468\n",
      "Phase: train. Epoch: 25. Loss: 0.0959581583738327\n",
      "25 train 39 480\n",
      "Phase: train. Epoch: 25. Loss: 0.052155714482069016\n",
      "25 train 40 492\n",
      "Phase: train. Epoch: 25. Loss: 0.073281429708004\n",
      "25 train 41 504\n",
      "Phase: train. Epoch: 25. Loss: 0.0441548153758049\n",
      "25 train 42 516\n",
      "Phase: train. Epoch: 25. Loss: 0.048295170068740845\n",
      "25 train 43 528\n",
      "Phase: train. Epoch: 25. Loss: 0.09823630750179291\n",
      "25 train 44 540\n",
      "Phase: train. Epoch: 25. Loss: 0.07920309156179428\n",
      "25 train 45 552\n",
      "Phase: train. Epoch: 25. Loss: 0.052540551871061325\n",
      "25 train 46 564\n",
      "Phase: train. Epoch: 25. Loss: 0.0562388114631176\n",
      "25 train 47 576\n",
      "Phase: train. Epoch: 25. Loss: 0.0846996158361435\n",
      "25 train 48 588\n",
      "Phase: train. Epoch: 25. Loss: 0.053447552025318146\n",
      "25 train 49 600\n",
      "Phase: train. Epoch: 25. Loss: 0.09928722679615021\n",
      "25 train 50 612\n",
      "Phase: train. Epoch: 25. Loss: 0.06564074754714966\n",
      "25 train 51 624\n",
      "Phase: train. Epoch: 25. Loss: 0.059244297444820404\n",
      "25 train 52 636\n",
      "Phase: train. Epoch: 25. Loss: 0.07513856142759323\n",
      "25 train 53 648\n",
      "Phase: train. Epoch: 25. Loss: 0.08721506595611572\n",
      "25 train 54 660\n",
      "Phase: train. Epoch: 25. Loss: 0.06333913654088974\n",
      "25 train 55 672\n",
      "Phase: train. Epoch: 25. Loss: 0.05232032388448715\n",
      "25 train 56 684\n",
      "Phase: train. Epoch: 25. Loss: 0.042070381343364716\n",
      "25 train 57 696\n",
      "Phase: train. Epoch: 25. Loss: 0.05674070119857788\n",
      "25 train 58 708\n",
      "Phase: train. Epoch: 25. Loss: 0.08068175613880157\n",
      "25 train 59 720\n",
      "Phase: train. Epoch: 25. Loss: 0.04319719597697258\n",
      "25 train 60 732\n",
      "Phase: train. Epoch: 25. Loss: 0.0803118497133255\n",
      "25 train 61 744\n",
      "Phase: train. Epoch: 25. Loss: 0.04781251400709152\n",
      "25 train 62 751\n",
      "Phase: train. Epoch: 25. Loss: 0.08279512822628021\n",
      "25 val 0 763\n",
      "Phase: val. Epoch: 25. Loss: 0.06828022003173828\n",
      "25 val 1 775\n",
      "Phase: val. Epoch: 25. Loss: 0.05655679851770401\n",
      "25 val 2 787\n",
      "Phase: val. Epoch: 25. Loss: 0.04325641319155693\n",
      "25 val 3 799\n",
      "Phase: val. Epoch: 25. Loss: 0.06541841477155685\n",
      "25 val 4 811\n",
      "Phase: val. Epoch: 25. Loss: 0.0859559029340744\n",
      "25 val 5 823\n",
      "Phase: val. Epoch: 25. Loss: 0.033076636493206024\n",
      "25 val 6 835\n",
      "Phase: val. Epoch: 25. Loss: 0.05181218311190605\n",
      "25 val 7 847\n",
      "Phase: val. Epoch: 25. Loss: 0.09826710820198059\n",
      "25 val 8 859\n",
      "Phase: val. Epoch: 25. Loss: 0.06689845770597458\n",
      "25 val 9 871\n",
      "Phase: val. Epoch: 25. Loss: 0.093589186668396\n",
      "25 val 10 883\n",
      "Phase: val. Epoch: 25. Loss: 0.041683491319417953\n",
      "25 val 11 884\n",
      "Phase: val. Epoch: 25. Loss: 0.08274178206920624\n",
      "26 train 0 12\n",
      "Phase: train. Epoch: 26. Loss: 0.08712649345397949\n",
      "26 train 1 24\n",
      "Phase: train. Epoch: 26. Loss: 0.07398239523172379\n",
      "26 train 2 36\n",
      "Phase: train. Epoch: 26. Loss: 0.10158108174800873\n",
      "26 train 3 48\n",
      "Phase: train. Epoch: 26. Loss: 0.0815601497888565\n",
      "26 train 4 60\n",
      "Phase: train. Epoch: 26. Loss: 0.049896489828825\n",
      "26 train 5 72\n",
      "Phase: train. Epoch: 26. Loss: 0.05862302705645561\n",
      "26 train 6 84\n",
      "Phase: train. Epoch: 26. Loss: 0.05786406621336937\n",
      "26 train 7 96\n",
      "Phase: train. Epoch: 26. Loss: 0.04703798145055771\n",
      "26 train 8 108\n",
      "Phase: train. Epoch: 26. Loss: 0.07865547388792038\n",
      "26 train 9 120\n",
      "Phase: train. Epoch: 26. Loss: 0.07195533812046051\n",
      "26 train 10 132\n",
      "Phase: train. Epoch: 26. Loss: 0.04617833346128464\n",
      "26 train 11 144\n",
      "Phase: train. Epoch: 26. Loss: 0.08371364325284958\n",
      "26 train 12 156\n",
      "Phase: train. Epoch: 26. Loss: 0.05951656401157379\n",
      "26 train 13 168\n",
      "Phase: train. Epoch: 26. Loss: 0.06220709905028343\n",
      "26 train 14 180\n",
      "Phase: train. Epoch: 26. Loss: 0.0806792601943016\n",
      "26 train 15 192\n",
      "Phase: train. Epoch: 26. Loss: 0.043508511036634445\n",
      "26 train 16 204\n",
      "Phase: train. Epoch: 26. Loss: 0.09439383447170258\n",
      "26 train 17 216\n",
      "Phase: train. Epoch: 26. Loss: 0.08076059073209763\n",
      "26 train 18 228\n",
      "Phase: train. Epoch: 26. Loss: 0.12807372212409973\n",
      "26 train 19 240\n",
      "Phase: train. Epoch: 26. Loss: 0.054849009960889816\n",
      "26 train 20 252\n",
      "Phase: train. Epoch: 26. Loss: 0.038483574986457825\n",
      "26 train 21 264\n",
      "Phase: train. Epoch: 26. Loss: 0.08598953485488892\n",
      "26 train 22 276\n",
      "Phase: train. Epoch: 26. Loss: 0.07410161197185516\n",
      "26 train 23 288\n",
      "Phase: train. Epoch: 26. Loss: 0.086408331990242\n",
      "26 train 24 300\n",
      "Phase: train. Epoch: 26. Loss: 0.0598439984023571\n",
      "26 train 25 312\n",
      "Phase: train. Epoch: 26. Loss: 0.07053737342357635\n",
      "26 train 26 324\n",
      "Phase: train. Epoch: 26. Loss: 0.07189653068780899\n",
      "26 train 27 336\n",
      "Phase: train. Epoch: 26. Loss: 0.052607372403144836\n",
      "26 train 28 348\n",
      "Phase: train. Epoch: 26. Loss: 0.06540732085704803\n",
      "26 train 29 360\n",
      "Phase: train. Epoch: 26. Loss: 0.06296515464782715\n",
      "26 train 30 372\n",
      "Phase: train. Epoch: 26. Loss: 0.05871353670954704\n",
      "26 train 31 384\n",
      "Phase: train. Epoch: 26. Loss: 0.07253845036029816\n",
      "26 train 32 396\n",
      "Phase: train. Epoch: 26. Loss: 0.045865416526794434\n",
      "26 train 33 408\n",
      "Phase: train. Epoch: 26. Loss: 0.05343262851238251\n",
      "26 train 34 420\n",
      "Phase: train. Epoch: 26. Loss: 0.06845929473638535\n",
      "26 train 35 432\n",
      "Phase: train. Epoch: 26. Loss: 0.06056592985987663\n",
      "26 train 36 444\n",
      "Phase: train. Epoch: 26. Loss: 0.07159975171089172\n",
      "26 train 37 456\n",
      "Phase: train. Epoch: 26. Loss: 0.04455193504691124\n",
      "26 train 38 468\n",
      "Phase: train. Epoch: 26. Loss: 0.07264222949743271\n",
      "26 train 39 480\n",
      "Phase: train. Epoch: 26. Loss: 0.07634110003709793\n",
      "26 train 40 492\n",
      "Phase: train. Epoch: 26. Loss: 0.04802747070789337\n",
      "26 train 41 504\n",
      "Phase: train. Epoch: 26. Loss: 0.06449242681264877\n",
      "26 train 42 516\n",
      "Phase: train. Epoch: 26. Loss: 0.06425908207893372\n",
      "26 train 43 528\n",
      "Phase: train. Epoch: 26. Loss: 0.09095953404903412\n",
      "26 train 44 540\n",
      "Phase: train. Epoch: 26. Loss: 0.06861533969640732\n",
      "26 train 45 552\n",
      "Phase: train. Epoch: 26. Loss: 0.06677179038524628\n",
      "26 train 46 564\n",
      "Phase: train. Epoch: 26. Loss: 0.054802656173706055\n",
      "26 train 47 576\n",
      "Phase: train. Epoch: 26. Loss: 0.05970069020986557\n",
      "26 train 48 588\n",
      "Phase: train. Epoch: 26. Loss: 0.09041403979063034\n",
      "26 train 49 600\n",
      "Phase: train. Epoch: 26. Loss: 0.0698574036359787\n",
      "26 train 50 612\n",
      "Phase: train. Epoch: 26. Loss: 0.08829470723867416\n",
      "26 train 51 624\n",
      "Phase: train. Epoch: 26. Loss: 0.09189822524785995\n",
      "26 train 52 636\n",
      "Phase: train. Epoch: 26. Loss: 0.06237372010946274\n",
      "26 train 53 648\n",
      "Phase: train. Epoch: 26. Loss: 0.07095906138420105\n",
      "26 train 54 660\n",
      "Phase: train. Epoch: 26. Loss: 0.07262808829545975\n",
      "26 train 55 672\n",
      "Phase: train. Epoch: 26. Loss: 0.05002879723906517\n",
      "26 train 56 684\n",
      "Phase: train. Epoch: 26. Loss: 0.11348003149032593\n",
      "26 train 57 696\n",
      "Phase: train. Epoch: 26. Loss: 0.07430464029312134\n",
      "26 train 58 708\n",
      "Phase: train. Epoch: 26. Loss: 0.06639806926250458\n",
      "26 train 59 720\n",
      "Phase: train. Epoch: 26. Loss: 0.07107656449079514\n",
      "26 train 60 732\n",
      "Phase: train. Epoch: 26. Loss: 0.06888023018836975\n",
      "26 train 61 744\n",
      "Phase: train. Epoch: 26. Loss: 0.07600146532058716\n",
      "26 train 62 751\n",
      "Phase: train. Epoch: 26. Loss: 0.11476927995681763\n",
      "26 val 0 763\n",
      "Phase: val. Epoch: 26. Loss: 0.0679565891623497\n",
      "26 val 1 775\n",
      "Phase: val. Epoch: 26. Loss: 0.0669672042131424\n",
      "26 val 2 787\n",
      "Phase: val. Epoch: 26. Loss: 0.05019703507423401\n",
      "26 val 3 799\n",
      "Phase: val. Epoch: 26. Loss: 0.07089881598949432\n",
      "26 val 4 811\n",
      "Phase: val. Epoch: 26. Loss: 0.08457402884960175\n",
      "26 val 5 823\n",
      "Phase: val. Epoch: 26. Loss: 0.07087250798940659\n",
      "26 val 6 835\n",
      "Phase: val. Epoch: 26. Loss: 0.04747632145881653\n",
      "26 val 7 847\n",
      "Phase: val. Epoch: 26. Loss: 0.08430793881416321\n",
      "26 val 8 859\n",
      "Phase: val. Epoch: 26. Loss: 0.0679403692483902\n",
      "26 val 9 871\n",
      "Phase: val. Epoch: 26. Loss: 0.06360864639282227\n",
      "26 val 10 883\n",
      "Phase: val. Epoch: 26. Loss: 0.03324601799249649\n",
      "26 val 11 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 26. Loss: 0.006644359789788723\n",
      "27 train 0 12\n",
      "Phase: train. Epoch: 27. Loss: 0.05165368318557739\n",
      "27 train 1 24\n",
      "Phase: train. Epoch: 27. Loss: 0.0735461413860321\n",
      "27 train 2 36\n",
      "Phase: train. Epoch: 27. Loss: 0.06467084586620331\n",
      "27 train 3 48\n",
      "Phase: train. Epoch: 27. Loss: 0.05845966935157776\n",
      "27 train 4 60\n",
      "Phase: train. Epoch: 27. Loss: 0.0602521114051342\n",
      "27 train 5 72\n",
      "Phase: train. Epoch: 27. Loss: 0.0672660693526268\n",
      "27 train 6 84\n",
      "Phase: train. Epoch: 27. Loss: 0.08881829679012299\n",
      "27 train 7 96\n",
      "Phase: train. Epoch: 27. Loss: 0.0508088544011116\n",
      "27 train 8 108\n",
      "Phase: train. Epoch: 27. Loss: 0.060354918241500854\n",
      "27 train 9 120\n",
      "Phase: train. Epoch: 27. Loss: 0.06501320004463196\n",
      "27 train 10 132\n",
      "Phase: train. Epoch: 27. Loss: 0.07580132782459259\n",
      "27 train 11 144\n",
      "Phase: train. Epoch: 27. Loss: 0.05650564283132553\n",
      "27 train 12 156\n",
      "Phase: train. Epoch: 27. Loss: 0.057143114507198334\n",
      "27 train 13 168\n",
      "Phase: train. Epoch: 27. Loss: 0.06704957783222198\n",
      "27 train 14 180\n",
      "Phase: train. Epoch: 27. Loss: 0.07446113228797913\n",
      "27 train 15 192\n",
      "Phase: train. Epoch: 27. Loss: 0.060496460646390915\n",
      "27 train 16 204\n",
      "Phase: train. Epoch: 27. Loss: 0.08659616112709045\n",
      "27 train 17 216\n",
      "Phase: train. Epoch: 27. Loss: 0.06825676560401917\n",
      "27 train 18 228\n",
      "Phase: train. Epoch: 27. Loss: 0.06322478502988815\n",
      "27 train 19 240\n",
      "Phase: train. Epoch: 27. Loss: 0.04857867956161499\n",
      "27 train 20 252\n",
      "Phase: train. Epoch: 27. Loss: 0.056953977793455124\n",
      "27 train 21 264\n",
      "Phase: train. Epoch: 27. Loss: 0.037167277187108994\n",
      "27 train 22 276\n",
      "Phase: train. Epoch: 27. Loss: 0.07169775664806366\n",
      "27 train 23 288\n",
      "Phase: train. Epoch: 27. Loss: 0.0645955502986908\n",
      "27 train 24 300\n",
      "Phase: train. Epoch: 27. Loss: 0.08189820498228073\n",
      "27 train 25 312\n",
      "Phase: train. Epoch: 27. Loss: 0.058196693658828735\n",
      "27 train 26 324\n",
      "Phase: train. Epoch: 27. Loss: 0.08631420135498047\n",
      "27 train 27 336\n",
      "Phase: train. Epoch: 27. Loss: 0.08898155391216278\n",
      "27 train 28 348\n",
      "Phase: train. Epoch: 27. Loss: 0.09178700298070908\n",
      "27 train 29 360\n",
      "Phase: train. Epoch: 27. Loss: 0.062414467334747314\n",
      "27 train 30 372\n",
      "Phase: train. Epoch: 27. Loss: 0.05632663518190384\n",
      "27 train 31 384\n",
      "Phase: train. Epoch: 27. Loss: 0.07161065191030502\n",
      "27 train 32 396\n",
      "Phase: train. Epoch: 27. Loss: 0.0507490336894989\n",
      "27 train 33 408\n",
      "Phase: train. Epoch: 27. Loss: 0.056407630443573\n",
      "27 train 34 420\n",
      "Phase: train. Epoch: 27. Loss: 0.07568643987178802\n",
      "27 train 35 432\n",
      "Phase: train. Epoch: 27. Loss: 0.07077620923519135\n",
      "27 train 36 444\n",
      "Phase: train. Epoch: 27. Loss: 0.08086735010147095\n",
      "27 train 37 456\n",
      "Phase: train. Epoch: 27. Loss: 0.07071797549724579\n",
      "27 train 38 468\n",
      "Phase: train. Epoch: 27. Loss: 0.0440128818154335\n",
      "27 train 39 480\n",
      "Phase: train. Epoch: 27. Loss: 0.0708627924323082\n",
      "27 train 40 492\n",
      "Phase: train. Epoch: 27. Loss: 0.09359189122915268\n",
      "27 train 41 504\n",
      "Phase: train. Epoch: 27. Loss: 0.08661868423223495\n",
      "27 train 42 516\n",
      "Phase: train. Epoch: 27. Loss: 0.08760427683591843\n",
      "27 train 43 528\n",
      "Phase: train. Epoch: 27. Loss: 0.05259425565600395\n",
      "27 train 44 540\n",
      "Phase: train. Epoch: 27. Loss: 0.06227099522948265\n",
      "27 train 45 552\n",
      "Phase: train. Epoch: 27. Loss: 0.06014266237616539\n",
      "27 train 46 564\n",
      "Phase: train. Epoch: 27. Loss: 0.07283191382884979\n",
      "27 train 47 576\n",
      "Phase: train. Epoch: 27. Loss: 0.08495309203863144\n",
      "27 train 48 588\n",
      "Phase: train. Epoch: 27. Loss: 0.09981118142604828\n",
      "27 train 49 600\n",
      "Phase: train. Epoch: 27. Loss: 0.05279011279344559\n",
      "27 train 50 612\n",
      "Phase: train. Epoch: 27. Loss: 0.08336395025253296\n",
      "27 train 51 624\n",
      "Phase: train. Epoch: 27. Loss: 0.0656813234090805\n",
      "27 train 52 636\n",
      "Phase: train. Epoch: 27. Loss: 0.0627124160528183\n",
      "27 train 53 648\n",
      "Phase: train. Epoch: 27. Loss: 0.07476744055747986\n",
      "27 train 54 660\n",
      "Phase: train. Epoch: 27. Loss: 0.0534496009349823\n",
      "27 train 55 672\n",
      "Phase: train. Epoch: 27. Loss: 0.060437947511672974\n",
      "27 train 56 684\n",
      "Phase: train. Epoch: 27. Loss: 0.07031285762786865\n",
      "27 train 57 696\n",
      "Phase: train. Epoch: 27. Loss: 0.04340001195669174\n",
      "27 train 58 708\n",
      "Phase: train. Epoch: 27. Loss: 0.07647484540939331\n",
      "27 train 59 720\n",
      "Phase: train. Epoch: 27. Loss: 0.07801046967506409\n",
      "27 train 60 732\n",
      "Phase: train. Epoch: 27. Loss: 0.06661076843738556\n",
      "27 train 61 744\n",
      "Phase: train. Epoch: 27. Loss: 0.05713116377592087\n",
      "27 train 62 751\n",
      "Phase: train. Epoch: 27. Loss: 0.043482858687639236\n",
      "27 val 0 763\n",
      "Phase: val. Epoch: 27. Loss: 0.06174233555793762\n",
      "27 val 1 775\n",
      "Phase: val. Epoch: 27. Loss: 0.05115492641925812\n",
      "27 val 2 787\n",
      "Phase: val. Epoch: 27. Loss: 0.07237248867750168\n",
      "27 val 3 799\n",
      "Phase: val. Epoch: 27. Loss: 0.06791609525680542\n",
      "27 val 4 811\n",
      "Phase: val. Epoch: 27. Loss: 0.04549556225538254\n",
      "27 val 5 823\n",
      "Phase: val. Epoch: 27. Loss: 0.07931408286094666\n",
      "27 val 6 835\n",
      "Phase: val. Epoch: 27. Loss: 0.08960571885108948\n",
      "27 val 7 847\n",
      "Phase: val. Epoch: 27. Loss: 0.05678287893533707\n",
      "27 val 8 859\n",
      "Phase: val. Epoch: 27. Loss: 0.08515703678131104\n",
      "27 val 9 871\n",
      "Phase: val. Epoch: 27. Loss: 0.067208431661129\n",
      "27 val 10 883\n",
      "Phase: val. Epoch: 27. Loss: 0.0318416953086853\n",
      "27 val 11 884\n",
      "Phase: val. Epoch: 27. Loss: 0.03536500781774521\n",
      "28 train 0 12\n",
      "Phase: train. Epoch: 28. Loss: 0.07431058585643768\n",
      "28 train 1 24\n",
      "Phase: train. Epoch: 28. Loss: 0.06656147539615631\n",
      "28 train 2 36\n",
      "Phase: train. Epoch: 28. Loss: 0.08769243210554123\n",
      "28 train 3 48\n",
      "Phase: train. Epoch: 28. Loss: 0.04758492857217789\n",
      "28 train 4 60\n",
      "Phase: train. Epoch: 28. Loss: 0.059111446142196655\n",
      "28 train 5 72\n",
      "Phase: train. Epoch: 28. Loss: 0.059094663709402084\n",
      "28 train 6 84\n",
      "Phase: train. Epoch: 28. Loss: 0.058731596916913986\n",
      "28 train 7 96\n",
      "Phase: train. Epoch: 28. Loss: 0.0641556978225708\n",
      "28 train 8 108\n",
      "Phase: train. Epoch: 28. Loss: 0.07003413140773773\n",
      "28 train 9 120\n",
      "Phase: train. Epoch: 28. Loss: 0.04568066820502281\n",
      "28 train 10 132\n",
      "Phase: train. Epoch: 28. Loss: 0.06187298148870468\n",
      "28 train 11 144\n",
      "Phase: train. Epoch: 28. Loss: 0.055534183979034424\n",
      "28 train 12 156\n",
      "Phase: train. Epoch: 28. Loss: 0.04739903658628464\n",
      "28 train 13 168\n",
      "Phase: train. Epoch: 28. Loss: 0.057563479989767075\n",
      "28 train 14 180\n",
      "Phase: train. Epoch: 28. Loss: 0.07471685111522675\n",
      "28 train 15 192\n",
      "Phase: train. Epoch: 28. Loss: 0.05775776505470276\n",
      "28 train 16 204\n",
      "Phase: train. Epoch: 28. Loss: 0.07209423184394836\n",
      "28 train 17 216\n",
      "Phase: train. Epoch: 28. Loss: 0.10145565122365952\n",
      "28 train 18 228\n",
      "Phase: train. Epoch: 28. Loss: 0.0729566141963005\n",
      "28 train 19 240\n",
      "Phase: train. Epoch: 28. Loss: 0.06644511222839355\n",
      "28 train 20 252\n",
      "Phase: train. Epoch: 28. Loss: 0.042265765368938446\n",
      "28 train 21 264\n",
      "Phase: train. Epoch: 28. Loss: 0.07771129161119461\n",
      "28 train 22 276\n",
      "Phase: train. Epoch: 28. Loss: 0.051619064062833786\n",
      "28 train 23 288\n",
      "Phase: train. Epoch: 28. Loss: 0.05720846354961395\n",
      "28 train 24 300\n",
      "Phase: train. Epoch: 28. Loss: 0.0750705823302269\n",
      "28 train 25 312\n",
      "Phase: train. Epoch: 28. Loss: 0.09627040475606918\n",
      "28 train 26 324\n",
      "Phase: train. Epoch: 28. Loss: 0.08900947868824005\n",
      "28 train 27 336\n",
      "Phase: train. Epoch: 28. Loss: 0.10045900195837021\n",
      "28 train 28 348\n",
      "Phase: train. Epoch: 28. Loss: 0.10849158465862274\n",
      "28 train 29 360\n",
      "Phase: train. Epoch: 28. Loss: 0.05034811794757843\n",
      "28 train 30 372\n",
      "Phase: train. Epoch: 28. Loss: 0.047313909977674484\n",
      "28 train 31 384\n",
      "Phase: train. Epoch: 28. Loss: 0.0610041469335556\n",
      "28 train 32 396\n",
      "Phase: train. Epoch: 28. Loss: 0.07920509576797485\n",
      "28 train 33 408\n",
      "Phase: train. Epoch: 28. Loss: 0.0604751817882061\n",
      "28 train 34 420\n",
      "Phase: train. Epoch: 28. Loss: 0.05933921039104462\n",
      "28 train 35 432\n",
      "Phase: train. Epoch: 28. Loss: 0.04978688806295395\n",
      "28 train 36 444\n",
      "Phase: train. Epoch: 28. Loss: 0.050932377576828\n",
      "28 train 37 456\n",
      "Phase: train. Epoch: 28. Loss: 0.08310946077108383\n",
      "28 train 38 468\n",
      "Phase: train. Epoch: 28. Loss: 0.0756276398897171\n",
      "28 train 39 480\n",
      "Phase: train. Epoch: 28. Loss: 0.07087709754705429\n",
      "28 train 40 492\n",
      "Phase: train. Epoch: 28. Loss: 0.03630658984184265\n",
      "28 train 41 504\n",
      "Phase: train. Epoch: 28. Loss: 0.06414054334163666\n",
      "28 train 42 516\n",
      "Phase: train. Epoch: 28. Loss: 0.09796818345785141\n",
      "28 train 43 528\n",
      "Phase: train. Epoch: 28. Loss: 0.055634211748838425\n",
      "28 train 44 540\n",
      "Phase: train. Epoch: 28. Loss: 0.047314804047346115\n",
      "28 train 45 552\n",
      "Phase: train. Epoch: 28. Loss: 0.06605200469493866\n",
      "28 train 46 564\n",
      "Phase: train. Epoch: 28. Loss: 0.06970762461423874\n",
      "28 train 47 576\n",
      "Phase: train. Epoch: 28. Loss: 0.08995261788368225\n",
      "28 train 48 588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 28. Loss: 0.04109048843383789\n",
      "28 train 49 600\n",
      "Phase: train. Epoch: 28. Loss: 0.039904676377773285\n",
      "28 train 50 612\n",
      "Phase: train. Epoch: 28. Loss: 0.06419073045253754\n",
      "28 train 51 624\n",
      "Phase: train. Epoch: 28. Loss: 0.08153177797794342\n",
      "28 train 52 636\n",
      "Phase: train. Epoch: 28. Loss: 0.03668639063835144\n",
      "28 train 53 648\n",
      "Phase: train. Epoch: 28. Loss: 0.058370113372802734\n",
      "28 train 54 660\n",
      "Phase: train. Epoch: 28. Loss: 0.07097214460372925\n",
      "28 train 55 672\n",
      "Phase: train. Epoch: 28. Loss: 0.07725609838962555\n",
      "28 train 56 684\n",
      "Phase: train. Epoch: 28. Loss: 0.06215537711977959\n",
      "28 train 57 696\n",
      "Phase: train. Epoch: 28. Loss: 0.06190554425120354\n",
      "28 train 58 708\n",
      "Phase: train. Epoch: 28. Loss: 0.06898849457502365\n",
      "28 train 59 720\n",
      "Phase: train. Epoch: 28. Loss: 0.05853737145662308\n",
      "28 train 60 732\n",
      "Phase: train. Epoch: 28. Loss: 0.07706323266029358\n",
      "28 train 61 744\n",
      "Phase: train. Epoch: 28. Loss: 0.06552331894636154\n",
      "28 train 62 751\n",
      "Phase: train. Epoch: 28. Loss: 0.046028751879930496\n",
      "28 val 0 763\n",
      "Phase: val. Epoch: 28. Loss: 0.06056547164916992\n",
      "28 val 1 775\n",
      "Phase: val. Epoch: 28. Loss: 0.08309992402791977\n",
      "28 val 2 787\n",
      "Phase: val. Epoch: 28. Loss: 0.05745374783873558\n",
      "28 val 3 799\n",
      "Phase: val. Epoch: 28. Loss: 0.05498644709587097\n",
      "28 val 4 811\n",
      "Phase: val. Epoch: 28. Loss: 0.07315677404403687\n",
      "28 val 5 823\n",
      "Phase: val. Epoch: 28. Loss: 0.04632722586393356\n",
      "28 val 6 835\n",
      "Phase: val. Epoch: 28. Loss: 0.05963495001196861\n",
      "28 val 7 847\n",
      "Phase: val. Epoch: 28. Loss: 0.05301786586642265\n",
      "28 val 8 859\n",
      "Phase: val. Epoch: 28. Loss: 0.0517595037817955\n",
      "28 val 9 871\n",
      "Phase: val. Epoch: 28. Loss: 0.06096816062927246\n",
      "28 val 10 883\n",
      "Phase: val. Epoch: 28. Loss: 0.05760632082819939\n",
      "28 val 11 884\n",
      "Phase: val. Epoch: 28. Loss: 0.009536392986774445\n",
      "29 train 0 12\n",
      "Phase: train. Epoch: 29. Loss: 0.07309360802173615\n",
      "29 train 1 24\n",
      "Phase: train. Epoch: 29. Loss: 0.06957178562879562\n",
      "29 train 2 36\n",
      "Phase: train. Epoch: 29. Loss: 0.0699290782213211\n",
      "29 train 3 48\n",
      "Phase: train. Epoch: 29. Loss: 0.07823978364467621\n",
      "29 train 4 60\n",
      "Phase: train. Epoch: 29. Loss: 0.07292324304580688\n",
      "29 train 5 72\n",
      "Phase: train. Epoch: 29. Loss: 0.046863995492458344\n",
      "29 train 6 84\n",
      "Phase: train. Epoch: 29. Loss: 0.07756930589675903\n",
      "29 train 7 96\n",
      "Phase: train. Epoch: 29. Loss: 0.09856297820806503\n",
      "29 train 8 108\n",
      "Phase: train. Epoch: 29. Loss: 0.05476370453834534\n",
      "29 train 9 120\n",
      "Phase: train. Epoch: 29. Loss: 0.048055499792099\n",
      "29 train 10 132\n",
      "Phase: train. Epoch: 29. Loss: 0.07789986580610275\n",
      "29 train 11 144\n",
      "Phase: train. Epoch: 29. Loss: 0.09138845652341843\n",
      "29 train 12 156\n",
      "Phase: train. Epoch: 29. Loss: 0.06055285781621933\n",
      "29 train 13 168\n",
      "Phase: train. Epoch: 29. Loss: 0.08593520522117615\n",
      "29 train 14 180\n",
      "Phase: train. Epoch: 29. Loss: 0.09314297139644623\n",
      "29 train 15 192\n",
      "Phase: train. Epoch: 29. Loss: 0.1029205322265625\n",
      "29 train 16 204\n",
      "Phase: train. Epoch: 29. Loss: 0.05807219073176384\n",
      "29 train 17 216\n",
      "Phase: train. Epoch: 29. Loss: 0.03173048049211502\n",
      "29 train 18 228\n",
      "Phase: train. Epoch: 29. Loss: 0.0385591983795166\n",
      "29 train 19 240\n",
      "Phase: train. Epoch: 29. Loss: 0.06835836172103882\n",
      "29 train 20 252\n",
      "Phase: train. Epoch: 29. Loss: 0.07104790210723877\n",
      "29 train 21 264\n",
      "Phase: train. Epoch: 29. Loss: 0.04785848781466484\n",
      "29 train 22 276\n",
      "Phase: train. Epoch: 29. Loss: 0.04919026046991348\n",
      "29 train 23 288\n",
      "Phase: train. Epoch: 29. Loss: 0.07042257487773895\n",
      "29 train 24 300\n",
      "Phase: train. Epoch: 29. Loss: 0.10017623007297516\n",
      "29 train 25 312\n",
      "Phase: train. Epoch: 29. Loss: 0.03710833936929703\n",
      "29 train 26 324\n",
      "Phase: train. Epoch: 29. Loss: 0.0554574653506279\n",
      "29 train 27 336\n",
      "Phase: train. Epoch: 29. Loss: 0.05868080258369446\n",
      "29 train 28 348\n",
      "Phase: train. Epoch: 29. Loss: 0.0721878856420517\n",
      "29 train 29 360\n",
      "Phase: train. Epoch: 29. Loss: 0.07265234738588333\n",
      "29 train 30 372\n",
      "Phase: train. Epoch: 29. Loss: 0.07924260944128036\n",
      "29 train 31 384\n",
      "Phase: train. Epoch: 29. Loss: 0.044545672833919525\n",
      "29 train 32 396\n",
      "Phase: train. Epoch: 29. Loss: 0.043183863162994385\n",
      "29 train 33 408\n",
      "Phase: train. Epoch: 29. Loss: 0.07914885133504868\n",
      "29 train 34 420\n",
      "Phase: train. Epoch: 29. Loss: 0.07953827828168869\n",
      "29 train 35 432\n",
      "Phase: train. Epoch: 29. Loss: 0.07537247985601425\n",
      "29 train 36 444\n",
      "Phase: train. Epoch: 29. Loss: 0.05813544988632202\n",
      "29 train 37 456\n",
      "Phase: train. Epoch: 29. Loss: 0.099995456635952\n",
      "29 train 38 468\n",
      "Phase: train. Epoch: 29. Loss: 0.0443231426179409\n",
      "29 train 39 480\n",
      "Phase: train. Epoch: 29. Loss: 0.06207674741744995\n",
      "29 train 40 492\n",
      "Phase: train. Epoch: 29. Loss: 0.06950663030147552\n",
      "29 train 41 504\n",
      "Phase: train. Epoch: 29. Loss: 0.0451631173491478\n",
      "29 train 42 516\n",
      "Phase: train. Epoch: 29. Loss: 0.07382583618164062\n",
      "29 train 43 528\n",
      "Phase: train. Epoch: 29. Loss: 0.0837704986333847\n",
      "29 train 44 540\n",
      "Phase: train. Epoch: 29. Loss: 0.04985780641436577\n",
      "29 train 45 552\n",
      "Phase: train. Epoch: 29. Loss: 0.07524259388446808\n",
      "29 train 46 564\n",
      "Phase: train. Epoch: 29. Loss: 0.05255528539419174\n",
      "29 train 47 576\n",
      "Phase: train. Epoch: 29. Loss: 0.06104237958788872\n",
      "29 train 48 588\n",
      "Phase: train. Epoch: 29. Loss: 0.05380890518426895\n",
      "29 train 49 600\n",
      "Phase: train. Epoch: 29. Loss: 0.07448210567235947\n",
      "29 train 50 612\n",
      "Phase: train. Epoch: 29. Loss: 0.07949145883321762\n",
      "29 train 51 624\n",
      "Phase: train. Epoch: 29. Loss: 0.042101748287677765\n",
      "29 train 52 636\n",
      "Phase: train. Epoch: 29. Loss: 0.11584268510341644\n",
      "29 train 53 648\n",
      "Phase: train. Epoch: 29. Loss: 0.05312451720237732\n",
      "29 train 54 660\n",
      "Phase: train. Epoch: 29. Loss: 0.06405772268772125\n",
      "29 train 55 672\n",
      "Phase: train. Epoch: 29. Loss: 0.07563351839780807\n",
      "29 train 56 684\n",
      "Phase: train. Epoch: 29. Loss: 0.05249391496181488\n",
      "29 train 57 696\n",
      "Phase: train. Epoch: 29. Loss: 0.06286907941102982\n",
      "29 train 58 708\n",
      "Phase: train. Epoch: 29. Loss: 0.06347072124481201\n",
      "29 train 59 720\n",
      "Phase: train. Epoch: 29. Loss: 0.05872026085853577\n",
      "29 train 60 732\n",
      "Phase: train. Epoch: 29. Loss: 0.06862454861402512\n",
      "29 train 61 744\n",
      "Phase: train. Epoch: 29. Loss: 0.0570409893989563\n",
      "29 train 62 751\n",
      "Phase: train. Epoch: 29. Loss: 0.07829295098781586\n",
      "29 val 0 763\n",
      "Phase: val. Epoch: 29. Loss: 0.06680050492286682\n",
      "29 val 1 775\n",
      "Phase: val. Epoch: 29. Loss: 0.09338726848363876\n",
      "29 val 2 787\n",
      "Phase: val. Epoch: 29. Loss: 0.07378904521465302\n",
      "29 val 3 799\n",
      "Phase: val. Epoch: 29. Loss: 0.08367335796356201\n",
      "29 val 4 811\n",
      "Phase: val. Epoch: 29. Loss: 0.05355208367109299\n",
      "29 val 5 823\n",
      "Phase: val. Epoch: 29. Loss: 0.025428254157304764\n",
      "29 val 6 835\n",
      "Phase: val. Epoch: 29. Loss: 0.06436435133218765\n",
      "29 val 7 847\n",
      "Phase: val. Epoch: 29. Loss: 0.0709158331155777\n",
      "29 val 8 859\n",
      "Phase: val. Epoch: 29. Loss: 0.05054525285959244\n",
      "29 val 9 871\n",
      "Phase: val. Epoch: 29. Loss: 0.05304994434118271\n",
      "29 val 10 883\n",
      "Phase: val. Epoch: 29. Loss: 0.098713219165802\n",
      "29 val 11 884\n",
      "Phase: val. Epoch: 29. Loss: 0.018101511523127556\n",
      "30 train 0 12\n",
      "Phase: train. Epoch: 30. Loss: 0.0615045540034771\n",
      "30 train 1 24\n",
      "Phase: train. Epoch: 30. Loss: 0.06271415203809738\n",
      "30 train 2 36\n",
      "Phase: train. Epoch: 30. Loss: 0.07722336053848267\n",
      "30 train 3 48\n",
      "Phase: train. Epoch: 30. Loss: 0.07783422619104385\n",
      "30 train 4 60\n",
      "Phase: train. Epoch: 30. Loss: 0.0838644951581955\n",
      "30 train 5 72\n",
      "Phase: train. Epoch: 30. Loss: 0.10783230513334274\n",
      "30 train 6 84\n",
      "Phase: train. Epoch: 30. Loss: 0.08069296181201935\n",
      "30 train 7 96\n",
      "Phase: train. Epoch: 30. Loss: 0.05868612229824066\n",
      "30 train 8 108\n",
      "Phase: train. Epoch: 30. Loss: 0.06946627795696259\n",
      "30 train 9 120\n",
      "Phase: train. Epoch: 30. Loss: 0.05745862051844597\n",
      "30 train 10 132\n",
      "Phase: train. Epoch: 30. Loss: 0.05481017753481865\n",
      "30 train 11 144\n",
      "Phase: train. Epoch: 30. Loss: 0.0559198260307312\n",
      "30 train 12 156\n",
      "Phase: train. Epoch: 30. Loss: 0.04581993818283081\n",
      "30 train 13 168\n",
      "Phase: train. Epoch: 30. Loss: 0.04621684551239014\n",
      "30 train 14 180\n",
      "Phase: train. Epoch: 30. Loss: 0.06291510164737701\n",
      "30 train 15 192\n",
      "Phase: train. Epoch: 30. Loss: 0.048379480838775635\n",
      "30 train 16 204\n",
      "Phase: train. Epoch: 30. Loss: 0.04264507815241814\n",
      "30 train 17 216\n",
      "Phase: train. Epoch: 30. Loss: 0.06683477759361267\n",
      "30 train 18 228\n",
      "Phase: train. Epoch: 30. Loss: 0.08538143336772919\n",
      "30 train 19 240\n",
      "Phase: train. Epoch: 30. Loss: 0.07491343468427658\n",
      "30 train 20 252\n",
      "Phase: train. Epoch: 30. Loss: 0.0577412024140358\n",
      "30 train 21 264\n",
      "Phase: train. Epoch: 30. Loss: 0.06037922203540802\n",
      "30 train 22 276\n",
      "Phase: train. Epoch: 30. Loss: 0.04064551740884781\n",
      "30 train 23 288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 30. Loss: 0.039972394704818726\n",
      "30 train 24 300\n",
      "Phase: train. Epoch: 30. Loss: 0.06728274375200272\n",
      "30 train 25 312\n",
      "Phase: train. Epoch: 30. Loss: 0.07118812203407288\n",
      "30 train 26 324\n",
      "Phase: train. Epoch: 30. Loss: 0.06159069761633873\n",
      "30 train 27 336\n",
      "Phase: train. Epoch: 30. Loss: 0.05773257464170456\n",
      "30 train 28 348\n",
      "Phase: train. Epoch: 30. Loss: 0.08953617513179779\n",
      "30 train 29 360\n",
      "Phase: train. Epoch: 30. Loss: 0.03829281032085419\n",
      "30 train 30 372\n",
      "Phase: train. Epoch: 30. Loss: 0.07917742431163788\n",
      "30 train 31 384\n",
      "Phase: train. Epoch: 30. Loss: 0.06605076789855957\n",
      "30 train 32 396\n",
      "Phase: train. Epoch: 30. Loss: 0.06195981800556183\n",
      "30 train 33 408\n",
      "Phase: train. Epoch: 30. Loss: 0.051332734525203705\n",
      "30 train 34 420\n",
      "Phase: train. Epoch: 30. Loss: 0.05920964106917381\n",
      "30 train 35 432\n",
      "Phase: train. Epoch: 30. Loss: 0.05934656783938408\n",
      "30 train 36 444\n",
      "Phase: train. Epoch: 30. Loss: 0.06960637867450714\n",
      "30 train 37 456\n",
      "Phase: train. Epoch: 30. Loss: 0.04208623617887497\n",
      "30 train 38 468\n",
      "Phase: train. Epoch: 30. Loss: 0.04093283414840698\n",
      "30 train 39 480\n",
      "Phase: train. Epoch: 30. Loss: 0.057240795344114304\n",
      "30 train 40 492\n",
      "Phase: train. Epoch: 30. Loss: 0.04186567664146423\n",
      "30 train 41 504\n",
      "Phase: train. Epoch: 30. Loss: 0.05794186517596245\n",
      "30 train 42 516\n",
      "Phase: train. Epoch: 30. Loss: 0.06710930168628693\n",
      "30 train 43 528\n",
      "Phase: train. Epoch: 30. Loss: 0.07244459539651871\n",
      "30 train 44 540\n",
      "Phase: train. Epoch: 30. Loss: 0.054382674396038055\n",
      "30 train 45 552\n",
      "Phase: train. Epoch: 30. Loss: 0.07471685856580734\n",
      "30 train 46 564\n",
      "Phase: train. Epoch: 30. Loss: 0.08661656081676483\n",
      "30 train 47 576\n",
      "Phase: train. Epoch: 30. Loss: 0.06411699950695038\n",
      "30 train 48 588\n",
      "Phase: train. Epoch: 30. Loss: 0.06316307932138443\n",
      "30 train 49 600\n",
      "Phase: train. Epoch: 30. Loss: 0.06263796985149384\n",
      "30 train 50 612\n",
      "Phase: train. Epoch: 30. Loss: 0.0915602445602417\n",
      "30 train 51 624\n",
      "Phase: train. Epoch: 30. Loss: 0.09270887821912766\n",
      "30 train 52 636\n",
      "Phase: train. Epoch: 30. Loss: 0.07737216353416443\n",
      "30 train 53 648\n",
      "Phase: train. Epoch: 30. Loss: 0.07040741294622421\n",
      "30 train 54 660\n",
      "Phase: train. Epoch: 30. Loss: 0.09287126362323761\n",
      "30 train 55 672\n",
      "Phase: train. Epoch: 30. Loss: 0.07665470242500305\n",
      "30 train 56 684\n",
      "Phase: train. Epoch: 30. Loss: 0.06337755173444748\n",
      "30 train 57 696\n",
      "Phase: train. Epoch: 30. Loss: 0.04736754298210144\n",
      "30 train 58 708\n",
      "Phase: train. Epoch: 30. Loss: 0.06670181453227997\n",
      "30 train 59 720\n",
      "Phase: train. Epoch: 30. Loss: 0.0659257024526596\n",
      "30 train 60 732\n",
      "Phase: train. Epoch: 30. Loss: 0.06379435211420059\n",
      "30 train 61 744\n",
      "Phase: train. Epoch: 30. Loss: 0.055544428527355194\n",
      "30 train 62 751\n",
      "Phase: train. Epoch: 30. Loss: 0.062331363558769226\n",
      "30 val 0 763\n",
      "Phase: val. Epoch: 30. Loss: 0.16098320484161377\n",
      "30 val 1 775\n",
      "Phase: val. Epoch: 30. Loss: 0.0503782257437706\n",
      "30 val 2 787\n",
      "Phase: val. Epoch: 30. Loss: 0.13422496616840363\n",
      "30 val 3 799\n",
      "Phase: val. Epoch: 30. Loss: 0.16149404644966125\n",
      "30 val 4 811\n",
      "Phase: val. Epoch: 30. Loss: 0.15233540534973145\n",
      "30 val 5 823\n",
      "Phase: val. Epoch: 30. Loss: 0.05626720190048218\n",
      "30 val 6 835\n",
      "Phase: val. Epoch: 30. Loss: 0.09515373408794403\n",
      "30 val 7 847\n",
      "Phase: val. Epoch: 30. Loss: 0.13604798913002014\n",
      "30 val 8 859\n",
      "Phase: val. Epoch: 30. Loss: 0.04607796669006348\n",
      "30 val 9 871\n",
      "Phase: val. Epoch: 30. Loss: 0.138036847114563\n",
      "30 val 10 883\n",
      "Phase: val. Epoch: 30. Loss: 0.047762781381607056\n",
      "30 val 11 884\n",
      "Phase: val. Epoch: 30. Loss: 0.009769446216523647\n",
      "31 train 0 12\n",
      "Phase: train. Epoch: 31. Loss: 0.06649073958396912\n",
      "31 train 1 24\n",
      "Phase: train. Epoch: 31. Loss: 0.0737137719988823\n",
      "31 train 2 36\n",
      "Phase: train. Epoch: 31. Loss: 0.08234216272830963\n",
      "31 train 3 48\n",
      "Phase: train. Epoch: 31. Loss: 0.07902710139751434\n",
      "31 train 4 60\n",
      "Phase: train. Epoch: 31. Loss: 0.05603019893169403\n",
      "31 train 5 72\n",
      "Phase: train. Epoch: 31. Loss: 0.05761604756116867\n",
      "31 train 6 84\n",
      "Phase: train. Epoch: 31. Loss: 0.06208796799182892\n",
      "31 train 7 96\n",
      "Phase: train. Epoch: 31. Loss: 0.07265228033065796\n",
      "31 train 8 108\n",
      "Phase: train. Epoch: 31. Loss: 0.07498639076948166\n",
      "31 train 9 120\n",
      "Phase: train. Epoch: 31. Loss: 0.058919757604599\n",
      "31 train 10 132\n",
      "Phase: train. Epoch: 31. Loss: 0.0584610216319561\n",
      "31 train 11 144\n",
      "Phase: train. Epoch: 31. Loss: 0.06003686040639877\n",
      "31 train 12 156\n",
      "Phase: train. Epoch: 31. Loss: 0.054432161152362823\n",
      "31 train 13 168\n",
      "Phase: train. Epoch: 31. Loss: 0.06900627911090851\n",
      "31 train 14 180\n",
      "Phase: train. Epoch: 31. Loss: 0.10399210453033447\n",
      "31 train 15 192\n",
      "Phase: train. Epoch: 31. Loss: 0.08485877513885498\n",
      "31 train 16 204\n",
      "Phase: train. Epoch: 31. Loss: 0.07156746089458466\n",
      "31 train 17 216\n",
      "Phase: train. Epoch: 31. Loss: 0.08371483534574509\n",
      "31 train 18 228\n",
      "Phase: train. Epoch: 31. Loss: 0.10698267072439194\n",
      "31 train 19 240\n",
      "Phase: train. Epoch: 31. Loss: 0.08648023009300232\n",
      "31 train 20 252\n",
      "Phase: train. Epoch: 31. Loss: 0.05472678691148758\n",
      "31 train 21 264\n",
      "Phase: train. Epoch: 31. Loss: 0.08182089030742645\n",
      "31 train 22 276\n",
      "Phase: train. Epoch: 31. Loss: 0.07305172085762024\n",
      "31 train 23 288\n",
      "Phase: train. Epoch: 31. Loss: 0.05963198468089104\n",
      "31 train 24 300\n",
      "Phase: train. Epoch: 31. Loss: 0.04890371486544609\n",
      "31 train 25 312\n",
      "Phase: train. Epoch: 31. Loss: 0.06338188052177429\n",
      "31 train 26 324\n",
      "Phase: train. Epoch: 31. Loss: 0.07122743129730225\n",
      "31 train 27 336\n",
      "Phase: train. Epoch: 31. Loss: 0.07574512809515\n",
      "31 train 28 348\n",
      "Phase: train. Epoch: 31. Loss: 0.06564480066299438\n",
      "31 train 29 360\n",
      "Phase: train. Epoch: 31. Loss: 0.05601112172007561\n",
      "31 train 30 372\n",
      "Phase: train. Epoch: 31. Loss: 0.08767817914485931\n",
      "31 train 31 384\n",
      "Phase: train. Epoch: 31. Loss: 0.05028960108757019\n",
      "31 train 32 396\n",
      "Phase: train. Epoch: 31. Loss: 0.06363876909017563\n",
      "31 train 33 408\n",
      "Phase: train. Epoch: 31. Loss: 0.060097821056842804\n",
      "31 train 34 420\n",
      "Phase: train. Epoch: 31. Loss: 0.06377458572387695\n",
      "31 train 35 432\n",
      "Phase: train. Epoch: 31. Loss: 0.046720366925001144\n",
      "31 train 36 444\n",
      "Phase: train. Epoch: 31. Loss: 0.044927895069122314\n",
      "31 train 37 456\n",
      "Phase: train. Epoch: 31. Loss: 0.08804281055927277\n",
      "31 train 38 468\n",
      "Phase: train. Epoch: 31. Loss: 0.06959235668182373\n",
      "31 train 39 480\n",
      "Phase: train. Epoch: 31. Loss: 0.06665874272584915\n",
      "31 train 40 492\n",
      "Phase: train. Epoch: 31. Loss: 0.048719219863414764\n",
      "31 train 41 504\n",
      "Phase: train. Epoch: 31. Loss: 0.0760909765958786\n",
      "31 train 42 516\n",
      "Phase: train. Epoch: 31. Loss: 0.06435146182775497\n",
      "31 train 43 528\n",
      "Phase: train. Epoch: 31. Loss: 0.051937639713287354\n",
      "31 train 44 540\n",
      "Phase: train. Epoch: 31. Loss: 0.05950336903333664\n",
      "31 train 45 552\n",
      "Phase: train. Epoch: 31. Loss: 0.0848119780421257\n",
      "31 train 46 564\n",
      "Phase: train. Epoch: 31. Loss: 0.061851125210523605\n",
      "31 train 47 576\n",
      "Phase: train. Epoch: 31. Loss: 0.07084833085536957\n",
      "31 train 48 588\n",
      "Phase: train. Epoch: 31. Loss: 0.08691209554672241\n",
      "31 train 49 600\n",
      "Phase: train. Epoch: 31. Loss: 0.070214182138443\n",
      "31 train 50 612\n",
      "Phase: train. Epoch: 31. Loss: 0.04622343182563782\n",
      "31 train 51 624\n",
      "Phase: train. Epoch: 31. Loss: 0.05941752716898918\n",
      "31 train 52 636\n",
      "Phase: train. Epoch: 31. Loss: 0.06789238750934601\n",
      "31 train 53 648\n",
      "Phase: train. Epoch: 31. Loss: 0.07051704823970795\n",
      "31 train 54 660\n",
      "Phase: train. Epoch: 31. Loss: 0.0790860503911972\n",
      "31 train 55 672\n",
      "Phase: train. Epoch: 31. Loss: 0.05506502836942673\n",
      "31 train 56 684\n",
      "Phase: train. Epoch: 31. Loss: 0.06619001924991608\n",
      "31 train 57 696\n",
      "Phase: train. Epoch: 31. Loss: 0.09346695989370346\n",
      "31 train 58 708\n",
      "Phase: train. Epoch: 31. Loss: 0.061947502195835114\n",
      "31 train 59 720\n",
      "Phase: train. Epoch: 31. Loss: 0.05691196769475937\n",
      "31 train 60 732\n",
      "Phase: train. Epoch: 31. Loss: 0.06935393065214157\n",
      "31 train 61 744\n",
      "Phase: train. Epoch: 31. Loss: 0.08525638282299042\n",
      "31 train 62 751\n",
      "Phase: train. Epoch: 31. Loss: 0.056231096386909485\n",
      "31 val 0 763\n",
      "Phase: val. Epoch: 31. Loss: 0.049985699355602264\n",
      "31 val 1 775\n",
      "Phase: val. Epoch: 31. Loss: 0.04551231861114502\n",
      "31 val 2 787\n",
      "Phase: val. Epoch: 31. Loss: 0.041336968541145325\n",
      "31 val 3 799\n",
      "Phase: val. Epoch: 31. Loss: 0.08016595989465714\n",
      "31 val 4 811\n",
      "Phase: val. Epoch: 31. Loss: 0.06753682345151901\n",
      "31 val 5 823\n",
      "Phase: val. Epoch: 31. Loss: 0.0643845647573471\n",
      "31 val 6 835\n",
      "Phase: val. Epoch: 31. Loss: 0.08024906367063522\n",
      "31 val 7 847\n",
      "Phase: val. Epoch: 31. Loss: 0.06630875915288925\n",
      "31 val 8 859\n",
      "Phase: val. Epoch: 31. Loss: 0.07440312951803207\n",
      "31 val 9 871\n",
      "Phase: val. Epoch: 31. Loss: 0.04752573370933533\n",
      "31 val 10 883\n",
      "Phase: val. Epoch: 31. Loss: 0.06884986162185669\n",
      "31 val 11 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 31. Loss: 0.08381002396345139\n",
      "32 train 0 12\n",
      "Phase: train. Epoch: 32. Loss: 0.039074718952178955\n",
      "32 train 1 24\n",
      "Phase: train. Epoch: 32. Loss: 0.06949169933795929\n",
      "32 train 2 36\n",
      "Phase: train. Epoch: 32. Loss: 0.058078162372112274\n",
      "32 train 3 48\n",
      "Phase: train. Epoch: 32. Loss: 0.05957050248980522\n",
      "32 train 4 60\n",
      "Phase: train. Epoch: 32. Loss: 0.05532894656062126\n",
      "32 train 5 72\n",
      "Phase: train. Epoch: 32. Loss: 0.04005827754735947\n",
      "32 train 6 84\n",
      "Phase: train. Epoch: 32. Loss: 0.054307352751493454\n",
      "32 train 7 96\n",
      "Phase: train. Epoch: 32. Loss: 0.10940803587436676\n",
      "32 train 8 108\n",
      "Phase: train. Epoch: 32. Loss: 0.05104619264602661\n",
      "32 train 9 120\n",
      "Phase: train. Epoch: 32. Loss: 0.075267493724823\n",
      "32 train 10 132\n",
      "Phase: train. Epoch: 32. Loss: 0.05060221999883652\n",
      "32 train 11 144\n",
      "Phase: train. Epoch: 32. Loss: 0.054312996566295624\n",
      "32 train 12 156\n",
      "Phase: train. Epoch: 32. Loss: 0.06145910173654556\n",
      "32 train 13 168\n",
      "Phase: train. Epoch: 32. Loss: 0.060805629938840866\n",
      "32 train 14 180\n",
      "Phase: train. Epoch: 32. Loss: 0.07728251814842224\n",
      "32 train 15 192\n",
      "Phase: train. Epoch: 32. Loss: 0.06167136877775192\n",
      "32 train 16 204\n",
      "Phase: train. Epoch: 32. Loss: 0.0662667378783226\n",
      "32 train 17 216\n",
      "Phase: train. Epoch: 32. Loss: 0.05016972869634628\n",
      "32 train 18 228\n",
      "Phase: train. Epoch: 32. Loss: 0.07400700449943542\n",
      "32 train 19 240\n",
      "Phase: train. Epoch: 32. Loss: 0.05793716013431549\n",
      "32 train 20 252\n",
      "Phase: train. Epoch: 32. Loss: 0.10033294558525085\n",
      "32 train 21 264\n",
      "Phase: train. Epoch: 32. Loss: 0.08719459176063538\n",
      "32 train 22 276\n",
      "Phase: train. Epoch: 32. Loss: 0.06774649769067764\n",
      "32 train 23 288\n",
      "Phase: train. Epoch: 32. Loss: 0.061831071972846985\n",
      "32 train 24 300\n",
      "Phase: train. Epoch: 32. Loss: 0.061114244163036346\n",
      "32 train 25 312\n",
      "Phase: train. Epoch: 32. Loss: 0.07883453369140625\n",
      "32 train 26 324\n",
      "Phase: train. Epoch: 32. Loss: 0.07033906131982803\n",
      "32 train 27 336\n",
      "Phase: train. Epoch: 32. Loss: 0.07961748540401459\n",
      "32 train 28 348\n",
      "Phase: train. Epoch: 32. Loss: 0.07622595131397247\n",
      "32 train 29 360\n",
      "Phase: train. Epoch: 32. Loss: 0.059539370238780975\n",
      "32 train 30 372\n",
      "Phase: train. Epoch: 32. Loss: 0.053297385573387146\n",
      "32 train 31 384\n",
      "Phase: train. Epoch: 32. Loss: 0.07501880824565887\n",
      "32 train 32 396\n",
      "Phase: train. Epoch: 32. Loss: 0.06280358880758286\n",
      "32 train 33 408\n",
      "Phase: train. Epoch: 32. Loss: 0.0893530398607254\n",
      "32 train 34 420\n",
      "Phase: train. Epoch: 32. Loss: 0.04779372736811638\n",
      "32 train 35 432\n",
      "Phase: train. Epoch: 32. Loss: 0.07795704901218414\n",
      "32 train 36 444\n",
      "Phase: train. Epoch: 32. Loss: 0.0433378741145134\n",
      "32 train 37 456\n",
      "Phase: train. Epoch: 32. Loss: 0.04718102142214775\n",
      "32 train 38 468\n",
      "Phase: train. Epoch: 32. Loss: 0.056819647550582886\n",
      "32 train 39 480\n",
      "Phase: train. Epoch: 32. Loss: 0.05825987458229065\n",
      "32 train 40 492\n",
      "Phase: train. Epoch: 32. Loss: 0.05379270017147064\n",
      "32 train 41 504\n",
      "Phase: train. Epoch: 32. Loss: 0.0553763210773468\n",
      "32 train 42 516\n",
      "Phase: train. Epoch: 32. Loss: 0.09389084577560425\n",
      "32 train 43 528\n",
      "Phase: train. Epoch: 32. Loss: 0.06645721942186356\n",
      "32 train 44 540\n",
      "Phase: train. Epoch: 32. Loss: 0.06052910536527634\n",
      "32 train 45 552\n",
      "Phase: train. Epoch: 32. Loss: 0.040033116936683655\n",
      "32 train 46 564\n",
      "Phase: train. Epoch: 32. Loss: 0.06418511271476746\n",
      "32 train 47 576\n",
      "Phase: train. Epoch: 32. Loss: 0.06083802133798599\n",
      "32 train 48 588\n",
      "Phase: train. Epoch: 32. Loss: 0.05812477320432663\n",
      "32 train 49 600\n",
      "Phase: train. Epoch: 32. Loss: 0.05637954920530319\n",
      "32 train 50 612\n",
      "Phase: train. Epoch: 32. Loss: 0.0730433389544487\n",
      "32 train 51 624\n",
      "Phase: train. Epoch: 32. Loss: 0.056816231459379196\n",
      "32 train 52 636\n",
      "Phase: train. Epoch: 32. Loss: 0.07210266590118408\n",
      "32 train 53 648\n",
      "Phase: train. Epoch: 32. Loss: 0.08509653806686401\n",
      "32 train 54 660\n",
      "Phase: train. Epoch: 32. Loss: 0.07802055031061172\n",
      "32 train 55 672\n",
      "Phase: train. Epoch: 32. Loss: 0.07061953097581863\n",
      "32 train 56 684\n",
      "Phase: train. Epoch: 32. Loss: 0.06316499412059784\n",
      "32 train 57 696\n",
      "Phase: train. Epoch: 32. Loss: 0.059952907264232635\n",
      "32 train 58 708\n",
      "Phase: train. Epoch: 32. Loss: 0.0998883992433548\n",
      "32 train 59 720\n",
      "Phase: train. Epoch: 32. Loss: 0.03844449669122696\n",
      "32 train 60 732\n",
      "Phase: train. Epoch: 32. Loss: 0.061637863516807556\n",
      "32 train 61 744\n",
      "Phase: train. Epoch: 32. Loss: 0.10410661995410919\n",
      "32 train 62 751\n",
      "Phase: train. Epoch: 32. Loss: 0.05693734809756279\n",
      "32 val 0 763\n",
      "Phase: val. Epoch: 32. Loss: 0.06064558029174805\n",
      "32 val 1 775\n",
      "Phase: val. Epoch: 32. Loss: 0.057737283408641815\n",
      "32 val 2 787\n",
      "Phase: val. Epoch: 32. Loss: 0.07768181711435318\n",
      "32 val 3 799\n",
      "Phase: val. Epoch: 32. Loss: 0.048499345779418945\n",
      "32 val 4 811\n",
      "Phase: val. Epoch: 32. Loss: 0.05917457863688469\n",
      "32 val 5 823\n",
      "Phase: val. Epoch: 32. Loss: 0.044669073075056076\n",
      "32 val 6 835\n",
      "Phase: val. Epoch: 32. Loss: 0.07161212712526321\n",
      "32 val 7 847\n",
      "Phase: val. Epoch: 32. Loss: 0.0457308366894722\n",
      "32 val 8 859\n",
      "Phase: val. Epoch: 32. Loss: 0.06662886589765549\n",
      "32 val 9 871\n",
      "Phase: val. Epoch: 32. Loss: 0.06894320249557495\n",
      "32 val 10 883\n",
      "Phase: val. Epoch: 32. Loss: 0.07956269383430481\n",
      "32 val 11 884\n",
      "Phase: val. Epoch: 32. Loss: 0.03375891596078873\n",
      "33 train 0 12\n",
      "Phase: train. Epoch: 33. Loss: 0.07274238765239716\n",
      "33 train 1 24\n",
      "Phase: train. Epoch: 33. Loss: 0.08736922591924667\n",
      "33 train 2 36\n",
      "Phase: train. Epoch: 33. Loss: 0.08657488226890564\n",
      "33 train 3 48\n",
      "Phase: train. Epoch: 33. Loss: 0.04851230978965759\n",
      "33 train 4 60\n",
      "Phase: train. Epoch: 33. Loss: 0.060121070593595505\n",
      "33 train 5 72\n",
      "Phase: train. Epoch: 33. Loss: 0.0730142816901207\n",
      "33 train 6 84\n",
      "Phase: train. Epoch: 33. Loss: 0.05770687013864517\n",
      "33 train 7 96\n",
      "Phase: train. Epoch: 33. Loss: 0.06527480483055115\n",
      "33 train 8 108\n",
      "Phase: train. Epoch: 33. Loss: 0.04265099763870239\n",
      "33 train 9 120\n",
      "Phase: train. Epoch: 33. Loss: 0.0716165155172348\n",
      "33 train 10 132\n",
      "Phase: train. Epoch: 33. Loss: 0.056969113647937775\n",
      "33 train 11 144\n",
      "Phase: train. Epoch: 33. Loss: 0.08049249649047852\n",
      "33 train 12 156\n",
      "Phase: train. Epoch: 33. Loss: 0.0834241658449173\n",
      "33 train 13 168\n",
      "Phase: train. Epoch: 33. Loss: 0.05922738462686539\n",
      "33 train 14 180\n",
      "Phase: train. Epoch: 33. Loss: 0.06244302913546562\n",
      "33 train 15 192\n",
      "Phase: train. Epoch: 33. Loss: 0.05802673101425171\n",
      "33 train 16 204\n",
      "Phase: train. Epoch: 33. Loss: 0.064856618642807\n",
      "33 train 17 216\n",
      "Phase: train. Epoch: 33. Loss: 0.048043143004179\n",
      "33 train 18 228\n",
      "Phase: train. Epoch: 33. Loss: 0.04759383201599121\n",
      "33 train 19 240\n",
      "Phase: train. Epoch: 33. Loss: 0.046172529458999634\n",
      "33 train 20 252\n",
      "Phase: train. Epoch: 33. Loss: 0.055209945887327194\n",
      "33 train 21 264\n",
      "Phase: train. Epoch: 33. Loss: 0.0861273854970932\n",
      "33 train 22 276\n",
      "Phase: train. Epoch: 33. Loss: 0.042841654270887375\n",
      "33 train 23 288\n",
      "Phase: train. Epoch: 33. Loss: 0.058770984411239624\n",
      "33 train 24 300\n",
      "Phase: train. Epoch: 33. Loss: 0.04769371077418327\n",
      "33 train 25 312\n",
      "Phase: train. Epoch: 33. Loss: 0.0877779945731163\n",
      "33 train 26 324\n",
      "Phase: train. Epoch: 33. Loss: 0.04479343444108963\n",
      "33 train 27 336\n",
      "Phase: train. Epoch: 33. Loss: 0.07701437920331955\n",
      "33 train 28 348\n",
      "Phase: train. Epoch: 33. Loss: 0.05544719099998474\n",
      "33 train 29 360\n",
      "Phase: train. Epoch: 33. Loss: 0.05565434321761131\n",
      "33 train 30 372\n",
      "Phase: train. Epoch: 33. Loss: 0.06315945833921432\n",
      "33 train 31 384\n",
      "Phase: train. Epoch: 33. Loss: 0.0629759430885315\n",
      "33 train 32 396\n",
      "Phase: train. Epoch: 33. Loss: 0.06004874408245087\n",
      "33 train 33 408\n",
      "Phase: train. Epoch: 33. Loss: 0.051364466547966\n",
      "33 train 34 420\n",
      "Phase: train. Epoch: 33. Loss: 0.10682352632284164\n",
      "33 train 35 432\n",
      "Phase: train. Epoch: 33. Loss: 0.04805761203169823\n",
      "33 train 36 444\n",
      "Phase: train. Epoch: 33. Loss: 0.08418404310941696\n",
      "33 train 37 456\n",
      "Phase: train. Epoch: 33. Loss: 0.05126988887786865\n",
      "33 train 38 468\n",
      "Phase: train. Epoch: 33. Loss: 0.0936812311410904\n",
      "33 train 39 480\n",
      "Phase: train. Epoch: 33. Loss: 0.0511503629386425\n",
      "33 train 40 492\n",
      "Phase: train. Epoch: 33. Loss: 0.06205044314265251\n",
      "33 train 41 504\n",
      "Phase: train. Epoch: 33. Loss: 0.09403490275144577\n",
      "33 train 42 516\n",
      "Phase: train. Epoch: 33. Loss: 0.07368402183055878\n",
      "33 train 43 528\n",
      "Phase: train. Epoch: 33. Loss: 0.05427705869078636\n",
      "33 train 44 540\n",
      "Phase: train. Epoch: 33. Loss: 0.06184292957186699\n",
      "33 train 45 552\n",
      "Phase: train. Epoch: 33. Loss: 0.06176367029547691\n",
      "33 train 46 564\n",
      "Phase: train. Epoch: 33. Loss: 0.06397233158349991\n",
      "33 train 47 576\n",
      "Phase: train. Epoch: 33. Loss: 0.05303748697042465\n",
      "33 train 48 588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 33. Loss: 0.06814099103212357\n",
      "33 train 49 600\n",
      "Phase: train. Epoch: 33. Loss: 0.09990766644477844\n",
      "33 train 50 612\n",
      "Phase: train. Epoch: 33. Loss: 0.07290586829185486\n",
      "33 train 51 624\n",
      "Phase: train. Epoch: 33. Loss: 0.0921349748969078\n",
      "33 train 52 636\n",
      "Phase: train. Epoch: 33. Loss: 0.039863474667072296\n",
      "33 train 53 648\n",
      "Phase: train. Epoch: 33. Loss: 0.07718832790851593\n",
      "33 train 54 660\n",
      "Phase: train. Epoch: 33. Loss: 0.06381317228078842\n",
      "33 train 55 672\n",
      "Phase: train. Epoch: 33. Loss: 0.07051191478967667\n",
      "33 train 56 684\n",
      "Phase: train. Epoch: 33. Loss: 0.05286949872970581\n",
      "33 train 57 696\n",
      "Phase: train. Epoch: 33. Loss: 0.04689167067408562\n",
      "33 train 58 708\n",
      "Phase: train. Epoch: 33. Loss: 0.07602674514055252\n",
      "33 train 59 720\n",
      "Phase: train. Epoch: 33. Loss: 0.05549423024058342\n",
      "33 train 60 732\n",
      "Phase: train. Epoch: 33. Loss: 0.09129230678081512\n",
      "33 train 61 744\n",
      "Phase: train. Epoch: 33. Loss: 0.05364007502794266\n",
      "33 train 62 751\n",
      "Phase: train. Epoch: 33. Loss: 0.05024382099509239\n",
      "33 val 0 763\n",
      "Phase: val. Epoch: 33. Loss: 0.08216284960508347\n",
      "33 val 1 775\n",
      "Phase: val. Epoch: 33. Loss: 0.05822424218058586\n",
      "33 val 2 787\n",
      "Phase: val. Epoch: 33. Loss: 0.07630275934934616\n",
      "33 val 3 799\n",
      "Phase: val. Epoch: 33. Loss: 0.0979696661233902\n",
      "33 val 4 811\n",
      "Phase: val. Epoch: 33. Loss: 0.061723046004772186\n",
      "33 val 5 823\n",
      "Phase: val. Epoch: 33. Loss: 0.0514536015689373\n",
      "33 val 6 835\n",
      "Phase: val. Epoch: 33. Loss: 0.06206604093313217\n",
      "33 val 7 847\n",
      "Phase: val. Epoch: 33. Loss: 0.03756514936685562\n",
      "33 val 8 859\n",
      "Phase: val. Epoch: 33. Loss: 0.05354616791009903\n",
      "33 val 9 871\n",
      "Phase: val. Epoch: 33. Loss: 0.042779430747032166\n",
      "33 val 10 883\n",
      "Phase: val. Epoch: 33. Loss: 0.05991643667221069\n",
      "33 val 11 884\n",
      "Phase: val. Epoch: 33. Loss: 0.004758787807077169\n",
      "34 train 0 12\n",
      "Phase: train. Epoch: 34. Loss: 0.05519556626677513\n",
      "34 train 1 24\n",
      "Phase: train. Epoch: 34. Loss: 0.04913210868835449\n",
      "34 train 2 36\n",
      "Phase: train. Epoch: 34. Loss: 0.06505542993545532\n",
      "34 train 3 48\n",
      "Phase: train. Epoch: 34. Loss: 0.06326296925544739\n",
      "34 train 4 60\n",
      "Phase: train. Epoch: 34. Loss: 0.08168205618858337\n",
      "34 train 5 72\n",
      "Phase: train. Epoch: 34. Loss: 0.11878444999456406\n",
      "34 train 6 84\n",
      "Phase: train. Epoch: 34. Loss: 0.049602657556533813\n",
      "34 train 7 96\n",
      "Phase: train. Epoch: 34. Loss: 0.04974592849612236\n",
      "34 train 8 108\n",
      "Phase: train. Epoch: 34. Loss: 0.07423749566078186\n",
      "34 train 9 120\n",
      "Phase: train. Epoch: 34. Loss: 0.05160249397158623\n",
      "34 train 10 132\n",
      "Phase: train. Epoch: 34. Loss: 0.07159152626991272\n",
      "34 train 11 144\n",
      "Phase: train. Epoch: 34. Loss: 0.05236884206533432\n",
      "34 train 12 156\n",
      "Phase: train. Epoch: 34. Loss: 0.06211516261100769\n",
      "34 train 13 168\n",
      "Phase: train. Epoch: 34. Loss: 0.08734805881977081\n",
      "34 train 14 180\n",
      "Phase: train. Epoch: 34. Loss: 0.055736199021339417\n",
      "34 train 15 192\n",
      "Phase: train. Epoch: 34. Loss: 0.06433673202991486\n",
      "34 train 16 204\n",
      "Phase: train. Epoch: 34. Loss: 0.0636986717581749\n",
      "34 train 17 216\n",
      "Phase: train. Epoch: 34. Loss: 0.052012115716934204\n",
      "34 train 18 228\n",
      "Phase: train. Epoch: 34. Loss: 0.07299412786960602\n",
      "34 train 19 240\n",
      "Phase: train. Epoch: 34. Loss: 0.0730237364768982\n",
      "34 train 20 252\n",
      "Phase: train. Epoch: 34. Loss: 0.06920500099658966\n",
      "34 train 21 264\n",
      "Phase: train. Epoch: 34. Loss: 0.03633388876914978\n",
      "34 train 22 276\n",
      "Phase: train. Epoch: 34. Loss: 0.07205457240343094\n",
      "34 train 23 288\n",
      "Phase: train. Epoch: 34. Loss: 0.07646495848894119\n",
      "34 train 24 300\n",
      "Phase: train. Epoch: 34. Loss: 0.053027860820293427\n",
      "34 train 25 312\n",
      "Phase: train. Epoch: 34. Loss: 0.06830757111310959\n",
      "34 train 26 324\n",
      "Phase: train. Epoch: 34. Loss: 0.03492346033453941\n",
      "34 train 27 336\n",
      "Phase: train. Epoch: 34. Loss: 0.05051938444375992\n",
      "34 train 28 348\n",
      "Phase: train. Epoch: 34. Loss: 0.07016919553279877\n",
      "34 train 29 360\n",
      "Phase: train. Epoch: 34. Loss: 0.05287712439894676\n",
      "34 train 30 372\n",
      "Phase: train. Epoch: 34. Loss: 0.05620991438627243\n",
      "34 train 31 384\n",
      "Phase: train. Epoch: 34. Loss: 0.08506438136100769\n",
      "34 train 32 396\n",
      "Phase: train. Epoch: 34. Loss: 0.05681990087032318\n",
      "34 train 33 408\n",
      "Phase: train. Epoch: 34. Loss: 0.11742953956127167\n",
      "34 train 34 420\n",
      "Phase: train. Epoch: 34. Loss: 0.06242622062563896\n",
      "34 train 35 432\n",
      "Phase: train. Epoch: 34. Loss: 0.05384720489382744\n",
      "34 train 36 444\n",
      "Phase: train. Epoch: 34. Loss: 0.04950275272130966\n",
      "34 train 37 456\n",
      "Phase: train. Epoch: 34. Loss: 0.04498627409338951\n",
      "34 train 38 468\n",
      "Phase: train. Epoch: 34. Loss: 0.06144886463880539\n",
      "34 train 39 480\n",
      "Phase: train. Epoch: 34. Loss: 0.050204645842313766\n",
      "34 train 40 492\n",
      "Phase: train. Epoch: 34. Loss: 0.06192256137728691\n",
      "34 train 41 504\n",
      "Phase: train. Epoch: 34. Loss: 0.059638865292072296\n",
      "34 train 42 516\n",
      "Phase: train. Epoch: 34. Loss: 0.04969664663076401\n",
      "34 train 43 528\n",
      "Phase: train. Epoch: 34. Loss: 0.07957467436790466\n",
      "34 train 44 540\n",
      "Phase: train. Epoch: 34. Loss: 0.04756965488195419\n",
      "34 train 45 552\n",
      "Phase: train. Epoch: 34. Loss: 0.05182771012187004\n",
      "34 train 46 564\n",
      "Phase: train. Epoch: 34. Loss: 0.08439180254936218\n",
      "34 train 47 576\n",
      "Phase: train. Epoch: 34. Loss: 0.061325803399086\n",
      "34 train 48 588\n",
      "Phase: train. Epoch: 34. Loss: 0.0695478618144989\n",
      "34 train 49 600\n",
      "Phase: train. Epoch: 34. Loss: 0.05475123971700668\n",
      "34 train 50 612\n",
      "Phase: train. Epoch: 34. Loss: 0.07806134223937988\n",
      "34 train 51 624\n",
      "Phase: train. Epoch: 34. Loss: 0.07829968631267548\n",
      "34 train 52 636\n",
      "Phase: train. Epoch: 34. Loss: 0.07950751483440399\n",
      "34 train 53 648\n",
      "Phase: train. Epoch: 34. Loss: 0.05053481459617615\n",
      "34 train 54 660\n",
      "Phase: train. Epoch: 34. Loss: 0.05937441438436508\n",
      "34 train 55 672\n",
      "Phase: train. Epoch: 34. Loss: 0.07799600064754486\n",
      "34 train 56 684\n",
      "Phase: train. Epoch: 34. Loss: 0.06023029983043671\n",
      "34 train 57 696\n",
      "Phase: train. Epoch: 34. Loss: 0.0645483136177063\n",
      "34 train 58 708\n",
      "Phase: train. Epoch: 34. Loss: 0.05727493390440941\n",
      "34 train 59 720\n",
      "Phase: train. Epoch: 34. Loss: 0.0967467725276947\n",
      "34 train 60 732\n",
      "Phase: train. Epoch: 34. Loss: 0.06913423538208008\n",
      "34 train 61 744\n",
      "Phase: train. Epoch: 34. Loss: 0.09321513026952744\n",
      "34 train 62 751\n",
      "Phase: train. Epoch: 34. Loss: 0.05990812927484512\n",
      "34 val 0 763\n",
      "Phase: val. Epoch: 34. Loss: 0.05010893940925598\n",
      "34 val 1 775\n",
      "Phase: val. Epoch: 34. Loss: 0.07426854968070984\n",
      "34 val 2 787\n",
      "Phase: val. Epoch: 34. Loss: 0.07304145395755768\n",
      "34 val 3 799\n",
      "Phase: val. Epoch: 34. Loss: 0.07357822358608246\n",
      "34 val 4 811\n",
      "Phase: val. Epoch: 34. Loss: 0.055467672646045685\n",
      "34 val 5 823\n",
      "Phase: val. Epoch: 34. Loss: 0.05211722478270531\n",
      "34 val 6 835\n",
      "Phase: val. Epoch: 34. Loss: 0.03918682038784027\n",
      "34 val 7 847\n",
      "Phase: val. Epoch: 34. Loss: 0.07238137722015381\n",
      "34 val 8 859\n",
      "Phase: val. Epoch: 34. Loss: 0.08711078763008118\n",
      "34 val 9 871\n",
      "Phase: val. Epoch: 34. Loss: 0.06547459214925766\n",
      "34 val 10 883\n",
      "Phase: val. Epoch: 34. Loss: 0.04560060426592827\n",
      "34 val 11 884\n",
      "Phase: val. Epoch: 34. Loss: 0.004325126297771931\n",
      "35 train 0 12\n",
      "Phase: train. Epoch: 35. Loss: 0.08218003809452057\n",
      "35 train 1 24\n",
      "Phase: train. Epoch: 35. Loss: 0.08574571460485458\n",
      "35 train 2 36\n",
      "Phase: train. Epoch: 35. Loss: 0.07586956024169922\n",
      "35 train 3 48\n",
      "Phase: train. Epoch: 35. Loss: 0.05683870241045952\n",
      "35 train 4 60\n",
      "Phase: train. Epoch: 35. Loss: 0.051707468926906586\n",
      "35 train 5 72\n",
      "Phase: train. Epoch: 35. Loss: 0.05256000906229019\n",
      "35 train 6 84\n",
      "Phase: train. Epoch: 35. Loss: 0.08930632472038269\n",
      "35 train 7 96\n",
      "Phase: train. Epoch: 35. Loss: 0.07073822617530823\n",
      "35 train 8 108\n",
      "Phase: train. Epoch: 35. Loss: 0.03959142416715622\n",
      "35 train 9 120\n",
      "Phase: train. Epoch: 35. Loss: 0.08267760276794434\n",
      "35 train 10 132\n",
      "Phase: train. Epoch: 35. Loss: 0.05335745960474014\n",
      "35 train 11 144\n",
      "Phase: train. Epoch: 35. Loss: 0.07291819900274277\n",
      "35 train 12 156\n",
      "Phase: train. Epoch: 35. Loss: 0.05069911479949951\n",
      "35 train 13 168\n",
      "Phase: train. Epoch: 35. Loss: 0.064888134598732\n",
      "35 train 14 180\n",
      "Phase: train. Epoch: 35. Loss: 0.03790311887860298\n",
      "35 train 15 192\n",
      "Phase: train. Epoch: 35. Loss: 0.057707756757736206\n",
      "35 train 16 204\n",
      "Phase: train. Epoch: 35. Loss: 0.05631525069475174\n",
      "35 train 17 216\n",
      "Phase: train. Epoch: 35. Loss: 0.08484425395727158\n",
      "35 train 18 228\n",
      "Phase: train. Epoch: 35. Loss: 0.04670165106654167\n",
      "35 train 19 240\n",
      "Phase: train. Epoch: 35. Loss: 0.05498514324426651\n",
      "35 train 20 252\n",
      "Phase: train. Epoch: 35. Loss: 0.05763249844312668\n",
      "35 train 21 264\n",
      "Phase: train. Epoch: 35. Loss: 0.06932923197746277\n",
      "35 train 22 276\n",
      "Phase: train. Epoch: 35. Loss: 0.06129007041454315\n",
      "35 train 23 288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 35. Loss: 0.10305576026439667\n",
      "35 train 24 300\n",
      "Phase: train. Epoch: 35. Loss: 0.05041761323809624\n",
      "35 train 25 312\n",
      "Phase: train. Epoch: 35. Loss: 0.05707515403628349\n",
      "35 train 26 324\n",
      "Phase: train. Epoch: 35. Loss: 0.04242352396249771\n",
      "35 train 27 336\n",
      "Phase: train. Epoch: 35. Loss: 0.061526697129011154\n",
      "35 train 28 348\n",
      "Phase: train. Epoch: 35. Loss: 0.05839143320918083\n",
      "35 train 29 360\n",
      "Phase: train. Epoch: 35. Loss: 0.04123922437429428\n",
      "35 train 30 372\n",
      "Phase: train. Epoch: 35. Loss: 0.06536789983510971\n",
      "35 train 31 384\n",
      "Phase: train. Epoch: 35. Loss: 0.05141731724143028\n",
      "35 train 32 396\n",
      "Phase: train. Epoch: 35. Loss: 0.06877833604812622\n",
      "35 train 33 408\n",
      "Phase: train. Epoch: 35. Loss: 0.08500837534666061\n",
      "35 train 34 420\n",
      "Phase: train. Epoch: 35. Loss: 0.09166841208934784\n",
      "35 train 35 432\n",
      "Phase: train. Epoch: 35. Loss: 0.09399625658988953\n",
      "35 train 36 444\n",
      "Phase: train. Epoch: 35. Loss: 0.06712524592876434\n",
      "35 train 37 456\n",
      "Phase: train. Epoch: 35. Loss: 0.03365744277834892\n",
      "35 train 38 468\n",
      "Phase: train. Epoch: 35. Loss: 0.0637689158320427\n",
      "35 train 39 480\n",
      "Phase: train. Epoch: 35. Loss: 0.08380061388015747\n",
      "35 train 40 492\n",
      "Phase: train. Epoch: 35. Loss: 0.06385426968336105\n",
      "35 train 41 504\n",
      "Phase: train. Epoch: 35. Loss: 0.05534861236810684\n",
      "35 train 42 516\n",
      "Phase: train. Epoch: 35. Loss: 0.05468614399433136\n",
      "35 train 43 528\n",
      "Phase: train. Epoch: 35. Loss: 0.06246352195739746\n",
      "35 train 44 540\n",
      "Phase: train. Epoch: 35. Loss: 0.05047720670700073\n",
      "35 train 45 552\n",
      "Phase: train. Epoch: 35. Loss: 0.05822674185037613\n",
      "35 train 46 564\n",
      "Phase: train. Epoch: 35. Loss: 0.06365682184696198\n",
      "35 train 47 576\n",
      "Phase: train. Epoch: 35. Loss: 0.085250124335289\n",
      "35 train 48 588\n",
      "Phase: train. Epoch: 35. Loss: 0.07778319716453552\n",
      "35 train 49 600\n",
      "Phase: train. Epoch: 35. Loss: 0.034945048391819\n",
      "35 train 50 612\n",
      "Phase: train. Epoch: 35. Loss: 0.06858475506305695\n",
      "35 train 51 624\n",
      "Phase: train. Epoch: 35. Loss: 0.053072914481163025\n",
      "35 train 52 636\n",
      "Phase: train. Epoch: 35. Loss: 0.058535248041152954\n",
      "35 train 53 648\n",
      "Phase: train. Epoch: 35. Loss: 0.06965856999158859\n",
      "35 train 54 660\n",
      "Phase: train. Epoch: 35. Loss: 0.061385542154312134\n",
      "35 train 55 672\n",
      "Phase: train. Epoch: 35. Loss: 0.08420120179653168\n",
      "35 train 56 684\n",
      "Phase: train. Epoch: 35. Loss: 0.05905526131391525\n",
      "35 train 57 696\n",
      "Phase: train. Epoch: 35. Loss: 0.06171024963259697\n",
      "35 train 58 708\n",
      "Phase: train. Epoch: 35. Loss: 0.07851780205965042\n",
      "35 train 59 720\n",
      "Phase: train. Epoch: 35. Loss: 0.0676422119140625\n",
      "35 train 60 732\n",
      "Phase: train. Epoch: 35. Loss: 0.05274791643023491\n",
      "35 train 61 744\n",
      "Phase: train. Epoch: 35. Loss: 0.07560328394174576\n",
      "35 train 62 751\n",
      "Phase: train. Epoch: 35. Loss: 0.031536247581243515\n",
      "35 val 0 763\n",
      "Phase: val. Epoch: 35. Loss: 0.03776288777589798\n",
      "35 val 1 775\n",
      "Phase: val. Epoch: 35. Loss: 0.05554623529314995\n",
      "35 val 2 787\n",
      "Phase: val. Epoch: 35. Loss: 0.06735597550868988\n",
      "35 val 3 799\n",
      "Phase: val. Epoch: 35. Loss: 0.04548877105116844\n",
      "35 val 4 811\n",
      "Phase: val. Epoch: 35. Loss: 0.09927164018154144\n",
      "35 val 5 823\n",
      "Phase: val. Epoch: 35. Loss: 0.06390343606472015\n",
      "35 val 6 835\n",
      "Phase: val. Epoch: 35. Loss: 0.06106702610850334\n",
      "35 val 7 847\n",
      "Phase: val. Epoch: 35. Loss: 0.043148480355739594\n",
      "35 val 8 859\n",
      "Phase: val. Epoch: 35. Loss: 0.036490701138973236\n",
      "35 val 9 871\n",
      "Phase: val. Epoch: 35. Loss: 0.071358323097229\n",
      "35 val 10 883\n",
      "Phase: val. Epoch: 35. Loss: 0.07788582146167755\n",
      "35 val 11 884\n",
      "Phase: val. Epoch: 35. Loss: 0.04771668463945389\n",
      "36 train 0 12\n",
      "Phase: train. Epoch: 36. Loss: 0.13525772094726562\n",
      "36 train 1 24\n",
      "Phase: train. Epoch: 36. Loss: 0.06212075054645538\n",
      "36 train 2 36\n",
      "Phase: train. Epoch: 36. Loss: 0.0809720903635025\n",
      "36 train 3 48\n",
      "Phase: train. Epoch: 36. Loss: 0.06559731066226959\n",
      "36 train 4 60\n",
      "Phase: train. Epoch: 36. Loss: 0.08301562070846558\n",
      "36 train 5 72\n",
      "Phase: train. Epoch: 36. Loss: 0.0707320049405098\n",
      "36 train 6 84\n",
      "Phase: train. Epoch: 36. Loss: 0.05403368920087814\n",
      "36 train 7 96\n",
      "Phase: train. Epoch: 36. Loss: 0.06375502049922943\n",
      "36 train 8 108\n",
      "Phase: train. Epoch: 36. Loss: 0.08323201537132263\n",
      "36 train 9 120\n",
      "Phase: train. Epoch: 36. Loss: 0.07646064460277557\n",
      "36 train 10 132\n",
      "Phase: train. Epoch: 36. Loss: 0.08265688270330429\n",
      "36 train 11 144\n",
      "Phase: train. Epoch: 36. Loss: 0.09751313924789429\n",
      "36 train 12 156\n",
      "Phase: train. Epoch: 36. Loss: 0.06195822358131409\n",
      "36 train 13 168\n",
      "Phase: train. Epoch: 36. Loss: 0.07161151617765427\n",
      "36 train 14 180\n",
      "Phase: train. Epoch: 36. Loss: 0.05131687968969345\n",
      "36 train 15 192\n",
      "Phase: train. Epoch: 36. Loss: 0.043578140437603\n",
      "36 train 16 204\n",
      "Phase: train. Epoch: 36. Loss: 0.05475839599967003\n",
      "36 train 17 216\n",
      "Phase: train. Epoch: 36. Loss: 0.04257402941584587\n",
      "36 train 18 228\n",
      "Phase: train. Epoch: 36. Loss: 0.06157552823424339\n",
      "36 train 19 240\n",
      "Phase: train. Epoch: 36. Loss: 0.06939980387687683\n",
      "36 train 20 252\n",
      "Phase: train. Epoch: 36. Loss: 0.03208814188838005\n",
      "36 train 21 264\n",
      "Phase: train. Epoch: 36. Loss: 0.05904011055827141\n",
      "36 train 22 276\n",
      "Phase: train. Epoch: 36. Loss: 0.0659598782658577\n",
      "36 train 23 288\n",
      "Phase: train. Epoch: 36. Loss: 0.03837057948112488\n",
      "36 train 24 300\n",
      "Phase: train. Epoch: 36. Loss: 0.05123297497630119\n",
      "36 train 25 312\n",
      "Phase: train. Epoch: 36. Loss: 0.028547104448080063\n",
      "36 train 26 324\n",
      "Phase: train. Epoch: 36. Loss: 0.059412144124507904\n",
      "36 train 27 336\n",
      "Phase: train. Epoch: 36. Loss: 0.05909102410078049\n",
      "36 train 28 348\n",
      "Phase: train. Epoch: 36. Loss: 0.04512956365942955\n",
      "36 train 29 360\n",
      "Phase: train. Epoch: 36. Loss: 0.08524072170257568\n",
      "36 train 30 372\n",
      "Phase: train. Epoch: 36. Loss: 0.0500778965651989\n",
      "36 train 31 384\n",
      "Phase: train. Epoch: 36. Loss: 0.06196567416191101\n",
      "36 train 32 396\n",
      "Phase: train. Epoch: 36. Loss: 0.06597734242677689\n",
      "36 train 33 408\n",
      "Phase: train. Epoch: 36. Loss: 0.08666783571243286\n",
      "36 train 34 420\n",
      "Phase: train. Epoch: 36. Loss: 0.06665179133415222\n",
      "36 train 35 432\n",
      "Phase: train. Epoch: 36. Loss: 0.07880546152591705\n",
      "36 train 36 444\n",
      "Phase: train. Epoch: 36. Loss: 0.06843330711126328\n",
      "36 train 37 456\n",
      "Phase: train. Epoch: 36. Loss: 0.06225253641605377\n",
      "36 train 38 468\n",
      "Phase: train. Epoch: 36. Loss: 0.04955219477415085\n",
      "36 train 39 480\n",
      "Phase: train. Epoch: 36. Loss: 0.056668683886528015\n",
      "36 train 40 492\n",
      "Phase: train. Epoch: 36. Loss: 0.06991268694400787\n",
      "36 train 41 504\n",
      "Phase: train. Epoch: 36. Loss: 0.05194743722677231\n",
      "36 train 42 516\n",
      "Phase: train. Epoch: 36. Loss: 0.056443579494953156\n",
      "36 train 43 528\n",
      "Phase: train. Epoch: 36. Loss: 0.0426768958568573\n",
      "36 train 44 540\n",
      "Phase: train. Epoch: 36. Loss: 0.057717591524124146\n",
      "36 train 45 552\n",
      "Phase: train. Epoch: 36. Loss: 0.05659468472003937\n",
      "36 train 46 564\n",
      "Phase: train. Epoch: 36. Loss: 0.0704897940158844\n",
      "36 train 47 576\n",
      "Phase: train. Epoch: 36. Loss: 0.07835517823696136\n",
      "36 train 48 588\n",
      "Phase: train. Epoch: 36. Loss: 0.06423932313919067\n",
      "36 train 49 600\n",
      "Phase: train. Epoch: 36. Loss: 0.09057554602622986\n",
      "36 train 50 612\n",
      "Phase: train. Epoch: 36. Loss: 0.10136507451534271\n",
      "36 train 51 624\n",
      "Phase: train. Epoch: 36. Loss: 0.043232809752225876\n",
      "36 train 52 636\n",
      "Phase: train. Epoch: 36. Loss: 0.06491856276988983\n",
      "36 train 53 648\n",
      "Phase: train. Epoch: 36. Loss: 0.07240615785121918\n",
      "36 train 54 660\n",
      "Phase: train. Epoch: 36. Loss: 0.0777202770113945\n",
      "36 train 55 672\n",
      "Phase: train. Epoch: 36. Loss: 0.07364337891340256\n",
      "36 train 56 684\n",
      "Phase: train. Epoch: 36. Loss: 0.06393460929393768\n",
      "36 train 57 696\n",
      "Phase: train. Epoch: 36. Loss: 0.07553597539663315\n",
      "36 train 58 708\n",
      "Phase: train. Epoch: 36. Loss: 0.05591354891657829\n",
      "36 train 59 720\n",
      "Phase: train. Epoch: 36. Loss: 0.061006080359220505\n",
      "36 train 60 732\n",
      "Phase: train. Epoch: 36. Loss: 0.04869640991091728\n",
      "36 train 61 744\n",
      "Phase: train. Epoch: 36. Loss: 0.072482630610466\n",
      "36 train 62 751\n",
      "Phase: train. Epoch: 36. Loss: 0.056158725172281265\n",
      "36 val 0 763\n",
      "Phase: val. Epoch: 36. Loss: 0.06094927713274956\n",
      "36 val 1 775\n",
      "Phase: val. Epoch: 36. Loss: 0.06506860256195068\n",
      "36 val 2 787\n",
      "Phase: val. Epoch: 36. Loss: 0.06892360746860504\n",
      "36 val 3 799\n",
      "Phase: val. Epoch: 36. Loss: 0.05966971442103386\n",
      "36 val 4 811\n",
      "Phase: val. Epoch: 36. Loss: 0.04260393977165222\n",
      "36 val 5 823\n",
      "Phase: val. Epoch: 36. Loss: 0.049826912581920624\n",
      "36 val 6 835\n",
      "Phase: val. Epoch: 36. Loss: 0.09954993426799774\n",
      "36 val 7 847\n",
      "Phase: val. Epoch: 36. Loss: 0.03923416882753372\n",
      "36 val 8 859\n",
      "Phase: val. Epoch: 36. Loss: 0.044431209564208984\n",
      "36 val 9 871\n",
      "Phase: val. Epoch: 36. Loss: 0.0638202652335167\n",
      "36 val 10 883\n",
      "Phase: val. Epoch: 36. Loss: 0.05555259436368942\n",
      "36 val 11 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val. Epoch: 36. Loss: 0.003771658521145582\n",
      "37 train 0 12\n",
      "Phase: train. Epoch: 37. Loss: 0.08102937042713165\n",
      "37 train 1 24\n",
      "Phase: train. Epoch: 37. Loss: 0.047159116715192795\n",
      "37 train 2 36\n",
      "Phase: train. Epoch: 37. Loss: 0.09719139337539673\n",
      "37 train 3 48\n",
      "Phase: train. Epoch: 37. Loss: 0.0607350692152977\n",
      "37 train 4 60\n",
      "Phase: train. Epoch: 37. Loss: 0.05017954856157303\n",
      "37 train 5 72\n",
      "Phase: train. Epoch: 37. Loss: 0.054869383573532104\n",
      "37 train 6 84\n",
      "Phase: train. Epoch: 37. Loss: 0.06504976004362106\n",
      "37 train 7 96\n",
      "Phase: train. Epoch: 37. Loss: 0.058054547756910324\n",
      "37 train 8 108\n",
      "Phase: train. Epoch: 37. Loss: 0.06770963221788406\n",
      "37 train 9 120\n",
      "Phase: train. Epoch: 37. Loss: 0.046858977526426315\n",
      "37 train 10 132\n",
      "Phase: train. Epoch: 37. Loss: 0.06184084713459015\n",
      "37 train 11 144\n",
      "Phase: train. Epoch: 37. Loss: 0.036086782813072205\n",
      "37 train 12 156\n",
      "Phase: train. Epoch: 37. Loss: 0.07631205767393112\n",
      "37 train 13 168\n",
      "Phase: train. Epoch: 37. Loss: 0.08186870068311691\n",
      "37 train 14 180\n",
      "Phase: train. Epoch: 37. Loss: 0.07097738981246948\n",
      "37 train 15 192\n",
      "Phase: train. Epoch: 37. Loss: 0.07829437404870987\n",
      "37 train 16 204\n",
      "Phase: train. Epoch: 37. Loss: 0.05774940177798271\n",
      "37 train 17 216\n",
      "Phase: train. Epoch: 37. Loss: 0.07590415328741074\n",
      "37 train 18 228\n",
      "Phase: train. Epoch: 37. Loss: 0.04669835418462753\n",
      "37 train 19 240\n",
      "Phase: train. Epoch: 37. Loss: 0.056181252002716064\n",
      "37 train 20 252\n",
      "Phase: train. Epoch: 37. Loss: 0.06451544165611267\n",
      "37 train 21 264\n",
      "Phase: train. Epoch: 37. Loss: 0.04193602502346039\n",
      "37 train 22 276\n",
      "Phase: train. Epoch: 37. Loss: 0.05393626540899277\n",
      "37 train 23 288\n",
      "Phase: train. Epoch: 37. Loss: 0.03692720830440521\n",
      "37 train 24 300\n",
      "Phase: train. Epoch: 37. Loss: 0.05646854266524315\n",
      "37 train 25 312\n",
      "Phase: train. Epoch: 37. Loss: 0.0424046665430069\n",
      "37 train 26 324\n",
      "Phase: train. Epoch: 37. Loss: 0.04212963208556175\n",
      "37 train 27 336\n",
      "Phase: train. Epoch: 37. Loss: 0.059236422181129456\n",
      "37 train 28 348\n",
      "Phase: train. Epoch: 37. Loss: 0.060155238956213\n",
      "37 train 29 360\n",
      "Phase: train. Epoch: 37. Loss: 0.07414491474628448\n",
      "37 train 30 372\n",
      "Phase: train. Epoch: 37. Loss: 0.06797115504741669\n",
      "37 train 31 384\n",
      "Phase: train. Epoch: 37. Loss: 0.03986448794603348\n",
      "37 train 32 396\n",
      "Phase: train. Epoch: 37. Loss: 0.05727740377187729\n",
      "37 train 33 408\n",
      "Phase: train. Epoch: 37. Loss: 0.04458331689238548\n",
      "37 train 34 420\n",
      "Phase: train. Epoch: 37. Loss: 0.07879621535539627\n",
      "37 train 35 432\n",
      "Phase: train. Epoch: 37. Loss: 0.08366121351718903\n",
      "37 train 36 444\n",
      "Phase: train. Epoch: 37. Loss: 0.044012121856212616\n",
      "37 train 37 456\n",
      "Phase: train. Epoch: 37. Loss: 0.05888988822698593\n",
      "37 train 38 468\n",
      "Phase: train. Epoch: 37. Loss: 0.04923553392291069\n",
      "37 train 39 480\n",
      "Phase: train. Epoch: 37. Loss: 0.04994521662592888\n",
      "37 train 40 492\n",
      "Phase: train. Epoch: 37. Loss: 0.05632490664720535\n",
      "37 train 41 504\n",
      "Phase: train. Epoch: 37. Loss: 0.0951019823551178\n",
      "37 train 42 516\n",
      "Phase: train. Epoch: 37. Loss: 0.04927906394004822\n",
      "37 train 43 528\n",
      "Phase: train. Epoch: 37. Loss: 0.03855661675333977\n",
      "37 train 44 540\n",
      "Phase: train. Epoch: 37. Loss: 0.08282598853111267\n",
      "37 train 45 552\n",
      "Phase: train. Epoch: 37. Loss: 0.05554003641009331\n",
      "37 train 46 564\n",
      "Phase: train. Epoch: 37. Loss: 0.055230479687452316\n",
      "37 train 47 576\n",
      "Phase: train. Epoch: 37. Loss: 0.06740422546863556\n",
      "37 train 48 588\n",
      "Phase: train. Epoch: 37. Loss: 0.07265625149011612\n",
      "37 train 49 600\n",
      "Phase: train. Epoch: 37. Loss: 0.07388925552368164\n",
      "37 train 50 612\n",
      "Phase: train. Epoch: 37. Loss: 0.05657140538096428\n",
      "37 train 51 624\n",
      "Phase: train. Epoch: 37. Loss: 0.10534723103046417\n",
      "37 train 52 636\n",
      "Phase: train. Epoch: 37. Loss: 0.05764719843864441\n",
      "37 train 53 648\n",
      "Phase: train. Epoch: 37. Loss: 0.07681286334991455\n",
      "37 train 54 660\n",
      "Phase: train. Epoch: 37. Loss: 0.05933818966150284\n",
      "37 train 55 672\n",
      "Phase: train. Epoch: 37. Loss: 0.06695078313350677\n",
      "37 train 56 684\n",
      "Phase: train. Epoch: 37. Loss: 0.050202738493680954\n",
      "37 train 57 696\n",
      "Phase: train. Epoch: 37. Loss: 0.09610697627067566\n",
      "37 train 58 708\n",
      "Phase: train. Epoch: 37. Loss: 0.056052885949611664\n",
      "37 train 59 720\n",
      "Phase: train. Epoch: 37. Loss: 0.0721379816532135\n",
      "37 train 60 732\n",
      "Phase: train. Epoch: 37. Loss: 0.06841602176427841\n",
      "37 train 61 744\n",
      "Phase: train. Epoch: 37. Loss: 0.05230743810534477\n",
      "37 train 62 751\n",
      "Phase: train. Epoch: 37. Loss: 0.08504226058721542\n",
      "37 val 0 763\n",
      "Phase: val. Epoch: 37. Loss: 0.06827746331691742\n",
      "37 val 1 775\n",
      "Phase: val. Epoch: 37. Loss: 0.041516467928886414\n",
      "37 val 2 787\n",
      "Phase: val. Epoch: 37. Loss: 0.09499610960483551\n",
      "37 val 3 799\n",
      "Phase: val. Epoch: 37. Loss: 0.031094243749976158\n",
      "37 val 4 811\n",
      "Phase: val. Epoch: 37. Loss: 0.06757285445928574\n",
      "37 val 5 823\n",
      "Phase: val. Epoch: 37. Loss: 0.05717980116605759\n",
      "37 val 6 835\n",
      "Phase: val. Epoch: 37. Loss: 0.045089222490787506\n",
      "37 val 7 847\n",
      "Phase: val. Epoch: 37. Loss: 0.05247814208269119\n",
      "37 val 8 859\n",
      "Phase: val. Epoch: 37. Loss: 0.0426030308008194\n",
      "37 val 9 871\n",
      "Phase: val. Epoch: 37. Loss: 0.06002943962812424\n",
      "37 val 10 883\n",
      "Phase: val. Epoch: 37. Loss: 0.07290619611740112\n",
      "37 val 11 884\n",
      "Phase: val. Epoch: 37. Loss: 0.15937840938568115\n",
      "38 train 0 12\n",
      "Phase: train. Epoch: 38. Loss: 0.055679574608802795\n",
      "38 train 1 24\n",
      "Phase: train. Epoch: 38. Loss: 0.08003854006528854\n",
      "38 train 2 36\n",
      "Phase: train. Epoch: 38. Loss: 0.04831352084875107\n",
      "38 train 3 48\n",
      "Phase: train. Epoch: 38. Loss: 0.03232920169830322\n",
      "38 train 4 60\n",
      "Phase: train. Epoch: 38. Loss: 0.04726063460111618\n",
      "38 train 5 72\n",
      "Phase: train. Epoch: 38. Loss: 0.06962130963802338\n",
      "38 train 6 84\n",
      "Phase: train. Epoch: 38. Loss: 0.06503766030073166\n",
      "38 train 7 96\n",
      "Phase: train. Epoch: 38. Loss: 0.05691438913345337\n",
      "38 train 8 108\n",
      "Phase: train. Epoch: 38. Loss: 0.050173621624708176\n",
      "38 train 9 120\n",
      "Phase: train. Epoch: 38. Loss: 0.0696195736527443\n",
      "38 train 10 132\n",
      "Phase: train. Epoch: 38. Loss: 0.07715945690870285\n",
      "38 train 11 144\n",
      "Phase: train. Epoch: 38. Loss: 0.0541333332657814\n",
      "38 train 12 156\n",
      "Phase: train. Epoch: 38. Loss: 0.053363196551799774\n",
      "38 train 13 168\n",
      "Phase: train. Epoch: 38. Loss: 0.08967314660549164\n",
      "38 train 14 180\n",
      "Phase: train. Epoch: 38. Loss: 0.04821319505572319\n",
      "38 train 15 192\n",
      "Phase: train. Epoch: 38. Loss: 0.05212235450744629\n",
      "38 train 16 204\n",
      "Phase: train. Epoch: 38. Loss: 0.05178969353437424\n",
      "38 train 17 216\n",
      "Phase: train. Epoch: 38. Loss: 0.09091109037399292\n",
      "38 train 18 228\n",
      "Phase: train. Epoch: 38. Loss: 0.11670815199613571\n",
      "38 train 19 240\n",
      "Phase: train. Epoch: 38. Loss: 0.05347609147429466\n",
      "38 train 20 252\n",
      "Phase: train. Epoch: 38. Loss: 0.06068843603134155\n",
      "38 train 21 264\n",
      "Phase: train. Epoch: 38. Loss: 0.07298548519611359\n",
      "38 train 22 276\n",
      "Phase: train. Epoch: 38. Loss: 0.0860360711812973\n",
      "38 train 23 288\n",
      "Phase: train. Epoch: 38. Loss: 0.05360628291964531\n",
      "38 train 24 300\n",
      "Phase: train. Epoch: 38. Loss: 0.05724906176328659\n",
      "38 train 25 312\n",
      "Phase: train. Epoch: 38. Loss: 0.08048194646835327\n",
      "38 train 26 324\n",
      "Phase: train. Epoch: 38. Loss: 0.06629212200641632\n",
      "38 train 27 336\n",
      "Phase: train. Epoch: 38. Loss: 0.04674312472343445\n",
      "38 train 28 348\n",
      "Phase: train. Epoch: 38. Loss: 0.04717279598116875\n",
      "38 train 29 360\n",
      "Phase: train. Epoch: 38. Loss: 0.06874111294746399\n",
      "38 train 30 372\n",
      "Phase: train. Epoch: 38. Loss: 0.06011175364255905\n",
      "38 train 31 384\n",
      "Phase: train. Epoch: 38. Loss: 0.06760561466217041\n",
      "38 train 32 396\n",
      "Phase: train. Epoch: 38. Loss: 0.05982404574751854\n",
      "38 train 33 408\n",
      "Phase: train. Epoch: 38. Loss: 0.06185172125697136\n",
      "38 train 34 420\n",
      "Phase: train. Epoch: 38. Loss: 0.06422287970781326\n",
      "38 train 35 432\n",
      "Phase: train. Epoch: 38. Loss: 0.060111574828624725\n",
      "38 train 36 444\n",
      "Phase: train. Epoch: 38. Loss: 0.05746100842952728\n",
      "38 train 37 456\n",
      "Phase: train. Epoch: 38. Loss: 0.06036490947008133\n",
      "38 train 38 468\n",
      "Phase: train. Epoch: 38. Loss: 0.06054822728037834\n",
      "38 train 39 480\n",
      "Phase: train. Epoch: 38. Loss: 0.06530090421438217\n",
      "38 train 40 492\n",
      "Phase: train. Epoch: 38. Loss: 0.1041363775730133\n",
      "38 train 41 504\n",
      "Phase: train. Epoch: 38. Loss: 0.05378603935241699\n",
      "38 train 42 516\n",
      "Phase: train. Epoch: 38. Loss: 0.05976541340351105\n",
      "38 train 43 528\n",
      "Phase: train. Epoch: 38. Loss: 0.04112604632973671\n",
      "38 train 44 540\n",
      "Phase: train. Epoch: 38. Loss: 0.050218772143125534\n",
      "38 train 45 552\n",
      "Phase: train. Epoch: 38. Loss: 0.06622686237096786\n",
      "38 train 46 564\n",
      "Phase: train. Epoch: 38. Loss: 0.06126609072089195\n",
      "38 train 47 576\n",
      "Phase: train. Epoch: 38. Loss: 0.060197196900844574\n",
      "38 train 48 588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train. Epoch: 38. Loss: 0.03966555744409561\n",
      "38 train 49 600\n",
      "Phase: train. Epoch: 38. Loss: 0.08220671117305756\n",
      "38 train 50 612\n",
      "Phase: train. Epoch: 38. Loss: 0.0876486748456955\n",
      "38 train 51 624\n",
      "Phase: train. Epoch: 38. Loss: 0.05364183336496353\n",
      "38 train 52 636\n",
      "Phase: train. Epoch: 38. Loss: 0.10220708698034286\n",
      "38 train 53 648\n",
      "Phase: train. Epoch: 38. Loss: 0.06283364444971085\n",
      "38 train 54 660\n",
      "Phase: train. Epoch: 38. Loss: 0.07993879914283752\n",
      "38 train 55 672\n",
      "Phase: train. Epoch: 38. Loss: 0.06317532062530518\n",
      "38 train 56 684\n",
      "Phase: train. Epoch: 38. Loss: 0.08510512113571167\n",
      "38 train 57 696\n",
      "Phase: train. Epoch: 38. Loss: 0.04567185044288635\n",
      "38 train 58 708\n",
      "Phase: train. Epoch: 38. Loss: 0.05940873548388481\n",
      "38 train 59 720\n",
      "Phase: train. Epoch: 38. Loss: 0.07591070234775543\n",
      "38 train 60 732\n",
      "Phase: train. Epoch: 38. Loss: 0.04819771647453308\n",
      "38 train 61 744\n",
      "Phase: train. Epoch: 38. Loss: 0.06413796544075012\n",
      "38 train 62 751\n",
      "Phase: train. Epoch: 38. Loss: 0.07427060604095459\n",
      "38 val 0 763\n",
      "Phase: val. Epoch: 38. Loss: 0.04478951543569565\n",
      "38 val 1 775\n",
      "Phase: val. Epoch: 38. Loss: 0.0975571870803833\n",
      "38 val 2 787\n",
      "Phase: val. Epoch: 38. Loss: 0.05999455973505974\n",
      "38 val 3 799\n",
      "Phase: val. Epoch: 38. Loss: 0.05982484668493271\n",
      "38 val 4 811\n",
      "Phase: val. Epoch: 38. Loss: 0.0494617335498333\n",
      "38 val 5 823\n",
      "Phase: val. Epoch: 38. Loss: 0.05380808934569359\n",
      "38 val 6 835\n",
      "Phase: val. Epoch: 38. Loss: 0.1133345365524292\n",
      "38 val 7 847\n",
      "Phase: val. Epoch: 38. Loss: 0.03945973142981529\n",
      "38 val 8 859\n",
      "Phase: val. Epoch: 38. Loss: 0.06666342914104462\n",
      "38 val 9 871\n",
      "Phase: val. Epoch: 38. Loss: 0.05075681954622269\n",
      "38 val 10 883\n",
      "Phase: val. Epoch: 38. Loss: 0.0527043454349041\n",
      "38 val 11 884\n",
      "Phase: val. Epoch: 38. Loss: 0.03702627122402191\n",
      "39 train 0 12\n",
      "Phase: train. Epoch: 39. Loss: 0.07426401227712631\n",
      "39 train 1 24\n",
      "Phase: train. Epoch: 39. Loss: 0.07528620213270187\n",
      "39 train 2 36\n",
      "Phase: train. Epoch: 39. Loss: 0.045029960572719574\n",
      "39 train 3 48\n",
      "Phase: train. Epoch: 39. Loss: 0.10014578700065613\n",
      "39 train 4 60\n",
      "Phase: train. Epoch: 39. Loss: 0.04852538928389549\n",
      "39 train 5 72\n",
      "Phase: train. Epoch: 39. Loss: 0.06460721790790558\n",
      "39 train 6 84\n",
      "Phase: train. Epoch: 39. Loss: 0.0530177466571331\n",
      "39 train 7 96\n",
      "Phase: train. Epoch: 39. Loss: 0.06428752839565277\n",
      "39 train 8 108\n",
      "Phase: train. Epoch: 39. Loss: 0.05528675764799118\n",
      "39 train 9 120\n",
      "Phase: train. Epoch: 39. Loss: 0.06850002706050873\n",
      "39 train 10 132\n",
      "Phase: train. Epoch: 39. Loss: 0.05686270445585251\n",
      "39 train 11 144\n",
      "Phase: train. Epoch: 39. Loss: 0.046435605734586716\n",
      "39 train 12 156\n",
      "Phase: train. Epoch: 39. Loss: 0.022268077358603477\n",
      "39 train 13 168\n",
      "Phase: train. Epoch: 39. Loss: 0.07983864843845367\n",
      "39 train 14 180\n",
      "Phase: train. Epoch: 39. Loss: 0.09092390537261963\n",
      "39 train 15 192\n",
      "Phase: train. Epoch: 39. Loss: 0.0716799721121788\n",
      "39 train 16 204\n",
      "Phase: train. Epoch: 39. Loss: 0.05464210361242294\n",
      "39 train 17 216\n",
      "Phase: train. Epoch: 39. Loss: 0.05387973040342331\n",
      "39 train 18 228\n",
      "Phase: train. Epoch: 39. Loss: 0.06186962127685547\n",
      "39 train 19 240\n",
      "Phase: train. Epoch: 39. Loss: 0.06669366359710693\n",
      "39 train 20 252\n",
      "Phase: train. Epoch: 39. Loss: 0.054260797798633575\n",
      "39 train 21 264\n",
      "Phase: train. Epoch: 39. Loss: 0.07530295848846436\n",
      "39 train 22 276\n",
      "Phase: train. Epoch: 39. Loss: 0.052063606679439545\n",
      "39 train 23 288\n",
      "Phase: train. Epoch: 39. Loss: 0.06904137134552002\n",
      "39 train 24 300\n",
      "Phase: train. Epoch: 39. Loss: 0.05675103887915611\n",
      "39 train 25 312\n",
      "Phase: train. Epoch: 39. Loss: 0.07592667639255524\n",
      "39 train 26 324\n",
      "Phase: train. Epoch: 39. Loss: 0.03041880950331688\n",
      "39 train 27 336\n",
      "Phase: train. Epoch: 39. Loss: 0.07772959768772125\n",
      "39 train 28 348\n",
      "Phase: train. Epoch: 39. Loss: 0.048737965524196625\n",
      "39 train 29 360\n",
      "Phase: train. Epoch: 39. Loss: 0.06288871169090271\n",
      "39 train 30 372\n",
      "Phase: train. Epoch: 39. Loss: 0.04002894088625908\n",
      "39 train 31 384\n",
      "Phase: train. Epoch: 39. Loss: 0.08062013983726501\n",
      "39 train 32 396\n",
      "Phase: train. Epoch: 39. Loss: 0.08659788221120834\n",
      "39 train 33 408\n",
      "Phase: train. Epoch: 39. Loss: 0.05020450800657272\n",
      "39 train 34 420\n",
      "Phase: train. Epoch: 39. Loss: 0.0610765740275383\n",
      "39 train 35 432\n",
      "Phase: train. Epoch: 39. Loss: 0.06650952994823456\n",
      "39 train 36 444\n",
      "Phase: train. Epoch: 39. Loss: 0.10653429478406906\n",
      "39 train 37 456\n",
      "Phase: train. Epoch: 39. Loss: 0.05619693547487259\n",
      "39 train 38 468\n",
      "Phase: train. Epoch: 39. Loss: 0.061761852353811264\n",
      "39 train 39 480\n",
      "Phase: train. Epoch: 39. Loss: 0.08561742305755615\n",
      "39 train 40 492\n",
      "Phase: train. Epoch: 39. Loss: 0.06102939695119858\n",
      "39 train 41 504\n",
      "Phase: train. Epoch: 39. Loss: 0.08115287125110626\n",
      "39 train 42 516\n",
      "Phase: train. Epoch: 39. Loss: 0.0913817435503006\n",
      "39 train 43 528\n",
      "Phase: train. Epoch: 39. Loss: 0.0487847626209259\n",
      "39 train 44 540\n",
      "Phase: train. Epoch: 39. Loss: 0.038758229464292526\n",
      "39 train 45 552\n",
      "Phase: train. Epoch: 39. Loss: 0.058643899857997894\n",
      "39 train 46 564\n",
      "Phase: train. Epoch: 39. Loss: 0.06809455156326294\n",
      "39 train 47 576\n",
      "Phase: train. Epoch: 39. Loss: 0.022638633847236633\n",
      "39 train 48 588\n",
      "Phase: train. Epoch: 39. Loss: 0.04757747799158096\n",
      "39 train 49 600\n",
      "Phase: train. Epoch: 39. Loss: 0.06496568024158478\n",
      "39 train 50 612\n",
      "Phase: train. Epoch: 39. Loss: 0.08787771314382553\n",
      "39 train 51 624\n",
      "Phase: train. Epoch: 39. Loss: 0.0343245193362236\n",
      "39 train 52 636\n",
      "Phase: train. Epoch: 39. Loss: 0.03978240489959717\n",
      "39 train 53 648\n",
      "Phase: train. Epoch: 39. Loss: 0.11570242047309875\n",
      "39 train 54 660\n",
      "Phase: train. Epoch: 39. Loss: 0.04797495901584625\n",
      "39 train 55 672\n",
      "Phase: train. Epoch: 39. Loss: 0.05161386728286743\n",
      "39 train 56 684\n",
      "Phase: train. Epoch: 39. Loss: 0.157107412815094\n",
      "39 train 57 696\n",
      "Phase: train. Epoch: 39. Loss: 0.0613764226436615\n",
      "39 train 58 708\n",
      "Phase: train. Epoch: 39. Loss: 0.07988740503787994\n",
      "39 train 59 720\n",
      "Phase: train. Epoch: 39. Loss: 0.07896396517753601\n",
      "39 train 60 732\n",
      "Phase: train. Epoch: 39. Loss: 0.06575720012187958\n",
      "39 train 61 744\n",
      "Phase: train. Epoch: 39. Loss: 0.088407002389431\n",
      "39 train 62 751\n",
      "Phase: train. Epoch: 39. Loss: 0.05310780927538872\n",
      "39 val 0 763\n",
      "Phase: val. Epoch: 39. Loss: 0.08503159880638123\n",
      "39 val 1 775\n",
      "Phase: val. Epoch: 39. Loss: 0.061478644609451294\n",
      "39 val 2 787\n",
      "Phase: val. Epoch: 39. Loss: 0.10060402005910873\n",
      "39 val 3 799\n",
      "Phase: val. Epoch: 39. Loss: 0.04417505860328674\n",
      "39 val 4 811\n",
      "Phase: val. Epoch: 39. Loss: 0.09147383272647858\n",
      "39 val 5 823\n",
      "Phase: val. Epoch: 39. Loss: 0.08644508570432663\n",
      "39 val 6 835\n",
      "Phase: val. Epoch: 39. Loss: 0.06850102543830872\n",
      "39 val 7 847\n",
      "Phase: val. Epoch: 39. Loss: 0.07619847357273102\n",
      "39 val 8 859\n",
      "Phase: val. Epoch: 39. Loss: 0.07524947822093964\n",
      "39 val 9 871\n",
      "Phase: val. Epoch: 39. Loss: 0.06077338755130768\n",
      "39 val 10 883\n",
      "Phase: val. Epoch: 39. Loss: 0.08218365907669067\n",
      "39 val 11 884\n",
      "Phase: val. Epoch: 39. Loss: 0.0789465680718422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Training Cell \n",
    "\n",
    "brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, model_state = train_validate(train_dataset, valid_dataset,lr)\n",
    "    \n",
    "date = datetime.now()\n",
    "pkl_name = \"results_TBI_model-End-\" + str(date.date()) + '-' + str(date.hour) + '.pkl'\n",
    "\n",
    "torch.save(model_state, os.path.join(os.getcwd(), \"Registered_Brains_FA/models_saved\", \"TBI_model-End-\" + str(date.date()) + '-' + str(date.hour) +\".pt\"))\n",
    "    \n",
    "# Saving the objects:\n",
    "with open(pkl_name, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_TBI_model-End-2020-09-30-19.pkl\n",
      "0.0\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "GPU is active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:29,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "#Only run this when the model has gone through training. \n",
    "\n",
    "del model #This removes any confliction with an existing model running on the GPU. \n",
    "model = smp.Unet(encoder_name = ENCODER, in_channels=1, classes = 1, aux_params = aux_params)\n",
    "    \n",
    "#read pickle\n",
    "if 'pkl_name' not in locals():\n",
    "    pkl_name = \"results_TBI_model-End-2020-09-30-13.pkl\"\n",
    "print(pkl_name)\n",
    "\n",
    "with open(pkl_name,'rb') as f:  \n",
    "    brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, test_dataset = pickle.load(f)\n",
    "    \n",
    "modelPath = \"/home/mccrinbc/Registered_Brains_FA/models_saved/TBI_model-End-2020-09-30-19.pt\"\n",
    "#modelPath = \"/Users/brianmccrindle/Documents/Research/TBIFinder_Final/Registered_Brains_FA/models_saved/TBI_model-epoch2-2020-08-27-9-55.pt\"\n",
    "    \n",
    "thresholds = np.array(range(101)) / 100\n",
    "#thresholds = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "TPR_list = []\n",
    "FPR_list = []\n",
    "for threshold in thresholds:\n",
    "    print(threshold)\n",
    "    #test the model to capture performance. Reported in the Confusion Matrix values\n",
    "    CM_values = testModel(test_dataset, modelPath, threshold) #tp, fn, fp, tn\n",
    "    \n",
    "    TPR = CM_values[0] / (CM_values[0] + CM_values[1])\n",
    "    FPR = CM_values[2] / (CM_values[2] + CM_values[3])\n",
    "    TPR_list.append(TPR)\n",
    "    FPR_list.append(FPR)\n",
    "    #print(TPR_list)\n",
    "    #print(FPR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n"
     ]
    }
   ],
   "source": [
    "difference_array = np.array(TPR_list) - (1-np.array(FPR_list))\n",
    "best_thresh = thresholds[abs(difference_array).argmin()] #Thresholds is already defined. \n",
    "print(best_thresh)\n",
    "#This is saving the test results into a pkl file\n",
    "with open('ROC_AUC_results.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([TPR_list,FPR_list, best_thresh], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the best threshold is not a variable, we're probably missing info. \n",
    "#Load this into memory. \n",
    "if 'best_thresh' not in locals():\n",
    "    with open('ROC_AUC_results.pkl','rb') as f:  \n",
    "        TPR_list, FPR_list, best_thresh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5b4577c5b0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the ROC curve and determine the optimal cut-off threshold point\n",
    "#evaluateModel.ROC_AUC(FPR_list, TPR_list)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(FPR_list, TPR_list)\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('1 - Specificity (FPR)')\n",
    "ax.set_ylabel('Sensitivity (TPR)')\n",
    "ax.set_title('Receiver-Operating Characteristic')\n",
    "ax.legend('T',loc = \"lower right\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is made to analyze how well the model performed overall. \n",
    "del model\n",
    "model = smp.Unet(encoder_name = ENCODER, in_channels=1, classes = 1, aux_params = aux_params)\n",
    "modelPath = \"/home/mccrinbc/Registered_Brains_FA/models_saved/TBI_model-End-2020-09-30-19.pt\"\n",
    "model.load_state_dict(torch.load(modelPath))\n",
    "model.eval() #put into evaluation mode\n",
    "model.to(torch.device('cpu'))\n",
    "    \n",
    "#These need to be the results correspodning to the model you're loading. \n",
    "with open('results_TBI_model-End-2020-09-30-19.pkl','rb') as f:  \n",
    "    brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, test_dataset = pickle.load(f)\n",
    "\n",
    "loader = DataLoader(test_dataset, batch_size = 1, shuffle = False, num_workers = num_workers)\n",
    "\n",
    "#data is called from the loader as loader.dataset[image-label_number][0 OR 1] (shape: [1,256,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomAffine(degrees=(0, 0), scale=[1, 1])\n",
      "    RandomHorizontalFlip(p=0)\n",
      "    Pad(padding=37, fill=0, padding_mode=constant)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_result(loader, index, model, threshold):\n",
    "    #The model expects there to be a batch of images. Need to specify a single image in batch. \n",
    "    \n",
    "    brain = loader.dataset[index][0].unsqueeze(0) #index brain/label, get brain only, compress to only 2D\n",
    "    label = loader.dataset[index][1][0]\n",
    "    \n",
    "    predictions, _ = model(brain)\n",
    "    predictions = torch.sigmoid(predictions) \n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,15))\n",
    "    #fig.suptitle('Brain, True Label, Prediction')\n",
    "    ax1.imshow(brain[0][0].detach().numpy())\n",
    "    ax2.imshow(label.detach().numpy())\n",
    "    ax3.imshow(predictions[0][0].detach().numpy() > best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_image_result(loader, 30, model, best_thresh) #Takes a second to run if on CPU. \n",
    "\n",
    "#non-deterministic behaviour might be occuring because of the transform function explicity built into the\n",
    "#dataset call above. Potentially retrain and remove transform from testing dataset. \n",
    "\n",
    "#Each call of loader.dataset could be passing in the FIRST image you expect through a set of RANDOM transformations\n",
    "#This would make sense as to why we're getting randomness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5ca4f1bac0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " plt.imshow(loader.dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-aeeaaa1641bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbrains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Pick batches you'd like to see. Must be <= batch_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Load the brains, Determine the Predictions, Threshold the prediction with optimal value, plot\n",
    "\n",
    "image_loc = 1\n",
    "brains, labels = loader[2] #Pick batches you'd like to see. Must be <= batch_total\n",
    "\n",
    "predictions, _ = model(brains)\n",
    "predictions = torch.sigmoid(predictions)\n",
    "\n",
    "predictions = predictions > best_thresh #not sure if this is going to work with a batch of images. \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(predictions[image_loc,0,:,:])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plt(labels[image_loc,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
