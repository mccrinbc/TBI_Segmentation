{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm #loading bar  \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as ParentDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "import segmentation_models_pytorch as smp #model we're using for now. \n",
    "import evaluateModel\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Errors associated to potantial randomness / non-deterministic behaviour is a VERY common issue in PT. \n",
    "#Look at the following github discussion for more information: \n",
    "#https://github.com/pytorch/pytorch/issues/7068\n",
    "#      sbelharbi commented on Apr 19, 2019\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is related to the init function of the worker(s) to be fed to the DataLoader\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBI_dataset(ParentDataset): #Obtain the attributes of ParentDataset from torch.utils.data\n",
    "#Finds Image and Label locations, creates random list of indicies for training / val / testing sets to be called\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        labels_dir,\n",
    "        train_size = 0.75, #fraction of total number of samples to be used in training set\n",
    "        subset=\"train\",\n",
    "        transform = None, #base transformation is into Tensor. \n",
    "        random_sampling=True,\n",
    "        seed=seed, #We'll get the same thing everytime if we keep using the same seed. \n",
    "    ):\n",
    "        #filter and sort the list\n",
    "        self.ImageIds = sorted(list(filter(('.DS_Store').__ne__,os.listdir(images_dir)))) \n",
    "        self.LabelIds = sorted(list(filter(('.DS_Store').__ne__,os.listdir(labels_dir))))\n",
    "        \n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ImageIds] #full_paths to slices\n",
    "        self.labels_fps = [os.path.join(labels_dir, image_id) for image_id in self.LabelIds] #full_paths to labels\n",
    "        \n",
    "        if random_sampling == True:\n",
    "            samples = list(range(0,len(self.images_fps))) #create a list of numbers\n",
    "            random.seed(seed) #set the seed\n",
    "            \n",
    "            #random sample train_size amount and then do a train/validation split \n",
    "            indicies = random.sample(samples,round(train_size*len(samples)))\n",
    "            self.val_indicies = indicies[0:round(len(indicies)*0.15)]\n",
    "            self.train_indicies = indicies[round(len(indicies)*0.15) : len(indicies)]\n",
    "            \n",
    "            test_indicies = samples\n",
    "            for j in sorted(indicies, reverse = True): #remove the train/val indicies from test set\n",
    "                del test_indicies[j]\n",
    "            \n",
    "            #suffles without replacement. \n",
    "            self.test_indicies = random.sample(test_indicies, len(test_indicies)) \n",
    "\n",
    "        #We define a mapping to use when calling the Dataset loader based on the parameter \"subset\"\n",
    "        if subset == \"train\":\n",
    "            self.mapping = self.train_indicies\n",
    "        elif subset == \"val\":\n",
    "            self.mapping = self.val_indicies\n",
    "        elif subset == \"test\":\n",
    "            self.mapping = self.test_indicies\n",
    "        else:\n",
    "            print(\"subset parameter requires train, val, or test exactly.\")\n",
    "            \n",
    "        self.transform = transform #trasform given by transform_function\n",
    "            \n",
    "    def __getitem__(self, ii): #ii is the index\n",
    "        \n",
    "        #Current implementations of transforms only use PIL images.\n",
    "        #Apparently we can use np.array(Image.open(...)) to remove the error that happens each epoch\n",
    "        image = Image.open(self.images_fps[self.mapping[ii]]) #open as PIL image. \n",
    "        label = Image.open(self.labels_fps[self.mapping[ii]])\n",
    "         \n",
    "        image = self.transform(image)\n",
    "        label = self.transform(label)\n",
    "             \n",
    "        return image, label #, self.images_fps[self.mapping[ii]],self.labels_fps[self.mapping[ii]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "    \n",
    "    \n",
    "def datasets(images_dir, labels_dir, train_size, aug_angle, aug_scale, flip_prob):\n",
    "    train = TBI_dataset(\n",
    "        images_dir = images_dir,\n",
    "        labels_dir = labels_dir,\n",
    "        train_size = 0.75,\n",
    "        subset = \"train\",\n",
    "        transform = transform_function(degrees=aug_angle, scale=aug_scale, flip_prob=flip_prob),\n",
    "    )\n",
    "    valid = TBI_dataset(\n",
    "        images_dir = images_dir,\n",
    "        labels_dir = labels_dir,\n",
    "        train_size = 0.75,\n",
    "        subset = \"val\",\n",
    "        transform = transform_function(degrees=aug_angle, scale=aug_scale, flip_prob=flip_prob),\n",
    "    )\n",
    "    \n",
    "    test = TBI_dataset(\n",
    "        images_dir = images_dir,\n",
    "        labels_dir = labels_dir,\n",
    "        train_size = 0.75,\n",
    "        subset=\"test\",\n",
    "        transform = transform_function(degrees=0, scale = [1,1], flip_prob = 0), #make sure nothing changes. \n",
    "    )\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def transform_function(degrees,scale,flip_prob):\n",
    "    transform_list = []\n",
    "    \n",
    "    transform_list.append(transforms.RandomAffine(degrees, scale = scale))\n",
    "    transform_list.append(transforms.RandomHorizontalFlip(p=flip_prob))\n",
    "    transform_list.append(transforms.Pad(37)) #all images should be 182x182 before padding. \n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    \n",
    "    return Compose(transform_list)\n",
    "\n",
    "def Weights(labels):\n",
    "    #expects an [batch_size,c,n,n] input \n",
    "    \n",
    "    weights = []\n",
    "    for batch_num in range(0,labels.shape[0]):\n",
    "        num_ones = torch.sum(labels[batch_num,0,:,:]);\n",
    "        resolution = labels.shape[2] * labels.shape[3]\n",
    "        num_zeros = resolution - num_ones \n",
    "        weights.append(num_zeros / (num_ones + 1))\n",
    "        \n",
    "    #this keeps the clas imbalance in check\n",
    "    return torch.Tensor(weights) #to ensure that we're getting a real number in the division  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.75\n",
    "batch_size = 12\n",
    "EPOCHS = 100\n",
    "lr = 0.0001\n",
    "aug_angle = 25\n",
    "aug_scale = [1,1.5]\n",
    "flip_prob = 0.5\n",
    "num_workers = 1\n",
    "images_dir = \"/home/mccrinbc/Registered_Brains_FA/normalized_slices\"\n",
    "labels_dir = \"/home/mccrinbc/Registered_Brains_FA/slice_labels\"\n",
    "\n",
    "#images_dir = \"/Users/brianmccrindle/Documents/Research/TBIFinder_Final/Registered_Brains_FA/test_slices\"\n",
    "#labels_dir = \"/Users/brianmccrindle/Documents/Research/TBIFinder_Final/Registered_Brains_FA/test_labels\"\n",
    "\n",
    "#smp specific variables\n",
    "ENCODER = 'resnet101'\n",
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.5,               # dropout ratio, default is None\n",
    "    #activation='softmax2d',    # activation function, default is None. This is the output activation. softmax2d specifies dim = 1 \n",
    "    classes=1,                 # define number of output labels\n",
    ")\n",
    "\n",
    "#classes = 2 for the softmax transformation. \n",
    "model = smp.Unet(encoder_name = ENCODER, in_channels=1, classes = 1, aux_params = aux_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = datasets(images_dir, labels_dir, train_size, aug_angle, aug_scale, flip_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomAffine(degrees=(0, 0), scale=[1, 1])\n",
       "    RandomHorizontalFlip(p=0)\n",
       "    Pad(padding=37, fill=0, padding_mode=constant)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(train_dataset, valid_dataset, lr):\n",
    "    \n",
    "    earlystop = False \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        dev =\"cuda:2\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "        \n",
    "    dev = torch.device(dev)\n",
    "    \n",
    "    #this might break, remove worker_init_fn = _init_fn(num_workers)) if so\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle = True, num_workers = num_workers, worker_init_fn = _init_fn(num_workers))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size, shuffle = True, num_workers = num_workers, worker_init_fn = _init_fn(num_workers))\n",
    "    \n",
    "    model.to(dev) #cast the model onto the device \n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr) #learning rate should change \n",
    "    \n",
    "    loss_function = torch.nn.BCELoss() #this takes in a weighted input and incorporates a sigmoid transformation\n",
    "    #loss_function = smp.utils.losses.DiceLoss()\n",
    "    #loss_function = DiceLoss()\n",
    "    #metrics = [smp.utils.metrics.IoU(threshold=0.5)]\n",
    "    \n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    epochLoss_train = []\n",
    "    epochLoss_valid = []\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        image_count = 0\n",
    "        for phase in [\"train\",\"val\"]:\n",
    "            \n",
    "            #This determines which portions of the model will have gradients turned off or on. \n",
    "            if phase == \"train\":\n",
    "                model.train() #put into training mode\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval() #evaluation mode.\n",
    "                loader = valid_loader\n",
    "                  \n",
    "            for ii, data in enumerate(loader): \n",
    "                \n",
    "                brains = data[0] #[batch_size,channels,height,width] \n",
    "                labels = data[1]\n",
    "                \n",
    "                image_count += len(brains)\n",
    "                print(epoch, phase, ii, image_count)\n",
    "                \n",
    "                brains,labels = brains.to(dev), labels.to(dev) #put the data onto the device\n",
    "                predictions, single_class = model(brains) #single class is not a useful output. \n",
    "                \n",
    "                predictions = torch.sigmoid(predictions) #using this so that the output is bounded [0,1]\n",
    "                single_class = torch.sigmoid(single_class)\n",
    "                \n",
    "                weights = Weights(labels) #generate the weights for each slice in the batch\n",
    "                loss_function.pos_weight = weights                    \n",
    "                \n",
    "                loss = loss_function(predictions, labels) #loss changes here. \n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    #employ this so we don't get multiples in the same list. \n",
    "                    if (loss_valid and ii == 0): #if loss_valid is NOT empty AND it's the first time we see this\n",
    "                        epochLoss_valid.append(loss_valid[-1]) #append the last value in the \n",
    "                        \n",
    "                    model.zero_grad() #recommended way to perform validation\n",
    "                    loss_train.append(loss.item())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    print(f\"Phase: {phase}. Epoch: {epoch}. Loss: {loss.item()}\") \n",
    "               \n",
    "                else:\n",
    "                    if (loss_train and ii == 0):#if loss_valid is NOT empty AND it's the first time we see this\n",
    "                        epochLoss_train.append(loss_train[-1]) #append the last value in the loss_train list.\n",
    "                        \n",
    "                    loss_valid.append(loss.item())\n",
    "                    print(f\"Phase: {phase}. Epoch: {epoch}. Loss: {loss.item()}\") \n",
    "                    \n",
    "                    #learning rate changes and early stopping\n",
    "                    if epoch > 0:\n",
    "                        if (epoch % 10) == 0: #if the epoch is divisable by 10\n",
    "                            meanVal = np.mean(loss_valid[epoch - 10 : epoch])\n",
    "                            if np.abs((meanVal - loss.item()) / meanVal) <= 0.05: #if the % difference is small\n",
    "                                for param_group in optimizer.param_groups:\n",
    "                                    print('Reducing the Learning Rate: ' )\n",
    "                                    lr = lr * 0.1 #reduce the learning rate by a factor of 10. \n",
    "                                    print('Reducing the Learning Rate: ', lr )\n",
    "                                    param_group['lr'] = lr\n",
    "                        \n",
    "                        if (epoch % 50) == 0:\n",
    "                            meanVal = np.mean(loss_valid[epoch - 50 : epoch])\n",
    "                            if np.abs((meanVal - loss.item()) / meanVal) <= 0.05:\n",
    "                                earlystop = True \n",
    "               \n",
    "                #Implementation of early stopping\n",
    "                if earlystop == True:\n",
    "                    date = datetime.now()\n",
    "                    torch.save(model.state_dict(), os.path.join(os.getcwd(), \"Registered_Brains_FA/models_saved\", \"TBI_model-epoch\" + str(epoch) + '-' + str(date.date()) + '-' + str(date.hour) + '-' + str(date.minute) +\"-EARLYSTOP.pt\")) #save the model \n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            #save the model at the end of this epoch.\n",
    "            #date = datetime.now()\n",
    "            #torch.save(model.state_dict(), os.path.join(os.getcwd(), \"Registered_Brains_FA/models_saved\", \"TBI_model-epoch\" + str(epoch) + '-' + str(date.date()) + '-' + str(date.hour) + '-' + str(date.minute) + \".pt\"))\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    #Need to add the last element from loss_valid to epochLoss_valid to equal the number of epochs. \n",
    "    epochLoss_valid.append(loss_valid[-1])\n",
    "    return brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function expects singular images. \n",
    "#Also biased from high class imbalance. function currently not in use. \n",
    "def IoU(prediction, label):\n",
    "    #Prediction IoU\n",
    "    intersection = int(torch.sum(torch.mul(prediction,label)))\n",
    "    union = int(torch.sum(prediction) + torch.sum(label)) - intersection\n",
    "    IOU_predicted = intersection / (union + 0.0001) #for stability\n",
    "    \n",
    "    #Background IoU\n",
    "    all_zeros = (prediction + label) > 0 #before the inversion\n",
    "    intersection = int(torch.sum(~all_zeros))\n",
    "    union = int(torch.sum(~ (prediction > 0)) + torch.sum(~ (label > 0)) - intersection)\n",
    "    IOU_background = intersection / (union + 0.0001)\n",
    "    \n",
    "    mean_IOU = (IOU_background + IOU_predicted)/2\n",
    "    return mean_IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(test_dataset, modelPath, threshold): #model = the model class = smp.UNet()\n",
    "\n",
    "    total_images = 0\n",
    "    mean_IoUs = []\n",
    "    CM_values = [0,0,0,0] #tp, fp, fn, tn\n",
    "    model.load_state_dict(torch.load(modelPath))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        dev =\"cuda:2\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "        \n",
    "    dev = torch.device(dev)\n",
    "    model.to(dev) \n",
    "    model.eval() #evaluation mode to turn off the gradients / training. \n",
    "    \n",
    "    #turn shuffle off \n",
    "    loader = DataLoader(test_dataset, batch_size = 8, shuffle = False, num_workers = num_workers)\n",
    "    for ii, data in tqdm(enumerate(loader)):\n",
    "        \n",
    "        brains = data[0]\n",
    "        labels = data[1]\n",
    "        \n",
    "        #move the data to the GPU \n",
    "        brains = brains.to(dev)\n",
    "        labels = labels.to(dev)\n",
    "        \n",
    "        total_images += brains.shape[0] #this would be the same if we used labels or predictions. \n",
    "        #print(total_images)\n",
    "        \n",
    "        predictions, _ = model(brains)\n",
    "        predictions = torch.sigmoid(predictions) \n",
    "        \n",
    "        predictions_numpy = predictions.cpu().detach().numpy()\n",
    "        labels_numpy = labels.cpu().detach().numpy()\n",
    "        for j in range(predictions.shape[0]):\n",
    "            #labels = [False, True] are needed to make sure we don't have errors with the shape of CM\n",
    "            #mean_IoUs.append(IoU(predictions[j,0,:,:].cpu() > threshold, labels[j,0,:,:].cpu() > threshold)) #determine the mean IoU\n",
    "            CM = sklearn.metrics.confusion_matrix(labels_numpy[j,0,:,:].ravel(), predictions_numpy[j,0,:,:].ravel() > threshold, labels = [True,False])\n",
    "            try: \n",
    "                CM_values[0] = CM_values[0] + CM[0][0]\n",
    "                CM_values[1] = CM_values[1] + CM[0][1]\n",
    "                CM_values[2] = CM_values[2] + CM[1][0]\n",
    "                CM_values[3] = CM_values[3] + CM[1][1]\n",
    "            except:\n",
    "                print(\"Error in Appending\")\n",
    "                return CM, CM_values\n",
    "            \n",
    "    del loader #delete loader, might be wrong to do this\n",
    "    return np.divide(CM_values , (total_images*(256*256)))#, np.divide(np.sum(mean_IoUs), len(mean_IoUs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train 0 12\n",
      "Phase: train. Epoch: 0. Loss: 0.7613860964775085\n",
      "0 train 1 24\n",
      "Phase: train. Epoch: 0. Loss: 0.7304354906082153\n",
      "0 train 2 36\n",
      "Phase: train. Epoch: 0. Loss: 0.7147244215011597\n",
      "0 train 3 48\n",
      "Phase: train. Epoch: 0. Loss: 0.6645830273628235\n",
      "0 train 4 60\n",
      "Phase: train. Epoch: 0. Loss: 0.6397489309310913\n",
      "0 train 5 72\n",
      "Phase: train. Epoch: 0. Loss: 0.6271371841430664\n",
      "0 train 6 84\n",
      "Phase: train. Epoch: 0. Loss: 0.5994313955307007\n",
      "0 train 7 96\n",
      "Phase: train. Epoch: 0. Loss: 0.5717529058456421\n",
      "0 train 8 108\n",
      "Phase: train. Epoch: 0. Loss: 0.5586785674095154\n",
      "0 train 9 120\n",
      "Phase: train. Epoch: 0. Loss: 0.5343924164772034\n",
      "0 train 10 132\n",
      "Phase: train. Epoch: 0. Loss: 0.528215765953064\n",
      "0 train 11 144\n",
      "Phase: train. Epoch: 0. Loss: 0.5062786340713501\n",
      "0 train 12 156\n",
      "Phase: train. Epoch: 0. Loss: 0.49849364161491394\n",
      "0 train 13 168\n",
      "Phase: train. Epoch: 0. Loss: 0.4870050847530365\n",
      "0 train 14 180\n",
      "Phase: train. Epoch: 0. Loss: 0.48245733976364136\n",
      "0 train 15 192\n",
      "Phase: train. Epoch: 0. Loss: 0.45955419540405273\n",
      "0 train 16 204\n",
      "Phase: train. Epoch: 0. Loss: 0.4563846290111542\n",
      "0 train 17 216\n",
      "Phase: train. Epoch: 0. Loss: 0.442505419254303\n",
      "0 train 18 228\n",
      "Phase: train. Epoch: 0. Loss: 0.43846750259399414\n",
      "0 train 19 240\n",
      "Phase: train. Epoch: 0. Loss: 0.4157795310020447\n",
      "0 train 20 252\n",
      "Phase: train. Epoch: 0. Loss: 0.42817553877830505\n",
      "0 train 21 264\n",
      "Phase: train. Epoch: 0. Loss: 0.4106041193008423\n",
      "0 train 22 276\n",
      "Phase: train. Epoch: 0. Loss: 0.4087498188018799\n",
      "0 train 23 288\n",
      "Phase: train. Epoch: 0. Loss: 0.40585798025131226\n",
      "0 train 24 300\n",
      "Phase: train. Epoch: 0. Loss: 0.38858693838119507\n",
      "0 train 25 312\n",
      "Phase: train. Epoch: 0. Loss: 0.39732879400253296\n",
      "0 train 26 324\n",
      "Phase: train. Epoch: 0. Loss: 0.3898451030254364\n",
      "0 train 27 336\n",
      "Phase: train. Epoch: 0. Loss: 0.3749629855155945\n",
      "0 train 28 348\n",
      "Phase: train. Epoch: 0. Loss: 0.36254942417144775\n",
      "0 train 29 360\n",
      "Phase: train. Epoch: 0. Loss: 0.36569249629974365\n",
      "0 train 30 372\n",
      "Phase: train. Epoch: 0. Loss: 0.3644298017024994\n",
      "0 train 31 384\n",
      "Phase: train. Epoch: 0. Loss: 0.3460261821746826\n",
      "0 train 32 396\n",
      "Phase: train. Epoch: 0. Loss: 0.36230120062828064\n",
      "0 train 33 408\n",
      "Phase: train. Epoch: 0. Loss: 0.3549842834472656\n",
      "0 train 34 420\n",
      "Phase: train. Epoch: 0. Loss: 0.342648446559906\n",
      "0 train 35 432\n",
      "Phase: train. Epoch: 0. Loss: 0.33478230237960815\n",
      "0 train 36 444\n",
      "Phase: train. Epoch: 0. Loss: 0.34428614377975464\n",
      "0 train 37 456\n",
      "Phase: train. Epoch: 0. Loss: 0.33745846152305603\n",
      "0 train 38 468\n",
      "Phase: train. Epoch: 0. Loss: 0.3392239511013031\n",
      "0 train 39 480\n",
      "Phase: train. Epoch: 0. Loss: 0.3250429630279541\n",
      "0 train 40 492\n",
      "Phase: train. Epoch: 0. Loss: 0.3326804041862488\n",
      "0 train 41 504\n",
      "Phase: train. Epoch: 0. Loss: 0.31117376685142517\n",
      "0 train 42 516\n",
      "Phase: train. Epoch: 0. Loss: 0.31876707077026367\n",
      "0 train 43 528\n",
      "Phase: train. Epoch: 0. Loss: 0.31368303298950195\n",
      "0 train 44 540\n",
      "Phase: train. Epoch: 0. Loss: 0.32236382365226746\n",
      "0 train 45 552\n",
      "Phase: train. Epoch: 0. Loss: 0.30382856726646423\n",
      "0 train 46 564\n",
      "Phase: train. Epoch: 0. Loss: 0.3045390248298645\n",
      "0 train 47 576\n",
      "Phase: train. Epoch: 0. Loss: 0.3023110628128052\n",
      "0 train 48 588\n",
      "Phase: train. Epoch: 0. Loss: 0.2999795377254486\n",
      "0 train 49 600\n",
      "Phase: train. Epoch: 0. Loss: 0.30106502771377563\n",
      "0 train 50 612\n",
      "Phase: train. Epoch: 0. Loss: 0.3015953302383423\n",
      "0 train 51 624\n",
      "Phase: train. Epoch: 0. Loss: 0.29858532547950745\n",
      "0 train 52 636\n",
      "Phase: train. Epoch: 0. Loss: 0.291780561208725\n",
      "0 train 53 648\n",
      "Phase: train. Epoch: 0. Loss: 0.28579723834991455\n",
      "0 train 54 660\n",
      "Phase: train. Epoch: 0. Loss: 0.2895752787590027\n",
      "0 train 55 672\n",
      "Phase: train. Epoch: 0. Loss: 0.2848235070705414\n",
      "0 train 56 684\n",
      "Phase: train. Epoch: 0. Loss: 0.2720701992511749\n",
      "0 train 57 696\n",
      "Phase: train. Epoch: 0. Loss: 0.2767842411994934\n",
      "0 train 58 708\n",
      "Phase: train. Epoch: 0. Loss: 0.2688139081001282\n",
      "0 train 59 720\n",
      "Phase: train. Epoch: 0. Loss: 0.2807093858718872\n",
      "0 train 60 732\n",
      "Phase: train. Epoch: 0. Loss: 0.2866818308830261\n",
      "0 train 61 744\n",
      "Phase: train. Epoch: 0. Loss: 0.27682024240493774\n",
      "0 train 62 751\n",
      "Phase: train. Epoch: 0. Loss: 0.2774403989315033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val 0 763\n",
      "Phase: val. Epoch: 0. Loss: 0.268738329410553\n",
      "0 val 1 775\n",
      "Phase: val. Epoch: 0. Loss: 0.287570059299469\n",
      "0 val 2 787\n",
      "Phase: val. Epoch: 0. Loss: 0.28846362233161926\n",
      "0 val 3 799\n",
      "Phase: val. Epoch: 0. Loss: 0.2623896300792694\n",
      "0 val 4 811\n",
      "Phase: val. Epoch: 0. Loss: 0.2783409357070923\n",
      "0 val 5 823\n",
      "Phase: val. Epoch: 0. Loss: 0.263107031583786\n",
      "0 val 6 835\n",
      "Phase: val. Epoch: 0. Loss: 0.2604798674583435\n",
      "0 val 7 847\n",
      "Phase: val. Epoch: 0. Loss: 0.25098615884780884\n",
      "0 val 8 859\n",
      "Phase: val. Epoch: 0. Loss: 0.2847547233104706\n",
      "0 val 9 871\n",
      "Phase: val. Epoch: 0. Loss: 0.26710236072540283\n",
      "0 val 10 883\n",
      "Phase: val. Epoch: 0. Loss: 0.2806107997894287\n",
      "0 val 11 884\n",
      "Phase: val. Epoch: 0. Loss: 0.2726135849952698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train 0 12\n",
      "Phase: train. Epoch: 1. Loss: 0.27667245268821716\n",
      "1 train 1 24\n",
      "Phase: train. Epoch: 1. Loss: 0.27143019437789917\n",
      "1 train 2 36\n",
      "Phase: train. Epoch: 1. Loss: 0.2677813768386841\n",
      "1 train 3 48\n",
      "Phase: train. Epoch: 1. Loss: 0.26369231939315796\n",
      "1 train 4 60\n",
      "Phase: train. Epoch: 1. Loss: 0.2612699866294861\n",
      "1 train 5 72\n",
      "Phase: train. Epoch: 1. Loss: 0.2837631404399872\n",
      "1 train 6 84\n",
      "Phase: train. Epoch: 1. Loss: 0.28324079513549805\n",
      "1 train 7 96\n",
      "Phase: train. Epoch: 1. Loss: 0.32632529735565186\n",
      "1 train 8 108\n",
      "Phase: train. Epoch: 1. Loss: 0.25138184428215027\n",
      "1 train 9 120\n",
      "Phase: train. Epoch: 1. Loss: 0.2675793468952179\n",
      "1 train 10 132\n",
      "Phase: train. Epoch: 1. Loss: 0.2559305429458618\n",
      "1 train 11 144\n",
      "Phase: train. Epoch: 1. Loss: 0.26624101400375366\n",
      "1 train 12 156\n",
      "Phase: train. Epoch: 1. Loss: 0.25381094217300415\n",
      "1 train 13 168\n",
      "Phase: train. Epoch: 1. Loss: 0.2597176730632782\n",
      "1 train 14 180\n",
      "Phase: train. Epoch: 1. Loss: 0.27217695116996765\n",
      "1 train 15 192\n",
      "Phase: train. Epoch: 1. Loss: 0.26754850149154663\n",
      "1 train 16 204\n",
      "Phase: train. Epoch: 1. Loss: 0.24906966090202332\n",
      "1 train 17 216\n",
      "Phase: train. Epoch: 1. Loss: 0.2845296263694763\n",
      "1 train 18 228\n",
      "Phase: train. Epoch: 1. Loss: 0.2472514808177948\n",
      "1 train 19 240\n",
      "Phase: train. Epoch: 1. Loss: 0.2561064660549164\n",
      "1 train 20 252\n",
      "Phase: train. Epoch: 1. Loss: 0.28163060545921326\n",
      "1 train 21 264\n",
      "Phase: train. Epoch: 1. Loss: 0.25587162375450134\n",
      "1 train 22 276\n",
      "Phase: train. Epoch: 1. Loss: 0.2653234302997589\n",
      "1 train 23 288\n",
      "Phase: train. Epoch: 1. Loss: 0.2633391320705414\n",
      "1 train 24 300\n",
      "Phase: train. Epoch: 1. Loss: 0.24626001715660095\n",
      "1 train 25 312\n",
      "Phase: train. Epoch: 1. Loss: 0.25319284200668335\n",
      "1 train 26 324\n",
      "Phase: train. Epoch: 1. Loss: 0.262518048286438\n",
      "1 train 27 336\n",
      "Phase: train. Epoch: 1. Loss: 0.2475535124540329\n",
      "1 train 28 348\n",
      "Phase: train. Epoch: 1. Loss: 0.242588609457016\n",
      "1 train 29 360\n",
      "Phase: train. Epoch: 1. Loss: 0.2481180876493454\n",
      "1 train 30 372\n",
      "Phase: train. Epoch: 1. Loss: 0.23769693076610565\n",
      "1 train 31 384\n",
      "Phase: train. Epoch: 1. Loss: 0.2531631886959076\n",
      "1 train 32 396\n",
      "Phase: train. Epoch: 1. Loss: 0.2291361689567566\n",
      "1 train 33 408\n",
      "Phase: train. Epoch: 1. Loss: 0.23678341507911682\n",
      "1 train 34 420\n",
      "Phase: train. Epoch: 1. Loss: 0.24542328715324402\n",
      "1 train 35 432\n",
      "Phase: train. Epoch: 1. Loss: 0.28063708543777466\n",
      "1 train 36 444\n",
      "Phase: train. Epoch: 1. Loss: 0.24665352702140808\n",
      "1 train 37 456\n",
      "Phase: train. Epoch: 1. Loss: 0.245181605219841\n",
      "1 train 38 468\n",
      "Phase: train. Epoch: 1. Loss: 0.2510320842266083\n",
      "1 train 39 480\n",
      "Phase: train. Epoch: 1. Loss: 0.230581134557724\n",
      "1 train 40 492\n",
      "Phase: train. Epoch: 1. Loss: 0.24863362312316895\n",
      "1 train 41 504\n",
      "Phase: train. Epoch: 1. Loss: 0.21996085345745087\n",
      "1 train 42 516\n",
      "Phase: train. Epoch: 1. Loss: 0.24762962758541107\n",
      "1 train 43 528\n",
      "Phase: train. Epoch: 1. Loss: 0.2184966504573822\n",
      "1 train 44 540\n",
      "Phase: train. Epoch: 1. Loss: 0.24129998683929443\n",
      "1 train 45 552\n",
      "Phase: train. Epoch: 1. Loss: 0.23036226630210876\n",
      "1 train 46 564\n",
      "Phase: train. Epoch: 1. Loss: 0.23026713728904724\n",
      "1 train 47 576\n",
      "Phase: train. Epoch: 1. Loss: 0.2317347526550293\n",
      "1 train 48 588\n",
      "Phase: train. Epoch: 1. Loss: 0.21362237632274628\n",
      "1 train 49 600\n",
      "Phase: train. Epoch: 1. Loss: 0.23265600204467773\n",
      "1 train 50 612\n",
      "Phase: train. Epoch: 1. Loss: 0.23175911605358124\n",
      "1 train 51 624\n",
      "Phase: train. Epoch: 1. Loss: 0.22435316443443298\n",
      "1 train 52 636\n",
      "Phase: train. Epoch: 1. Loss: 0.21901163458824158\n",
      "1 train 53 648\n",
      "Phase: train. Epoch: 1. Loss: 0.23108458518981934\n",
      "1 train 54 660\n",
      "Phase: train. Epoch: 1. Loss: 0.23129194974899292\n",
      "1 train 55 672\n",
      "Phase: train. Epoch: 1. Loss: 0.21434800326824188\n",
      "1 train 56 684\n",
      "Phase: train. Epoch: 1. Loss: 0.21566657721996307\n",
      "1 train 57 696\n",
      "Phase: train. Epoch: 1. Loss: 0.22587665915489197\n",
      "1 train 58 708\n",
      "Phase: train. Epoch: 1. Loss: 0.22954851388931274\n",
      "1 train 59 720\n",
      "Phase: train. Epoch: 1. Loss: 0.21309486031532288\n",
      "1 train 60 732\n",
      "Phase: train. Epoch: 1. Loss: 0.21497851610183716\n",
      "1 train 61 744\n",
      "Phase: train. Epoch: 1. Loss: 0.22511504590511322\n",
      "1 train 62 751\n",
      "Phase: train. Epoch: 1. Loss: 0.21684031188488007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 val 0 763\n",
      "Phase: val. Epoch: 1. Loss: 0.18984286487102509\n",
      "1 val 1 775\n",
      "Phase: val. Epoch: 1. Loss: 0.2602764368057251\n",
      "1 val 2 787\n",
      "Phase: val. Epoch: 1. Loss: 0.225211501121521\n",
      "1 val 3 799\n",
      "Phase: val. Epoch: 1. Loss: 0.2364155352115631\n",
      "1 val 4 811\n",
      "Phase: val. Epoch: 1. Loss: 0.22378885746002197\n",
      "1 val 5 823\n",
      "Phase: val. Epoch: 1. Loss: 0.20510295033454895\n",
      "1 val 6 835\n",
      "Phase: val. Epoch: 1. Loss: 0.22766022384166718\n",
      "1 val 7 847\n",
      "Phase: val. Epoch: 1. Loss: 0.20898878574371338\n",
      "1 val 8 859\n",
      "Phase: val. Epoch: 1. Loss: 0.21932844817638397\n",
      "1 val 9 871\n",
      "Phase: val. Epoch: 1. Loss: 0.2042456567287445\n",
      "1 val 10 883\n",
      "Phase: val. Epoch: 1. Loss: 0.20065751671791077\n",
      "1 val 11 884\n",
      "Phase: val. Epoch: 1. Loss: 0.24380502104759216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 train 0 12\n",
      "Phase: train. Epoch: 2. Loss: 0.21978452801704407\n",
      "2 train 1 24\n",
      "Phase: train. Epoch: 2. Loss: 0.21747474372386932\n",
      "2 train 2 36\n",
      "Phase: train. Epoch: 2. Loss: 0.21830014884471893\n",
      "2 train 3 48\n",
      "Phase: train. Epoch: 2. Loss: 0.2226334810256958\n",
      "2 train 4 60\n",
      "Phase: train. Epoch: 2. Loss: 0.23596100509166718\n",
      "2 train 5 72\n",
      "Phase: train. Epoch: 2. Loss: 0.20299017429351807\n",
      "2 train 6 84\n",
      "Phase: train. Epoch: 2. Loss: 0.2120794951915741\n",
      "2 train 7 96\n",
      "Phase: train. Epoch: 2. Loss: 0.22011002898216248\n",
      "2 train 8 108\n",
      "Phase: train. Epoch: 2. Loss: 0.20977318286895752\n",
      "2 train 9 120\n",
      "Phase: train. Epoch: 2. Loss: 0.20583777129650116\n",
      "2 train 10 132\n",
      "Phase: train. Epoch: 2. Loss: 0.21863369643688202\n",
      "2 train 11 144\n",
      "Phase: train. Epoch: 2. Loss: 0.22245121002197266\n",
      "2 train 12 156\n",
      "Phase: train. Epoch: 2. Loss: 0.2068115919828415\n",
      "2 train 13 168\n",
      "Phase: train. Epoch: 2. Loss: 0.22627536952495575\n",
      "2 train 14 180\n",
      "Phase: train. Epoch: 2. Loss: 0.20688080787658691\n",
      "2 train 15 192\n",
      "Phase: train. Epoch: 2. Loss: 0.20844954252243042\n",
      "2 train 16 204\n",
      "Phase: train. Epoch: 2. Loss: 0.22258691489696503\n",
      "2 train 17 216\n",
      "Phase: train. Epoch: 2. Loss: 0.21223604679107666\n",
      "2 train 18 228\n",
      "Phase: train. Epoch: 2. Loss: 0.20447233319282532\n",
      "2 train 19 240\n",
      "Phase: train. Epoch: 2. Loss: 0.19806358218193054\n",
      "2 train 20 252\n",
      "Phase: train. Epoch: 2. Loss: 0.20753979682922363\n",
      "2 train 21 264\n",
      "Phase: train. Epoch: 2. Loss: 0.19324460625648499\n",
      "2 train 22 276\n",
      "Phase: train. Epoch: 2. Loss: 0.19842581450939178\n",
      "2 train 23 288\n",
      "Phase: train. Epoch: 2. Loss: 0.20425701141357422\n",
      "2 train 24 300\n",
      "Phase: train. Epoch: 2. Loss: 0.20795822143554688\n",
      "2 train 25 312\n",
      "Phase: train. Epoch: 2. Loss: 0.21628348529338837\n",
      "2 train 26 324\n",
      "Phase: train. Epoch: 2. Loss: 0.2109159678220749\n",
      "2 train 27 336\n",
      "Phase: train. Epoch: 2. Loss: 0.22550338506698608\n",
      "2 train 28 348\n",
      "Phase: train. Epoch: 2. Loss: 0.2192237824201584\n",
      "2 train 29 360\n",
      "Phase: train. Epoch: 2. Loss: 0.19397681951522827\n",
      "2 train 30 372\n",
      "Phase: train. Epoch: 2. Loss: 0.21116766333580017\n",
      "2 train 31 384\n",
      "Phase: train. Epoch: 2. Loss: 0.1965126395225525\n",
      "2 train 32 396\n",
      "Phase: train. Epoch: 2. Loss: 0.1937757432460785\n",
      "2 train 33 408\n",
      "Phase: train. Epoch: 2. Loss: 0.20845793187618256\n",
      "2 train 34 420\n",
      "Phase: train. Epoch: 2. Loss: 0.19261494278907776\n",
      "2 train 35 432\n",
      "Phase: train. Epoch: 2. Loss: 0.19998377561569214\n",
      "2 train 36 444\n",
      "Phase: train. Epoch: 2. Loss: 0.1918623447418213\n",
      "2 train 37 456\n",
      "Phase: train. Epoch: 2. Loss: 0.2175460308790207\n",
      "2 train 38 468\n",
      "Phase: train. Epoch: 2. Loss: 0.18213459849357605\n",
      "2 train 39 480\n",
      "Phase: train. Epoch: 2. Loss: 0.18997785449028015\n",
      "2 train 40 492\n",
      "Phase: train. Epoch: 2. Loss: 0.18849250674247742\n",
      "2 train 41 504\n",
      "Phase: train. Epoch: 2. Loss: 0.19996196031570435\n",
      "2 train 42 516\n",
      "Phase: train. Epoch: 2. Loss: 0.1991020143032074\n",
      "2 train 43 528\n",
      "Phase: train. Epoch: 2. Loss: 0.20181910693645477\n",
      "2 train 44 540\n",
      "Phase: train. Epoch: 2. Loss: 0.18839988112449646\n",
      "2 train 45 552\n",
      "Phase: train. Epoch: 2. Loss: 0.2100151628255844\n",
      "2 train 46 564\n",
      "Phase: train. Epoch: 2. Loss: 0.20133835077285767\n",
      "2 train 47 576\n",
      "Phase: train. Epoch: 2. Loss: 0.18100354075431824\n",
      "2 train 48 588\n",
      "Phase: train. Epoch: 2. Loss: 0.20304176211357117\n",
      "2 train 49 600\n",
      "Phase: train. Epoch: 2. Loss: 0.20303630828857422\n",
      "2 train 50 612\n",
      "Phase: train. Epoch: 2. Loss: 0.1982906311750412\n",
      "2 train 51 624\n",
      "Phase: train. Epoch: 2. Loss: 0.1982075572013855\n",
      "2 train 52 636\n",
      "Phase: train. Epoch: 2. Loss: 0.18590793013572693\n",
      "2 train 53 648\n",
      "Phase: train. Epoch: 2. Loss: 0.1841905564069748\n",
      "2 train 54 660\n",
      "Phase: train. Epoch: 2. Loss: 0.19679057598114014\n",
      "2 train 55 672\n",
      "Phase: train. Epoch: 2. Loss: 0.1959245800971985\n",
      "2 train 56 684\n",
      "Phase: train. Epoch: 2. Loss: 0.17700159549713135\n",
      "2 train 57 696\n",
      "Phase: train. Epoch: 2. Loss: 0.1873762309551239\n",
      "2 train 58 708\n",
      "Phase: train. Epoch: 2. Loss: 0.20093980431556702\n",
      "2 train 59 720\n",
      "Phase: train. Epoch: 2. Loss: 0.1805119812488556\n",
      "2 train 60 732\n",
      "Phase: train. Epoch: 2. Loss: 0.17643576860427856\n",
      "2 train 61 744\n",
      "Phase: train. Epoch: 2. Loss: 0.17150168120861053\n",
      "2 train 62 751\n",
      "Phase: train. Epoch: 2. Loss: 0.19350889325141907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 val 0 763\n",
      "Phase: val. Epoch: 2. Loss: 0.16733555495738983\n",
      "2 val 1 775\n",
      "Phase: val. Epoch: 2. Loss: 0.18983015418052673\n",
      "2 val 2 787\n",
      "Phase: val. Epoch: 2. Loss: 0.19426636397838593\n",
      "2 val 3 799\n",
      "Phase: val. Epoch: 2. Loss: 0.20217499136924744\n",
      "2 val 4 811\n",
      "Phase: val. Epoch: 2. Loss: 0.18309900164604187\n",
      "2 val 5 823\n",
      "Phase: val. Epoch: 2. Loss: 0.1735251396894455\n",
      "2 val 6 835\n",
      "Phase: val. Epoch: 2. Loss: 0.1885346621274948\n",
      "2 val 7 847\n",
      "Phase: val. Epoch: 2. Loss: 0.1897304356098175\n",
      "2 val 8 859\n",
      "Phase: val. Epoch: 2. Loss: 0.17125707864761353\n",
      "2 val 9 871\n",
      "Phase: val. Epoch: 2. Loss: 0.20167820155620575\n",
      "2 val 10 883\n",
      "Phase: val. Epoch: 2. Loss: 0.199980229139328\n",
      "2 val 11 884\n",
      "Phase: val. Epoch: 2. Loss: 0.13275359570980072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 train 0 12\n",
      "Phase: train. Epoch: 3. Loss: 0.17720934748649597\n",
      "3 train 1 24\n",
      "Phase: train. Epoch: 3. Loss: 0.1950557678937912\n",
      "3 train 2 36\n",
      "Phase: train. Epoch: 3. Loss: 0.16927841305732727\n",
      "3 train 3 48\n",
      "Phase: train. Epoch: 3. Loss: 0.18381044268608093\n",
      "3 train 4 60\n",
      "Phase: train. Epoch: 3. Loss: 0.17715303599834442\n",
      "3 train 5 72\n",
      "Phase: train. Epoch: 3. Loss: 0.18237367272377014\n",
      "3 train 6 84\n",
      "Phase: train. Epoch: 3. Loss: 0.20279228687286377\n",
      "3 train 7 96\n",
      "Phase: train. Epoch: 3. Loss: 0.17763632535934448\n",
      "3 train 8 108\n",
      "Phase: train. Epoch: 3. Loss: 0.18780168890953064\n",
      "3 train 9 120\n",
      "Phase: train. Epoch: 3. Loss: 0.16649335622787476\n",
      "3 train 10 132\n",
      "Phase: train. Epoch: 3. Loss: 0.1872834861278534\n",
      "3 train 11 144\n",
      "Phase: train. Epoch: 3. Loss: 0.1748175323009491\n",
      "3 train 12 156\n",
      "Phase: train. Epoch: 3. Loss: 0.18177998065948486\n",
      "3 train 13 168\n",
      "Phase: train. Epoch: 3. Loss: 0.1797114461660385\n",
      "3 train 14 180\n",
      "Phase: train. Epoch: 3. Loss: 0.15605199337005615\n",
      "3 train 15 192\n",
      "Phase: train. Epoch: 3. Loss: 0.17572033405303955\n",
      "3 train 16 204\n",
      "Phase: train. Epoch: 3. Loss: 0.1700788140296936\n",
      "3 train 17 216\n",
      "Phase: train. Epoch: 3. Loss: 0.17804694175720215\n",
      "3 train 18 228\n",
      "Phase: train. Epoch: 3. Loss: 0.20222097635269165\n",
      "3 train 19 240\n",
      "Phase: train. Epoch: 3. Loss: 0.17829787731170654\n",
      "3 train 20 252\n",
      "Phase: train. Epoch: 3. Loss: 0.19597916305065155\n",
      "3 train 21 264\n",
      "Phase: train. Epoch: 3. Loss: 0.19290804862976074\n",
      "3 train 22 276\n",
      "Phase: train. Epoch: 3. Loss: 0.16778576374053955\n",
      "3 train 23 288\n",
      "Phase: train. Epoch: 3. Loss: 0.18433308601379395\n",
      "3 train 24 300\n",
      "Phase: train. Epoch: 3. Loss: 0.1577252745628357\n",
      "3 train 25 312\n",
      "Phase: train. Epoch: 3. Loss: 0.17120151221752167\n",
      "3 train 26 324\n",
      "Phase: train. Epoch: 3. Loss: 0.17004919052124023\n",
      "3 train 27 336\n",
      "Phase: train. Epoch: 3. Loss: 0.17452745139598846\n",
      "3 train 28 348\n",
      "Phase: train. Epoch: 3. Loss: 0.1848519742488861\n",
      "3 train 29 360\n",
      "Phase: train. Epoch: 3. Loss: 0.18174134194850922\n",
      "3 train 30 372\n",
      "Phase: train. Epoch: 3. Loss: 0.16676145792007446\n",
      "3 train 31 384\n",
      "Phase: train. Epoch: 3. Loss: 0.19986450672149658\n",
      "3 train 32 396\n",
      "Phase: train. Epoch: 3. Loss: 0.1851029098033905\n",
      "3 train 33 408\n",
      "Phase: train. Epoch: 3. Loss: 0.16414916515350342\n",
      "3 train 34 420\n",
      "Phase: train. Epoch: 3. Loss: 0.15998825430870056\n",
      "3 train 35 432\n",
      "Phase: train. Epoch: 3. Loss: 0.15726207196712494\n",
      "3 train 36 444\n",
      "Phase: train. Epoch: 3. Loss: 0.15356013178825378\n",
      "3 train 37 456\n",
      "Phase: train. Epoch: 3. Loss: 0.15557454526424408\n",
      "3 train 38 468\n",
      "Phase: train. Epoch: 3. Loss: 0.17710570991039276\n",
      "3 train 39 480\n",
      "Phase: train. Epoch: 3. Loss: 0.18067553639411926\n",
      "3 train 40 492\n",
      "Phase: train. Epoch: 3. Loss: 0.16592279076576233\n",
      "3 train 41 504\n",
      "Phase: train. Epoch: 3. Loss: 0.14644864201545715\n",
      "3 train 42 516\n",
      "Phase: train. Epoch: 3. Loss: 0.18565937876701355\n",
      "3 train 43 528\n",
      "Phase: train. Epoch: 3. Loss: 0.17285661399364471\n",
      "3 train 44 540\n",
      "Phase: train. Epoch: 3. Loss: 0.18775629997253418\n",
      "3 train 45 552\n",
      "Phase: train. Epoch: 3. Loss: 0.17118094861507416\n",
      "3 train 46 564\n",
      "Phase: train. Epoch: 3. Loss: 0.17425048351287842\n",
      "3 train 47 576\n",
      "Phase: train. Epoch: 3. Loss: 0.16888432204723358\n",
      "3 train 48 588\n",
      "Phase: train. Epoch: 3. Loss: 0.17560511827468872\n",
      "3 train 49 600\n",
      "Phase: train. Epoch: 3. Loss: 0.16774730384349823\n",
      "3 train 50 612\n",
      "Phase: train. Epoch: 3. Loss: 0.18431447446346283\n",
      "3 train 51 624\n",
      "Phase: train. Epoch: 3. Loss: 0.1480836570262909\n",
      "3 train 52 636\n",
      "Phase: train. Epoch: 3. Loss: 0.18641528487205505\n",
      "3 train 53 648\n",
      "Phase: train. Epoch: 3. Loss: 0.2032407969236374\n",
      "3 train 54 660\n",
      "Phase: train. Epoch: 3. Loss: 0.14902442693710327\n",
      "3 train 55 672\n",
      "Phase: train. Epoch: 3. Loss: 0.1871984452009201\n",
      "3 train 56 684\n",
      "Phase: train. Epoch: 3. Loss: 0.16359925270080566\n",
      "3 train 57 696\n",
      "Phase: train. Epoch: 3. Loss: 0.1620367467403412\n",
      "3 train 58 708\n",
      "Phase: train. Epoch: 3. Loss: 0.14111372828483582\n",
      "3 train 59 720\n",
      "Phase: train. Epoch: 3. Loss: 0.17155447602272034\n",
      "3 train 60 732\n",
      "Phase: train. Epoch: 3. Loss: 0.165456622838974\n",
      "3 train 61 744\n",
      "Phase: train. Epoch: 3. Loss: 0.15140299499034882\n",
      "3 train 62 751\n",
      "Phase: train. Epoch: 3. Loss: 0.182536780834198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 val 0 763\n",
      "Phase: val. Epoch: 3. Loss: 0.15570154786109924\n",
      "3 val 1 775\n",
      "Phase: val. Epoch: 3. Loss: 0.15442326664924622\n",
      "3 val 2 787\n",
      "Phase: val. Epoch: 3. Loss: 0.15200409293174744\n",
      "3 val 3 799\n",
      "Phase: val. Epoch: 3. Loss: 0.16265028715133667\n",
      "3 val 4 811\n",
      "Phase: val. Epoch: 3. Loss: 0.1451190710067749\n",
      "3 val 5 823\n",
      "Phase: val. Epoch: 3. Loss: 0.1515306830406189\n",
      "3 val 6 835\n",
      "Phase: val. Epoch: 3. Loss: 0.1763329952955246\n",
      "3 val 7 847\n",
      "Phase: val. Epoch: 3. Loss: 0.14967788755893707\n",
      "3 val 8 859\n",
      "Phase: val. Epoch: 3. Loss: 0.1522655189037323\n",
      "3 val 9 871\n",
      "Phase: val. Epoch: 3. Loss: 0.16357356309890747\n",
      "3 val 10 883\n",
      "Phase: val. Epoch: 3. Loss: 0.15249168872833252\n",
      "3 val 11 884\n",
      "Phase: val. Epoch: 3. Loss: 0.21639370918273926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 train 0 12\n",
      "Phase: train. Epoch: 4. Loss: 0.14701873064041138\n",
      "4 train 1 24\n",
      "Phase: train. Epoch: 4. Loss: 0.15282005071640015\n",
      "4 train 2 36\n",
      "Phase: train. Epoch: 4. Loss: 0.22173741459846497\n",
      "4 train 3 48\n",
      "Phase: train. Epoch: 4. Loss: 0.1612384170293808\n",
      "4 train 4 60\n",
      "Phase: train. Epoch: 4. Loss: 0.144424706697464\n",
      "4 train 5 72\n",
      "Phase: train. Epoch: 4. Loss: 0.15861181914806366\n",
      "4 train 6 84\n",
      "Phase: train. Epoch: 4. Loss: 0.1445557326078415\n",
      "4 train 7 96\n",
      "Phase: train. Epoch: 4. Loss: 0.15608087182044983\n",
      "4 train 8 108\n",
      "Phase: train. Epoch: 4. Loss: 0.16677866876125336\n",
      "4 train 9 120\n",
      "Phase: train. Epoch: 4. Loss: 0.1385532021522522\n",
      "4 train 10 132\n",
      "Phase: train. Epoch: 4. Loss: 0.17078068852424622\n",
      "4 train 11 144\n",
      "Phase: train. Epoch: 4. Loss: 0.1764531135559082\n",
      "4 train 12 156\n",
      "Phase: train. Epoch: 4. Loss: 0.15780030190944672\n",
      "4 train 13 168\n",
      "Phase: train. Epoch: 4. Loss: 0.1629551351070404\n",
      "4 train 14 180\n",
      "Phase: train. Epoch: 4. Loss: 0.14956976473331451\n",
      "4 train 15 192\n",
      "Phase: train. Epoch: 4. Loss: 0.163810595870018\n",
      "4 train 16 204\n",
      "Phase: train. Epoch: 4. Loss: 0.15703585743904114\n",
      "4 train 17 216\n",
      "Phase: train. Epoch: 4. Loss: 0.1600750982761383\n",
      "4 train 18 228\n",
      "Phase: train. Epoch: 4. Loss: 0.1557091772556305\n",
      "4 train 19 240\n",
      "Phase: train. Epoch: 4. Loss: 0.14841866493225098\n",
      "4 train 20 252\n",
      "Phase: train. Epoch: 4. Loss: 0.14841896295547485\n",
      "4 train 21 264\n",
      "Phase: train. Epoch: 4. Loss: 0.14120157063007355\n",
      "4 train 22 276\n",
      "Phase: train. Epoch: 4. Loss: 0.17732766270637512\n",
      "4 train 23 288\n",
      "Phase: train. Epoch: 4. Loss: 0.14510072767734528\n",
      "4 train 24 300\n",
      "Phase: train. Epoch: 4. Loss: 0.15405111014842987\n",
      "4 train 25 312\n",
      "Phase: train. Epoch: 4. Loss: 0.14499783515930176\n",
      "4 train 26 324\n",
      "Phase: train. Epoch: 4. Loss: 0.15732169151306152\n",
      "4 train 27 336\n",
      "Phase: train. Epoch: 4. Loss: 0.1618151068687439\n",
      "4 train 28 348\n",
      "Phase: train. Epoch: 4. Loss: 0.15033838152885437\n",
      "4 train 29 360\n",
      "Phase: train. Epoch: 4. Loss: 0.1444406509399414\n",
      "4 train 30 372\n",
      "Phase: train. Epoch: 4. Loss: 0.18858325481414795\n",
      "4 train 31 384\n",
      "Phase: train. Epoch: 4. Loss: 0.15695567429065704\n",
      "4 train 32 396\n",
      "Phase: train. Epoch: 4. Loss: 0.14165043830871582\n",
      "4 train 33 408\n",
      "Phase: train. Epoch: 4. Loss: 0.15953329205513\n",
      "4 train 34 420\n",
      "Phase: train. Epoch: 4. Loss: 0.15908941626548767\n",
      "4 train 35 432\n",
      "Phase: train. Epoch: 4. Loss: 0.16663508117198944\n",
      "4 train 36 444\n",
      "Phase: train. Epoch: 4. Loss: 0.17330670356750488\n",
      "4 train 37 456\n",
      "Phase: train. Epoch: 4. Loss: 0.136794775724411\n",
      "4 train 38 468\n",
      "Phase: train. Epoch: 4. Loss: 0.1423487663269043\n",
      "4 train 39 480\n",
      "Phase: train. Epoch: 4. Loss: 0.1633836030960083\n",
      "4 train 40 492\n",
      "Phase: train. Epoch: 4. Loss: 0.13591957092285156\n",
      "4 train 41 504\n",
      "Phase: train. Epoch: 4. Loss: 0.18861854076385498\n",
      "4 train 42 516\n",
      "Phase: train. Epoch: 4. Loss: 0.15073198080062866\n",
      "4 train 43 528\n",
      "Phase: train. Epoch: 4. Loss: 0.168246328830719\n",
      "4 train 44 540\n",
      "Phase: train. Epoch: 4. Loss: 0.17210142314434052\n",
      "4 train 45 552\n",
      "Phase: train. Epoch: 4. Loss: 0.16533030569553375\n",
      "4 train 46 564\n",
      "Phase: train. Epoch: 4. Loss: 0.1597663015127182\n",
      "4 train 47 576\n",
      "Phase: train. Epoch: 4. Loss: 0.1585145890712738\n",
      "4 train 48 588\n",
      "Phase: train. Epoch: 4. Loss: 0.15281817317008972\n",
      "4 train 49 600\n",
      "Phase: train. Epoch: 4. Loss: 0.13729992508888245\n",
      "4 train 50 612\n",
      "Phase: train. Epoch: 4. Loss: 0.16573192179203033\n",
      "4 train 51 624\n",
      "Phase: train. Epoch: 4. Loss: 0.1556277722120285\n",
      "4 train 52 636\n",
      "Phase: train. Epoch: 4. Loss: 0.1683960258960724\n",
      "4 train 53 648\n",
      "Phase: train. Epoch: 4. Loss: 0.17063000798225403\n",
      "4 train 54 660\n",
      "Phase: train. Epoch: 4. Loss: 0.16088268160820007\n",
      "4 train 55 672\n",
      "Phase: train. Epoch: 4. Loss: 0.15697690844535828\n",
      "4 train 56 684\n",
      "Phase: train. Epoch: 4. Loss: 0.12949584424495697\n",
      "4 train 57 696\n",
      "Phase: train. Epoch: 4. Loss: 0.1367500126361847\n",
      "4 train 58 708\n",
      "Phase: train. Epoch: 4. Loss: 0.1557634323835373\n",
      "4 train 59 720\n",
      "Phase: train. Epoch: 4. Loss: 0.14080286026000977\n",
      "4 train 60 732\n",
      "Phase: train. Epoch: 4. Loss: 0.1302473247051239\n",
      "4 train 61 744\n",
      "Phase: train. Epoch: 4. Loss: 0.12862859666347504\n",
      "4 train 62 751\n",
      "Phase: train. Epoch: 4. Loss: 0.12634754180908203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 val 0 763\n",
      "Phase: val. Epoch: 4. Loss: 0.11561737954616547\n",
      "4 val 1 775\n",
      "Phase: val. Epoch: 4. Loss: 0.14158028364181519\n",
      "4 val 2 787\n",
      "Phase: val. Epoch: 4. Loss: 0.18128201365470886\n",
      "4 val 3 799\n",
      "Phase: val. Epoch: 4. Loss: 0.13637307286262512\n",
      "4 val 4 811\n",
      "Phase: val. Epoch: 4. Loss: 0.14396755397319794\n",
      "4 val 5 823\n",
      "Phase: val. Epoch: 4. Loss: 0.10777193307876587\n",
      "4 val 6 835\n",
      "Phase: val. Epoch: 4. Loss: 0.15729865431785583\n",
      "4 val 7 847\n",
      "Phase: val. Epoch: 4. Loss: 0.1609068363904953\n",
      "4 val 8 859\n",
      "Phase: val. Epoch: 4. Loss: 0.12926827371120453\n",
      "4 val 9 871\n",
      "Phase: val. Epoch: 4. Loss: 0.12248244136571884\n",
      "4 val 10 883\n",
      "Phase: val. Epoch: 4. Loss: 0.15753701329231262\n",
      "4 val 11 884\n",
      "Phase: val. Epoch: 4. Loss: 0.10098059475421906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 train 0 12\n",
      "Phase: train. Epoch: 5. Loss: 0.1661810278892517\n",
      "5 train 1 24\n",
      "Phase: train. Epoch: 5. Loss: 0.1589730978012085\n",
      "5 train 2 36\n",
      "Phase: train. Epoch: 5. Loss: 0.15433304011821747\n",
      "5 train 3 48\n",
      "Phase: train. Epoch: 5. Loss: 0.18817226588726044\n",
      "5 train 4 60\n",
      "Phase: train. Epoch: 5. Loss: 0.15673598647117615\n",
      "5 train 5 72\n",
      "Phase: train. Epoch: 5. Loss: 0.14332203567028046\n",
      "5 train 6 84\n",
      "Phase: train. Epoch: 5. Loss: 0.15801456570625305\n",
      "5 train 7 96\n",
      "Phase: train. Epoch: 5. Loss: 0.15365992486476898\n",
      "5 train 8 108\n",
      "Phase: train. Epoch: 5. Loss: 0.13587328791618347\n",
      "5 train 9 120\n",
      "Phase: train. Epoch: 5. Loss: 0.131633460521698\n",
      "5 train 10 132\n",
      "Phase: train. Epoch: 5. Loss: 0.1586139053106308\n",
      "5 train 11 144\n",
      "Phase: train. Epoch: 5. Loss: 0.15285737812519073\n",
      "5 train 12 156\n",
      "Phase: train. Epoch: 5. Loss: 0.15966539084911346\n",
      "5 train 13 168\n",
      "Phase: train. Epoch: 5. Loss: 0.1922711580991745\n",
      "5 train 14 180\n",
      "Phase: train. Epoch: 5. Loss: 0.14522843062877655\n",
      "5 train 15 192\n",
      "Phase: train. Epoch: 5. Loss: 0.14637601375579834\n",
      "5 train 16 204\n",
      "Phase: train. Epoch: 5. Loss: 0.14733806252479553\n",
      "5 train 17 216\n",
      "Phase: train. Epoch: 5. Loss: 0.12867984175682068\n",
      "5 train 18 228\n",
      "Phase: train. Epoch: 5. Loss: 0.14221547544002533\n",
      "5 train 19 240\n",
      "Phase: train. Epoch: 5. Loss: 0.13567200303077698\n",
      "5 train 20 252\n",
      "Phase: train. Epoch: 5. Loss: 0.14387448132038116\n",
      "5 train 21 264\n",
      "Phase: train. Epoch: 5. Loss: 0.13723641633987427\n",
      "5 train 22 276\n",
      "Phase: train. Epoch: 5. Loss: 0.17325431108474731\n",
      "5 train 23 288\n",
      "Phase: train. Epoch: 5. Loss: 0.1272788643836975\n",
      "5 train 24 300\n",
      "Phase: train. Epoch: 5. Loss: 0.14223818480968475\n",
      "5 train 25 312\n",
      "Phase: train. Epoch: 5. Loss: 0.12570536136627197\n",
      "5 train 26 324\n",
      "Phase: train. Epoch: 5. Loss: 0.15532980859279633\n",
      "5 train 27 336\n",
      "Phase: train. Epoch: 5. Loss: 0.14794965088367462\n",
      "5 train 28 348\n",
      "Phase: train. Epoch: 5. Loss: 0.12933580577373505\n",
      "5 train 29 360\n",
      "Phase: train. Epoch: 5. Loss: 0.14896610379219055\n",
      "5 train 30 372\n",
      "Phase: train. Epoch: 5. Loss: 0.1567022204399109\n",
      "5 train 31 384\n",
      "Phase: train. Epoch: 5. Loss: 0.15780317783355713\n",
      "5 train 32 396\n",
      "Phase: train. Epoch: 5. Loss: 0.13394972681999207\n",
      "5 train 33 408\n",
      "Phase: train. Epoch: 5. Loss: 0.17452363669872284\n",
      "5 train 34 420\n",
      "Phase: train. Epoch: 5. Loss: 0.14516383409500122\n",
      "5 train 35 432\n",
      "Phase: train. Epoch: 5. Loss: 0.12098120898008347\n",
      "5 train 36 444\n",
      "Phase: train. Epoch: 5. Loss: 0.13799555599689484\n",
      "5 train 37 456\n",
      "Phase: train. Epoch: 5. Loss: 0.13480202853679657\n",
      "5 train 38 468\n",
      "Phase: train. Epoch: 5. Loss: 0.1636141538619995\n",
      "5 train 39 480\n",
      "Phase: train. Epoch: 5. Loss: 0.13091038167476654\n",
      "5 train 40 492\n",
      "Phase: train. Epoch: 5. Loss: 0.1704929769039154\n",
      "5 train 41 504\n",
      "Phase: train. Epoch: 5. Loss: 0.12448390573263168\n",
      "5 train 42 516\n",
      "Phase: train. Epoch: 5. Loss: 0.12268140912055969\n",
      "5 train 43 528\n",
      "Phase: train. Epoch: 5. Loss: 0.12763015925884247\n",
      "5 train 44 540\n",
      "Phase: train. Epoch: 5. Loss: 0.1325509399175644\n",
      "5 train 45 552\n",
      "Phase: train. Epoch: 5. Loss: 0.14163877069950104\n",
      "5 train 46 564\n",
      "Phase: train. Epoch: 5. Loss: 0.16051137447357178\n",
      "5 train 47 576\n",
      "Phase: train. Epoch: 5. Loss: 0.11212267726659775\n",
      "5 train 48 588\n",
      "Phase: train. Epoch: 5. Loss: 0.11356087774038315\n",
      "5 train 49 600\n",
      "Phase: train. Epoch: 5. Loss: 0.13492411375045776\n",
      "5 train 50 612\n",
      "Phase: train. Epoch: 5. Loss: 0.11711201071739197\n",
      "5 train 51 624\n",
      "Phase: train. Epoch: 5. Loss: 0.12426622956991196\n",
      "5 train 52 636\n",
      "Phase: train. Epoch: 5. Loss: 0.151620015501976\n",
      "5 train 53 648\n",
      "Phase: train. Epoch: 5. Loss: 0.10767873376607895\n",
      "5 train 54 660\n",
      "Phase: train. Epoch: 5. Loss: 0.13928218185901642\n",
      "5 train 55 672\n",
      "Phase: train. Epoch: 5. Loss: 0.15431265532970428\n",
      "5 train 56 684\n",
      "Phase: train. Epoch: 5. Loss: 0.12051715701818466\n",
      "5 train 57 696\n",
      "Phase: train. Epoch: 5. Loss: 0.13817402720451355\n",
      "5 train 58 708\n",
      "Phase: train. Epoch: 5. Loss: 0.12239387631416321\n",
      "5 train 59 720\n",
      "Phase: train. Epoch: 5. Loss: 0.12415077537298203\n",
      "5 train 60 732\n",
      "Phase: train. Epoch: 5. Loss: 0.1738455891609192\n",
      "5 train 61 744\n",
      "Phase: train. Epoch: 5. Loss: 0.1360732614994049\n",
      "5 train 62 751\n",
      "Phase: train. Epoch: 5. Loss: 0.13654717803001404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 val 0 763\n",
      "Phase: val. Epoch: 5. Loss: 0.12553682923316956\n",
      "5 val 1 775\n",
      "Phase: val. Epoch: 5. Loss: 0.12348996102809906\n",
      "5 val 2 787\n",
      "Phase: val. Epoch: 5. Loss: 0.12458185851573944\n",
      "5 val 3 799\n",
      "Phase: val. Epoch: 5. Loss: 0.12042932212352753\n",
      "5 val 4 811\n",
      "Phase: val. Epoch: 5. Loss: 0.12079788744449615\n",
      "5 val 5 823\n",
      "Phase: val. Epoch: 5. Loss: 0.15605740249156952\n",
      "5 val 6 835\n",
      "Phase: val. Epoch: 5. Loss: 0.14012381434440613\n",
      "5 val 7 847\n",
      "Phase: val. Epoch: 5. Loss: 0.14802658557891846\n",
      "5 val 8 859\n",
      "Phase: val. Epoch: 5. Loss: 0.12343297898769379\n",
      "5 val 9 871\n",
      "Phase: val. Epoch: 5. Loss: 0.12166571617126465\n",
      "5 val 10 883\n",
      "Phase: val. Epoch: 5. Loss: 0.11779257655143738\n",
      "5 val 11 884\n",
      "Phase: val. Epoch: 5. Loss: 0.16403742134571075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 train 0 12\n",
      "Phase: train. Epoch: 6. Loss: 0.15953171253204346\n",
      "6 train 1 24\n",
      "Phase: train. Epoch: 6. Loss: 0.13081970810890198\n",
      "6 train 2 36\n",
      "Phase: train. Epoch: 6. Loss: 0.14486843347549438\n",
      "6 train 3 48\n",
      "Phase: train. Epoch: 6. Loss: 0.13294897973537445\n",
      "6 train 4 60\n",
      "Phase: train. Epoch: 6. Loss: 0.12785086035728455\n",
      "6 train 5 72\n",
      "Phase: train. Epoch: 6. Loss: 0.12836770713329315\n",
      "6 train 6 84\n",
      "Phase: train. Epoch: 6. Loss: 0.12532325088977814\n",
      "6 train 7 96\n",
      "Phase: train. Epoch: 6. Loss: 0.15820561349391937\n",
      "6 train 8 108\n",
      "Phase: train. Epoch: 6. Loss: 0.14374864101409912\n",
      "6 train 9 120\n",
      "Phase: train. Epoch: 6. Loss: 0.12359727919101715\n",
      "6 train 10 132\n",
      "Phase: train. Epoch: 6. Loss: 0.11588114500045776\n",
      "6 train 11 144\n",
      "Phase: train. Epoch: 6. Loss: 0.12060604989528656\n",
      "6 train 12 156\n",
      "Phase: train. Epoch: 6. Loss: 0.1232171282172203\n",
      "6 train 13 168\n",
      "Phase: train. Epoch: 6. Loss: 0.12476937472820282\n",
      "6 train 14 180\n",
      "Phase: train. Epoch: 6. Loss: 0.12384748458862305\n",
      "6 train 15 192\n",
      "Phase: train. Epoch: 6. Loss: 0.1266857087612152\n",
      "6 train 16 204\n",
      "Phase: train. Epoch: 6. Loss: 0.1525992453098297\n",
      "6 train 17 216\n",
      "Phase: train. Epoch: 6. Loss: 0.11177351325750351\n",
      "6 train 18 228\n",
      "Phase: train. Epoch: 6. Loss: 0.13367609679698944\n",
      "6 train 19 240\n",
      "Phase: train. Epoch: 6. Loss: 0.16185292601585388\n",
      "6 train 20 252\n",
      "Phase: train. Epoch: 6. Loss: 0.1588972806930542\n",
      "6 train 21 264\n",
      "Phase: train. Epoch: 6. Loss: 0.13379985094070435\n",
      "6 train 22 276\n",
      "Phase: train. Epoch: 6. Loss: 0.12303505837917328\n",
      "6 train 23 288\n",
      "Phase: train. Epoch: 6. Loss: 0.13923870027065277\n",
      "6 train 24 300\n",
      "Phase: train. Epoch: 6. Loss: 0.14144906401634216\n",
      "6 train 25 312\n",
      "Phase: train. Epoch: 6. Loss: 0.13177648186683655\n",
      "6 train 26 324\n",
      "Phase: train. Epoch: 6. Loss: 0.11733180284500122\n",
      "6 train 27 336\n",
      "Phase: train. Epoch: 6. Loss: 0.15054847300052643\n",
      "6 train 28 348\n",
      "Phase: train. Epoch: 6. Loss: 0.11785852909088135\n",
      "6 train 29 360\n",
      "Phase: train. Epoch: 6. Loss: 0.14453242719173431\n",
      "6 train 30 372\n",
      "Phase: train. Epoch: 6. Loss: 0.14376495778560638\n",
      "6 train 31 384\n",
      "Phase: train. Epoch: 6. Loss: 0.12676408886909485\n",
      "6 train 32 396\n",
      "Phase: train. Epoch: 6. Loss: 0.14209526777267456\n",
      "6 train 33 408\n",
      "Phase: train. Epoch: 6. Loss: 0.12979665398597717\n",
      "6 train 34 420\n",
      "Phase: train. Epoch: 6. Loss: 0.15409724414348602\n",
      "6 train 35 432\n",
      "Phase: train. Epoch: 6. Loss: 0.12753362953662872\n",
      "6 train 36 444\n",
      "Phase: train. Epoch: 6. Loss: 0.15056335926055908\n",
      "6 train 37 456\n",
      "Phase: train. Epoch: 6. Loss: 0.1476796418428421\n",
      "6 train 38 468\n",
      "Phase: train. Epoch: 6. Loss: 0.12841680645942688\n",
      "6 train 39 480\n",
      "Phase: train. Epoch: 6. Loss: 0.12734699249267578\n",
      "6 train 40 492\n",
      "Phase: train. Epoch: 6. Loss: 0.151364266872406\n",
      "6 train 41 504\n",
      "Phase: train. Epoch: 6. Loss: 0.13353046774864197\n",
      "6 train 42 516\n",
      "Phase: train. Epoch: 6. Loss: 0.1148914024233818\n",
      "6 train 43 528\n",
      "Phase: train. Epoch: 6. Loss: 0.1142873466014862\n",
      "6 train 44 540\n",
      "Phase: train. Epoch: 6. Loss: 0.144980326294899\n",
      "6 train 45 552\n",
      "Phase: train. Epoch: 6. Loss: 0.11790035665035248\n",
      "6 train 46 564\n",
      "Phase: train. Epoch: 6. Loss: 0.11586420238018036\n",
      "6 train 47 576\n",
      "Phase: train. Epoch: 6. Loss: 0.1378079652786255\n",
      "6 train 48 588\n",
      "Phase: train. Epoch: 6. Loss: 0.11625638604164124\n",
      "6 train 49 600\n",
      "Phase: train. Epoch: 6. Loss: 0.1480015218257904\n",
      "6 train 50 612\n",
      "Phase: train. Epoch: 6. Loss: 0.14487597346305847\n",
      "6 train 51 624\n",
      "Phase: train. Epoch: 6. Loss: 0.12937138974666595\n",
      "6 train 52 636\n",
      "Phase: train. Epoch: 6. Loss: 0.12175145000219345\n",
      "6 train 53 648\n",
      "Phase: train. Epoch: 6. Loss: 0.11970265954732895\n",
      "6 train 54 660\n",
      "Phase: train. Epoch: 6. Loss: 0.10777634382247925\n",
      "6 train 55 672\n",
      "Phase: train. Epoch: 6. Loss: 0.131321519613266\n",
      "6 train 56 684\n",
      "Phase: train. Epoch: 6. Loss: 0.11800257861614227\n",
      "6 train 57 696\n",
      "Phase: train. Epoch: 6. Loss: 0.10919010639190674\n",
      "6 train 58 708\n",
      "Phase: train. Epoch: 6. Loss: 0.12273403257131577\n",
      "6 train 59 720\n",
      "Phase: train. Epoch: 6. Loss: 0.11164026707410812\n",
      "6 train 60 732\n",
      "Phase: train. Epoch: 6. Loss: 0.12136708199977875\n",
      "6 train 61 744\n",
      "Phase: train. Epoch: 6. Loss: 0.11825704574584961\n",
      "6 train 62 751\n",
      "Phase: train. Epoch: 6. Loss: 0.11832845211029053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 val 0 763\n",
      "Phase: val. Epoch: 6. Loss: 0.12352454662322998\n",
      "6 val 1 775\n",
      "Phase: val. Epoch: 6. Loss: 0.11278687417507172\n",
      "6 val 2 787\n",
      "Phase: val. Epoch: 6. Loss: 0.14729009568691254\n",
      "6 val 3 799\n",
      "Phase: val. Epoch: 6. Loss: 0.11383466422557831\n",
      "6 val 4 811\n",
      "Phase: val. Epoch: 6. Loss: 0.12767153978347778\n",
      "6 val 5 823\n",
      "Phase: val. Epoch: 6. Loss: 0.11145957559347153\n",
      "6 val 6 835\n",
      "Phase: val. Epoch: 6. Loss: 0.12543624639511108\n",
      "6 val 7 847\n",
      "Phase: val. Epoch: 6. Loss: 0.15851978957653046\n",
      "6 val 8 859\n",
      "Phase: val. Epoch: 6. Loss: 0.12337635457515717\n",
      "6 val 9 871\n",
      "Phase: val. Epoch: 6. Loss: 0.12663644552230835\n",
      "6 val 10 883\n",
      "Phase: val. Epoch: 6. Loss: 0.08978140354156494\n",
      "6 val 11 884\n",
      "Phase: val. Epoch: 6. Loss: 0.0600275844335556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 train 0 12\n",
      "Phase: train. Epoch: 7. Loss: 0.13396960496902466\n",
      "7 train 1 24\n",
      "Phase: train. Epoch: 7. Loss: 0.14327667653560638\n",
      "7 train 2 36\n",
      "Phase: train. Epoch: 7. Loss: 0.116505466401577\n",
      "7 train 3 48\n",
      "Phase: train. Epoch: 7. Loss: 0.15733584761619568\n",
      "7 train 4 60\n",
      "Phase: train. Epoch: 7. Loss: 0.11104544997215271\n",
      "7 train 5 72\n",
      "Phase: train. Epoch: 7. Loss: 0.1192280501127243\n",
      "7 train 6 84\n",
      "Phase: train. Epoch: 7. Loss: 0.11407846212387085\n",
      "7 train 7 96\n",
      "Phase: train. Epoch: 7. Loss: 0.15776406228542328\n",
      "7 train 8 108\n",
      "Phase: train. Epoch: 7. Loss: 0.1103251725435257\n",
      "7 train 9 120\n",
      "Phase: train. Epoch: 7. Loss: 0.1472809910774231\n",
      "7 train 10 132\n",
      "Phase: train. Epoch: 7. Loss: 0.1285647749900818\n",
      "7 train 11 144\n",
      "Phase: train. Epoch: 7. Loss: 0.13388381898403168\n",
      "7 train 12 156\n",
      "Phase: train. Epoch: 7. Loss: 0.12722548842430115\n",
      "7 train 13 168\n",
      "Phase: train. Epoch: 7. Loss: 0.10693353414535522\n",
      "7 train 14 180\n",
      "Phase: train. Epoch: 7. Loss: 0.13171596825122833\n",
      "7 train 15 192\n",
      "Phase: train. Epoch: 7. Loss: 0.12074463069438934\n",
      "7 train 16 204\n",
      "Phase: train. Epoch: 7. Loss: 0.13830383121967316\n",
      "7 train 17 216\n",
      "Phase: train. Epoch: 7. Loss: 0.12793686985969543\n",
      "7 train 18 228\n",
      "Phase: train. Epoch: 7. Loss: 0.11672906577587128\n",
      "7 train 19 240\n",
      "Phase: train. Epoch: 7. Loss: 0.11380618810653687\n",
      "7 train 20 252\n",
      "Phase: train. Epoch: 7. Loss: 0.1268293708562851\n",
      "7 train 21 264\n",
      "Phase: train. Epoch: 7. Loss: 0.12497782707214355\n",
      "7 train 22 276\n",
      "Phase: train. Epoch: 7. Loss: 0.12334509938955307\n",
      "7 train 23 288\n",
      "Phase: train. Epoch: 7. Loss: 0.13103178143501282\n",
      "7 train 24 300\n",
      "Phase: train. Epoch: 7. Loss: 0.1294735074043274\n",
      "7 train 25 312\n",
      "Phase: train. Epoch: 7. Loss: 0.1494121551513672\n",
      "7 train 26 324\n",
      "Phase: train. Epoch: 7. Loss: 0.15916860103607178\n",
      "7 train 27 336\n",
      "Phase: train. Epoch: 7. Loss: 0.13848553597927094\n",
      "7 train 28 348\n",
      "Phase: train. Epoch: 7. Loss: 0.10141732543706894\n",
      "7 train 29 360\n",
      "Phase: train. Epoch: 7. Loss: 0.09827103465795517\n",
      "7 train 30 372\n",
      "Phase: train. Epoch: 7. Loss: 0.12629655003547668\n",
      "7 train 31 384\n",
      "Phase: train. Epoch: 7. Loss: 0.11246787011623383\n",
      "7 train 32 396\n",
      "Phase: train. Epoch: 7. Loss: 0.1025996208190918\n",
      "7 train 33 408\n",
      "Phase: train. Epoch: 7. Loss: 0.09487396478652954\n",
      "7 train 34 420\n",
      "Phase: train. Epoch: 7. Loss: 0.11586661636829376\n",
      "7 train 35 432\n",
      "Phase: train. Epoch: 7. Loss: 0.1134941428899765\n",
      "7 train 36 444\n",
      "Phase: train. Epoch: 7. Loss: 0.13523808121681213\n",
      "7 train 37 456\n",
      "Phase: train. Epoch: 7. Loss: 0.1217779740691185\n",
      "7 train 38 468\n",
      "Phase: train. Epoch: 7. Loss: 0.10000742971897125\n",
      "7 train 39 480\n",
      "Phase: train. Epoch: 7. Loss: 0.11287681758403778\n",
      "7 train 40 492\n",
      "Phase: train. Epoch: 7. Loss: 0.118634432554245\n",
      "7 train 41 504\n",
      "Phase: train. Epoch: 7. Loss: 0.1075662225484848\n",
      "7 train 42 516\n",
      "Phase: train. Epoch: 7. Loss: 0.11422769725322723\n",
      "7 train 43 528\n",
      "Phase: train. Epoch: 7. Loss: 0.13205024600028992\n",
      "7 train 44 540\n",
      "Phase: train. Epoch: 7. Loss: 0.10952170938253403\n",
      "7 train 45 552\n",
      "Phase: train. Epoch: 7. Loss: 0.09002859890460968\n",
      "7 train 46 564\n",
      "Phase: train. Epoch: 7. Loss: 0.1353093683719635\n",
      "7 train 47 576\n",
      "Phase: train. Epoch: 7. Loss: 0.12002640962600708\n",
      "7 train 48 588\n",
      "Phase: train. Epoch: 7. Loss: 0.12963777780532837\n",
      "7 train 49 600\n",
      "Phase: train. Epoch: 7. Loss: 0.13030055165290833\n",
      "7 train 50 612\n",
      "Phase: train. Epoch: 7. Loss: 0.10463603585958481\n",
      "7 train 51 624\n",
      "Phase: train. Epoch: 7. Loss: 0.12407764047384262\n",
      "7 train 52 636\n",
      "Phase: train. Epoch: 7. Loss: 0.10968693345785141\n",
      "7 train 53 648\n",
      "Phase: train. Epoch: 7. Loss: 0.10772727429866791\n",
      "7 train 54 660\n",
      "Phase: train. Epoch: 7. Loss: 0.09911813586950302\n",
      "7 train 55 672\n",
      "Phase: train. Epoch: 7. Loss: 0.09747463464736938\n",
      "7 train 56 684\n",
      "Phase: train. Epoch: 7. Loss: 0.11091996729373932\n",
      "7 train 57 696\n",
      "Phase: train. Epoch: 7. Loss: 0.12285207211971283\n",
      "7 train 58 708\n",
      "Phase: train. Epoch: 7. Loss: 0.12009509652853012\n",
      "7 train 59 720\n",
      "Phase: train. Epoch: 7. Loss: 0.09756779670715332\n",
      "7 train 60 732\n",
      "Phase: train. Epoch: 7. Loss: 0.11972232908010483\n",
      "7 train 61 744\n",
      "Phase: train. Epoch: 7. Loss: 0.10192660987377167\n",
      "7 train 62 751\n",
      "Phase: train. Epoch: 7. Loss: 0.11032699048519135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 val 0 763\n",
      "Phase: val. Epoch: 7. Loss: 0.10925163328647614\n",
      "7 val 1 775\n",
      "Phase: val. Epoch: 7. Loss: 0.10839593410491943\n",
      "7 val 2 787\n",
      "Phase: val. Epoch: 7. Loss: 0.1374296396970749\n",
      "7 val 3 799\n",
      "Phase: val. Epoch: 7. Loss: 0.10892721265554428\n",
      "7 val 4 811\n",
      "Phase: val. Epoch: 7. Loss: 0.09625928103923798\n",
      "7 val 5 823\n",
      "Phase: val. Epoch: 7. Loss: 0.1543968915939331\n",
      "7 val 6 835\n",
      "Phase: val. Epoch: 7. Loss: 0.08383815735578537\n",
      "7 val 7 847\n",
      "Phase: val. Epoch: 7. Loss: 0.09971389174461365\n",
      "7 val 8 859\n",
      "Phase: val. Epoch: 7. Loss: 0.1063293069601059\n",
      "7 val 9 871\n",
      "Phase: val. Epoch: 7. Loss: 0.11860739439725876\n",
      "7 val 10 883\n",
      "Phase: val. Epoch: 7. Loss: 0.12537363171577454\n",
      "7 val 11 884\n",
      "Phase: val. Epoch: 7. Loss: 0.09509047865867615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 train 0 12\n",
      "Phase: train. Epoch: 8. Loss: 0.14082178473472595\n",
      "8 train 1 24\n",
      "Phase: train. Epoch: 8. Loss: 0.10908560454845428\n",
      "8 train 2 36\n",
      "Phase: train. Epoch: 8. Loss: 0.11933120340108871\n",
      "8 train 3 48\n",
      "Phase: train. Epoch: 8. Loss: 0.11569397151470184\n",
      "8 train 4 60\n",
      "Phase: train. Epoch: 8. Loss: 0.11124537885189056\n",
      "8 train 5 72\n",
      "Phase: train. Epoch: 8. Loss: 0.1289144605398178\n",
      "8 train 6 84\n",
      "Phase: train. Epoch: 8. Loss: 0.12121963500976562\n",
      "8 train 7 96\n",
      "Phase: train. Epoch: 8. Loss: 0.10610850900411606\n",
      "8 train 8 108\n",
      "Phase: train. Epoch: 8. Loss: 0.13483521342277527\n",
      "8 train 9 120\n",
      "Phase: train. Epoch: 8. Loss: 0.12479250133037567\n",
      "8 train 10 132\n",
      "Phase: train. Epoch: 8. Loss: 0.11063991487026215\n",
      "8 train 11 144\n",
      "Phase: train. Epoch: 8. Loss: 0.09837879985570908\n",
      "8 train 12 156\n",
      "Phase: train. Epoch: 8. Loss: 0.13195407390594482\n",
      "8 train 13 168\n",
      "Phase: train. Epoch: 8. Loss: 0.11243482679128647\n",
      "8 train 14 180\n",
      "Phase: train. Epoch: 8. Loss: 0.13630355894565582\n",
      "8 train 15 192\n",
      "Phase: train. Epoch: 8. Loss: 0.12155783176422119\n",
      "8 train 16 204\n",
      "Phase: train. Epoch: 8. Loss: 0.13749665021896362\n",
      "8 train 17 216\n",
      "Phase: train. Epoch: 8. Loss: 0.11171619594097137\n",
      "8 train 18 228\n",
      "Phase: train. Epoch: 8. Loss: 0.09078793227672577\n",
      "8 train 19 240\n",
      "Phase: train. Epoch: 8. Loss: 0.12901633977890015\n",
      "8 train 20 252\n",
      "Phase: train. Epoch: 8. Loss: 0.1308605670928955\n",
      "8 train 21 264\n",
      "Phase: train. Epoch: 8. Loss: 0.11100675910711288\n",
      "8 train 22 276\n",
      "Phase: train. Epoch: 8. Loss: 0.09133442491292953\n",
      "8 train 23 288\n",
      "Phase: train. Epoch: 8. Loss: 0.10025808215141296\n",
      "8 train 24 300\n",
      "Phase: train. Epoch: 8. Loss: 0.09592714160680771\n",
      "8 train 25 312\n",
      "Phase: train. Epoch: 8. Loss: 0.10824119299650192\n",
      "8 train 26 324\n",
      "Phase: train. Epoch: 8. Loss: 0.10549794137477875\n",
      "8 train 27 336\n",
      "Phase: train. Epoch: 8. Loss: 0.09217709302902222\n",
      "8 train 28 348\n",
      "Phase: train. Epoch: 8. Loss: 0.11655211448669434\n",
      "8 train 29 360\n",
      "Phase: train. Epoch: 8. Loss: 0.11861295998096466\n",
      "8 train 30 372\n",
      "Phase: train. Epoch: 8. Loss: 0.11760660260915756\n",
      "8 train 31 384\n",
      "Phase: train. Epoch: 8. Loss: 0.1306171715259552\n",
      "8 train 32 396\n",
      "Phase: train. Epoch: 8. Loss: 0.1036892980337143\n",
      "8 train 33 408\n",
      "Phase: train. Epoch: 8. Loss: 0.1387287974357605\n",
      "8 train 34 420\n",
      "Phase: train. Epoch: 8. Loss: 0.09603099524974823\n",
      "8 train 35 432\n",
      "Phase: train. Epoch: 8. Loss: 0.12655340135097504\n",
      "8 train 36 444\n",
      "Phase: train. Epoch: 8. Loss: 0.10875293612480164\n",
      "8 train 37 456\n",
      "Phase: train. Epoch: 8. Loss: 0.15031571686267853\n",
      "8 train 38 468\n",
      "Phase: train. Epoch: 8. Loss: 0.09945505857467651\n",
      "8 train 39 480\n",
      "Phase: train. Epoch: 8. Loss: 0.10853167623281479\n",
      "8 train 40 492\n",
      "Phase: train. Epoch: 8. Loss: 0.09995824098587036\n",
      "8 train 41 504\n",
      "Phase: train. Epoch: 8. Loss: 0.09891080856323242\n",
      "8 train 42 516\n",
      "Phase: train. Epoch: 8. Loss: 0.0914355143904686\n",
      "8 train 43 528\n",
      "Phase: train. Epoch: 8. Loss: 0.10331903398036957\n",
      "8 train 44 540\n",
      "Phase: train. Epoch: 8. Loss: 0.10567270219326019\n",
      "8 train 45 552\n",
      "Phase: train. Epoch: 8. Loss: 0.10938946902751923\n",
      "8 train 46 564\n",
      "Phase: train. Epoch: 8. Loss: 0.10187240689992905\n",
      "8 train 47 576\n",
      "Phase: train. Epoch: 8. Loss: 0.10545600950717926\n",
      "8 train 48 588\n",
      "Phase: train. Epoch: 8. Loss: 0.1429363638162613\n",
      "8 train 49 600\n",
      "Phase: train. Epoch: 8. Loss: 0.13193804025650024\n",
      "8 train 50 612\n",
      "Phase: train. Epoch: 8. Loss: 0.10174097120761871\n",
      "8 train 51 624\n",
      "Phase: train. Epoch: 8. Loss: 0.09587094187736511\n",
      "8 train 52 636\n",
      "Phase: train. Epoch: 8. Loss: 0.10946779698133469\n",
      "8 train 53 648\n",
      "Phase: train. Epoch: 8. Loss: 0.10227726399898529\n",
      "8 train 54 660\n",
      "Phase: train. Epoch: 8. Loss: 0.09804084897041321\n",
      "8 train 55 672\n",
      "Phase: train. Epoch: 8. Loss: 0.10237770527601242\n",
      "8 train 56 684\n",
      "Phase: train. Epoch: 8. Loss: 0.098041832447052\n",
      "8 train 57 696\n",
      "Phase: train. Epoch: 8. Loss: 0.12832236289978027\n",
      "8 train 58 708\n",
      "Phase: train. Epoch: 8. Loss: 0.10584547370672226\n",
      "8 train 59 720\n",
      "Phase: train. Epoch: 8. Loss: 0.12271749973297119\n",
      "8 train 60 732\n",
      "Phase: train. Epoch: 8. Loss: 0.08878686279058456\n",
      "8 train 61 744\n",
      "Phase: train. Epoch: 8. Loss: 0.12544339895248413\n",
      "8 train 62 751\n",
      "Phase: train. Epoch: 8. Loss: 0.12116879224777222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 val 0 763\n",
      "Phase: val. Epoch: 8. Loss: 0.10819977521896362\n",
      "8 val 1 775\n",
      "Phase: val. Epoch: 8. Loss: 0.07607744634151459\n",
      "8 val 2 787\n",
      "Phase: val. Epoch: 8. Loss: 0.0994371771812439\n",
      "8 val 3 799\n",
      "Phase: val. Epoch: 8. Loss: 0.10564616322517395\n",
      "8 val 4 811\n",
      "Phase: val. Epoch: 8. Loss: 0.11463092267513275\n",
      "8 val 5 823\n",
      "Phase: val. Epoch: 8. Loss: 0.10106005519628525\n",
      "8 val 6 835\n",
      "Phase: val. Epoch: 8. Loss: 0.14603686332702637\n",
      "8 val 7 847\n",
      "Phase: val. Epoch: 8. Loss: 0.1064857766032219\n",
      "8 val 8 859\n",
      "Phase: val. Epoch: 8. Loss: 0.11896999180316925\n",
      "8 val 9 871\n",
      "Phase: val. Epoch: 8. Loss: 0.13755951821804047\n",
      "8 val 10 883\n",
      "Phase: val. Epoch: 8. Loss: 0.10409997403621674\n",
      "8 val 11 884\n",
      "Phase: val. Epoch: 8. Loss: 0.05306706205010414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 train 0 12\n",
      "Phase: train. Epoch: 9. Loss: 0.0949171632528305\n",
      "9 train 1 24\n",
      "Phase: train. Epoch: 9. Loss: 0.1258409023284912\n",
      "9 train 2 36\n",
      "Phase: train. Epoch: 9. Loss: 0.11072062700986862\n",
      "9 train 3 48\n",
      "Phase: train. Epoch: 9. Loss: 0.08825157582759857\n",
      "9 train 4 60\n",
      "Phase: train. Epoch: 9. Loss: 0.12485154718160629\n",
      "9 train 5 72\n",
      "Phase: train. Epoch: 9. Loss: 0.14804211258888245\n",
      "9 train 6 84\n",
      "Phase: train. Epoch: 9. Loss: 0.10105203092098236\n",
      "9 train 7 96\n",
      "Phase: train. Epoch: 9. Loss: 0.0900493785738945\n",
      "9 train 8 108\n",
      "Phase: train. Epoch: 9. Loss: 0.12882807850837708\n",
      "9 train 9 120\n",
      "Phase: train. Epoch: 9. Loss: 0.10273091495037079\n",
      "9 train 10 132\n",
      "Phase: train. Epoch: 9. Loss: 0.11886363476514816\n",
      "9 train 11 144\n",
      "Phase: train. Epoch: 9. Loss: 0.14898203313350677\n",
      "9 train 12 156\n",
      "Phase: train. Epoch: 9. Loss: 0.13541632890701294\n",
      "9 train 13 168\n",
      "Phase: train. Epoch: 9. Loss: 0.09868123382329941\n",
      "9 train 14 180\n",
      "Phase: train. Epoch: 9. Loss: 0.10476836562156677\n",
      "9 train 15 192\n",
      "Phase: train. Epoch: 9. Loss: 0.11055248975753784\n",
      "9 train 16 204\n",
      "Phase: train. Epoch: 9. Loss: 0.10315253585577011\n",
      "9 train 17 216\n",
      "Phase: train. Epoch: 9. Loss: 0.08659525960683823\n",
      "9 train 18 228\n",
      "Phase: train. Epoch: 9. Loss: 0.09714920818805695\n",
      "9 train 19 240\n",
      "Phase: train. Epoch: 9. Loss: 0.10792618244886398\n",
      "9 train 20 252\n",
      "Phase: train. Epoch: 9. Loss: 0.10400614142417908\n",
      "9 train 21 264\n",
      "Phase: train. Epoch: 9. Loss: 0.0856688842177391\n",
      "9 train 22 276\n",
      "Phase: train. Epoch: 9. Loss: 0.11915843188762665\n",
      "9 train 23 288\n",
      "Phase: train. Epoch: 9. Loss: 0.09561051428318024\n",
      "9 train 24 300\n",
      "Phase: train. Epoch: 9. Loss: 0.10950265824794769\n",
      "9 train 25 312\n",
      "Phase: train. Epoch: 9. Loss: 0.09832360595464706\n",
      "9 train 26 324\n",
      "Phase: train. Epoch: 9. Loss: 0.12179087847471237\n",
      "9 train 27 336\n",
      "Phase: train. Epoch: 9. Loss: 0.10232089459896088\n",
      "9 train 28 348\n",
      "Phase: train. Epoch: 9. Loss: 0.09969858825206757\n",
      "9 train 29 360\n",
      "Phase: train. Epoch: 9. Loss: 0.10881509631872177\n",
      "9 train 30 372\n",
      "Phase: train. Epoch: 9. Loss: 0.10006815195083618\n",
      "9 train 31 384\n",
      "Phase: train. Epoch: 9. Loss: 0.10477417707443237\n",
      "9 train 32 396\n",
      "Phase: train. Epoch: 9. Loss: 0.09109678119421005\n",
      "9 train 33 408\n",
      "Phase: train. Epoch: 9. Loss: 0.11754697561264038\n",
      "9 train 34 420\n",
      "Phase: train. Epoch: 9. Loss: 0.11086533963680267\n",
      "9 train 35 432\n",
      "Phase: train. Epoch: 9. Loss: 0.13362078368663788\n",
      "9 train 36 444\n",
      "Phase: train. Epoch: 9. Loss: 0.0825696736574173\n",
      "9 train 37 456\n",
      "Phase: train. Epoch: 9. Loss: 0.12917020916938782\n",
      "9 train 38 468\n",
      "Phase: train. Epoch: 9. Loss: 0.11651639640331268\n",
      "9 train 39 480\n",
      "Phase: train. Epoch: 9. Loss: 0.09676362574100494\n",
      "9 train 40 492\n",
      "Phase: train. Epoch: 9. Loss: 0.1110997200012207\n",
      "9 train 41 504\n",
      "Phase: train. Epoch: 9. Loss: 0.10553677380084991\n",
      "9 train 42 516\n",
      "Phase: train. Epoch: 9. Loss: 0.09854477643966675\n",
      "9 train 43 528\n",
      "Phase: train. Epoch: 9. Loss: 0.09874065965414047\n",
      "9 train 44 540\n",
      "Phase: train. Epoch: 9. Loss: 0.09800039231777191\n",
      "9 train 45 552\n",
      "Phase: train. Epoch: 9. Loss: 0.1172662228345871\n",
      "9 train 46 564\n",
      "Phase: train. Epoch: 9. Loss: 0.10971816629171371\n",
      "9 train 47 576\n",
      "Phase: train. Epoch: 9. Loss: 0.09709622710943222\n",
      "9 train 48 588\n",
      "Phase: train. Epoch: 9. Loss: 0.12052744626998901\n",
      "9 train 49 600\n",
      "Phase: train. Epoch: 9. Loss: 0.09902986139059067\n",
      "9 train 50 612\n",
      "Phase: train. Epoch: 9. Loss: 0.11261989921331406\n",
      "9 train 51 624\n",
      "Phase: train. Epoch: 9. Loss: 0.09137149155139923\n",
      "9 train 52 636\n",
      "Phase: train. Epoch: 9. Loss: 0.10167902708053589\n",
      "9 train 53 648\n",
      "Phase: train. Epoch: 9. Loss: 0.09754008054733276\n",
      "9 train 54 660\n",
      "Phase: train. Epoch: 9. Loss: 0.11077539622783661\n",
      "9 train 55 672\n",
      "Phase: train. Epoch: 9. Loss: 0.10618239641189575\n",
      "9 train 56 684\n",
      "Phase: train. Epoch: 9. Loss: 0.11271234601736069\n",
      "9 train 57 696\n",
      "Phase: train. Epoch: 9. Loss: 0.11976154893636703\n",
      "9 train 58 708\n",
      "Phase: train. Epoch: 9. Loss: 0.12348674237728119\n",
      "9 train 59 720\n",
      "Phase: train. Epoch: 9. Loss: 0.0936679095029831\n",
      "9 train 60 732\n",
      "Phase: train. Epoch: 9. Loss: 0.11785483360290527\n",
      "9 train 61 744\n",
      "Phase: train. Epoch: 9. Loss: 0.08916723728179932\n",
      "9 train 62 751\n",
      "Phase: train. Epoch: 9. Loss: 0.09655048698186874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 val 0 763\n",
      "Phase: val. Epoch: 9. Loss: 0.10014830529689789\n",
      "9 val 1 775\n",
      "Phase: val. Epoch: 9. Loss: 0.07176493108272552\n",
      "9 val 2 787\n",
      "Phase: val. Epoch: 9. Loss: 0.12449322640895844\n",
      "9 val 3 799\n",
      "Phase: val. Epoch: 9. Loss: 0.07680509984493256\n",
      "9 val 4 811\n",
      "Phase: val. Epoch: 9. Loss: 0.1266936957836151\n",
      "9 val 5 823\n",
      "Phase: val. Epoch: 9. Loss: 0.10372291505336761\n",
      "9 val 6 835\n",
      "Phase: val. Epoch: 9. Loss: 0.07634469121694565\n",
      "9 val 7 847\n",
      "Phase: val. Epoch: 9. Loss: 0.11361075937747955\n",
      "9 val 8 859\n",
      "Phase: val. Epoch: 9. Loss: 0.08639061450958252\n",
      "9 val 9 871\n",
      "Phase: val. Epoch: 9. Loss: 0.09798087924718857\n",
      "9 val 10 883\n",
      "Phase: val. Epoch: 9. Loss: 0.11477991193532944\n",
      "9 val 11 884\n",
      "Phase: val. Epoch: 9. Loss: 0.13751715421676636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 train 0 12\n",
      "Phase: train. Epoch: 10. Loss: 0.1316915899515152\n",
      "10 train 1 24\n",
      "Phase: train. Epoch: 10. Loss: 0.10260312259197235\n",
      "10 train 2 36\n",
      "Phase: train. Epoch: 10. Loss: 0.10239436477422714\n",
      "10 train 3 48\n",
      "Phase: train. Epoch: 10. Loss: 0.10591434687376022\n",
      "10 train 4 60\n",
      "Phase: train. Epoch: 10. Loss: 0.11115818470716476\n",
      "10 train 5 72\n",
      "Phase: train. Epoch: 10. Loss: 0.11882251501083374\n",
      "10 train 6 84\n",
      "Phase: train. Epoch: 10. Loss: 0.09224997460842133\n",
      "10 train 7 96\n",
      "Phase: train. Epoch: 10. Loss: 0.11081463098526001\n",
      "10 train 8 108\n",
      "Phase: train. Epoch: 10. Loss: 0.11729557812213898\n",
      "10 train 9 120\n",
      "Phase: train. Epoch: 10. Loss: 0.0894065722823143\n",
      "10 train 10 132\n",
      "Phase: train. Epoch: 10. Loss: 0.10751836001873016\n",
      "10 train 11 144\n",
      "Phase: train. Epoch: 10. Loss: 0.09375091642141342\n",
      "10 train 12 156\n",
      "Phase: train. Epoch: 10. Loss: 0.09722772240638733\n",
      "10 train 13 168\n",
      "Phase: train. Epoch: 10. Loss: 0.10392485558986664\n",
      "10 train 14 180\n",
      "Phase: train. Epoch: 10. Loss: 0.0931297093629837\n",
      "10 train 15 192\n",
      "Phase: train. Epoch: 10. Loss: 0.10468807071447372\n",
      "10 train 16 204\n",
      "Phase: train. Epoch: 10. Loss: 0.11161927878856659\n",
      "10 train 17 216\n",
      "Phase: train. Epoch: 10. Loss: 0.10933058708906174\n",
      "10 train 18 228\n",
      "Phase: train. Epoch: 10. Loss: 0.1085330992937088\n",
      "10 train 19 240\n",
      "Phase: train. Epoch: 10. Loss: 0.08773386478424072\n",
      "10 train 20 252\n",
      "Phase: train. Epoch: 10. Loss: 0.11724179983139038\n",
      "10 train 21 264\n",
      "Phase: train. Epoch: 10. Loss: 0.11079996824264526\n",
      "10 train 22 276\n",
      "Phase: train. Epoch: 10. Loss: 0.10275720059871674\n",
      "10 train 23 288\n",
      "Phase: train. Epoch: 10. Loss: 0.11057951301336288\n",
      "10 train 24 300\n",
      "Phase: train. Epoch: 10. Loss: 0.09466485679149628\n",
      "10 train 25 312\n",
      "Phase: train. Epoch: 10. Loss: 0.08334066718816757\n",
      "10 train 26 324\n",
      "Phase: train. Epoch: 10. Loss: 0.08594951033592224\n",
      "10 train 27 336\n",
      "Phase: train. Epoch: 10. Loss: 0.09458304941654205\n",
      "10 train 28 348\n",
      "Phase: train. Epoch: 10. Loss: 0.10440047830343246\n",
      "10 train 29 360\n",
      "Phase: train. Epoch: 10. Loss: 0.13626793026924133\n",
      "10 train 30 372\n",
      "Phase: train. Epoch: 10. Loss: 0.10380380600690842\n",
      "10 train 31 384\n",
      "Phase: train. Epoch: 10. Loss: 0.1166919693350792\n",
      "10 train 32 396\n",
      "Phase: train. Epoch: 10. Loss: 0.10675334930419922\n",
      "10 train 33 408\n",
      "Phase: train. Epoch: 10. Loss: 0.08691086620092392\n",
      "10 train 34 420\n",
      "Phase: train. Epoch: 10. Loss: 0.08557214587926865\n",
      "10 train 35 432\n",
      "Phase: train. Epoch: 10. Loss: 0.10960881412029266\n",
      "10 train 36 444\n",
      "Phase: train. Epoch: 10. Loss: 0.108848437666893\n",
      "10 train 37 456\n",
      "Phase: train. Epoch: 10. Loss: 0.1338188499212265\n",
      "10 train 38 468\n",
      "Phase: train. Epoch: 10. Loss: 0.09359164535999298\n",
      "10 train 39 480\n",
      "Phase: train. Epoch: 10. Loss: 0.10692005604505539\n",
      "10 train 40 492\n",
      "Phase: train. Epoch: 10. Loss: 0.08262696117162704\n",
      "10 train 41 504\n",
      "Phase: train. Epoch: 10. Loss: 0.10375320911407471\n",
      "10 train 42 516\n",
      "Phase: train. Epoch: 10. Loss: 0.1457894891500473\n",
      "10 train 43 528\n",
      "Phase: train. Epoch: 10. Loss: 0.09352700412273407\n",
      "10 train 44 540\n",
      "Phase: train. Epoch: 10. Loss: 0.09912851452827454\n",
      "10 train 45 552\n",
      "Phase: train. Epoch: 10. Loss: 0.12018094956874847\n",
      "10 train 46 564\n",
      "Phase: train. Epoch: 10. Loss: 0.10393308103084564\n",
      "10 train 47 576\n",
      "Phase: train. Epoch: 10. Loss: 0.07843217253684998\n",
      "10 train 48 588\n",
      "Phase: train. Epoch: 10. Loss: 0.10847894847393036\n",
      "10 train 49 600\n",
      "Phase: train. Epoch: 10. Loss: 0.10154151916503906\n",
      "10 train 50 612\n",
      "Phase: train. Epoch: 10. Loss: 0.09145250171422958\n",
      "10 train 51 624\n",
      "Phase: train. Epoch: 10. Loss: 0.10565185546875\n",
      "10 train 52 636\n",
      "Phase: train. Epoch: 10. Loss: 0.10396718978881836\n",
      "10 train 53 648\n",
      "Phase: train. Epoch: 10. Loss: 0.09761849045753479\n",
      "10 train 54 660\n",
      "Phase: train. Epoch: 10. Loss: 0.10373371839523315\n",
      "10 train 55 672\n",
      "Phase: train. Epoch: 10. Loss: 0.07919739186763763\n",
      "10 train 56 684\n",
      "Phase: train. Epoch: 10. Loss: 0.09786020964384079\n",
      "10 train 57 696\n",
      "Phase: train. Epoch: 10. Loss: 0.07939194142818451\n",
      "10 train 58 708\n",
      "Phase: train. Epoch: 10. Loss: 0.0871124267578125\n",
      "10 train 59 720\n",
      "Phase: train. Epoch: 10. Loss: 0.1268591731786728\n",
      "10 train 60 732\n",
      "Phase: train. Epoch: 10. Loss: 0.09765017032623291\n",
      "10 train 61 744\n",
      "Phase: train. Epoch: 10. Loss: 0.08105210959911346\n",
      "10 train 62 751\n",
      "Phase: train. Epoch: 10. Loss: 0.08756740391254425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 val 0 763\n",
      "Phase: val. Epoch: 10. Loss: 0.13111625611782074\n",
      "10 val 1 775\n",
      "Phase: val. Epoch: 10. Loss: 0.07457078993320465\n",
      "10 val 2 787\n",
      "Phase: val. Epoch: 10. Loss: 0.07522641122341156\n",
      "10 val 3 799\n",
      "Phase: val. Epoch: 10. Loss: 0.11279749870300293\n",
      "10 val 4 811\n",
      "Phase: val. Epoch: 10. Loss: 0.06387239694595337\n",
      "10 val 5 823\n",
      "Phase: val. Epoch: 10. Loss: 0.10049234330654144\n",
      "10 val 6 835\n",
      "Phase: val. Epoch: 10. Loss: 0.09435378015041351\n",
      "10 val 7 847\n",
      "Phase: val. Epoch: 10. Loss: 0.09026569128036499\n",
      "10 val 8 859\n",
      "Phase: val. Epoch: 10. Loss: 0.11250852048397064\n",
      "10 val 9 871\n",
      "Phase: val. Epoch: 10. Loss: 0.11872845888137817\n",
      "10 val 10 883\n",
      "Phase: val. Epoch: 10. Loss: 0.057757046073675156\n",
      "10 val 11 884\n",
      "Phase: val. Epoch: 10. Loss: 0.2697215676307678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 train 0 12\n",
      "Phase: train. Epoch: 11. Loss: 0.13171164691448212\n",
      "11 train 1 24\n",
      "Phase: train. Epoch: 11. Loss: 0.08079864084720612\n",
      "11 train 2 36\n",
      "Phase: train. Epoch: 11. Loss: 0.08230356872081757\n",
      "11 train 3 48\n",
      "Phase: train. Epoch: 11. Loss: 0.0806719958782196\n",
      "11 train 4 60\n",
      "Phase: train. Epoch: 11. Loss: 0.11665521562099457\n",
      "11 train 5 72\n",
      "Phase: train. Epoch: 11. Loss: 0.11719109863042831\n",
      "11 train 6 84\n",
      "Phase: train. Epoch: 11. Loss: 0.09604867547750473\n",
      "11 train 7 96\n",
      "Phase: train. Epoch: 11. Loss: 0.09918894618749619\n",
      "11 train 8 108\n",
      "Phase: train. Epoch: 11. Loss: 0.12799906730651855\n",
      "11 train 9 120\n",
      "Phase: train. Epoch: 11. Loss: 0.11488813161849976\n",
      "11 train 10 132\n",
      "Phase: train. Epoch: 11. Loss: 0.13314402103424072\n",
      "11 train 11 144\n",
      "Phase: train. Epoch: 11. Loss: 0.0777859091758728\n",
      "11 train 12 156\n",
      "Phase: train. Epoch: 11. Loss: 0.1073281466960907\n",
      "11 train 13 168\n",
      "Phase: train. Epoch: 11. Loss: 0.1192062720656395\n",
      "11 train 14 180\n",
      "Phase: train. Epoch: 11. Loss: 0.11001943051815033\n",
      "11 train 15 192\n",
      "Phase: train. Epoch: 11. Loss: 0.08407707512378693\n",
      "11 train 16 204\n",
      "Phase: train. Epoch: 11. Loss: 0.10187627375125885\n",
      "11 train 17 216\n",
      "Phase: train. Epoch: 11. Loss: 0.08882741630077362\n",
      "11 train 18 228\n",
      "Phase: train. Epoch: 11. Loss: 0.11354385316371918\n",
      "11 train 19 240\n",
      "Phase: train. Epoch: 11. Loss: 0.08787135779857635\n",
      "11 train 20 252\n",
      "Phase: train. Epoch: 11. Loss: 0.12417200207710266\n",
      "11 train 21 264\n",
      "Phase: train. Epoch: 11. Loss: 0.12070296704769135\n",
      "11 train 22 276\n",
      "Phase: train. Epoch: 11. Loss: 0.07018352299928665\n",
      "11 train 23 288\n",
      "Phase: train. Epoch: 11. Loss: 0.1125536784529686\n",
      "11 train 24 300\n",
      "Phase: train. Epoch: 11. Loss: 0.0827193409204483\n",
      "11 train 25 312\n",
      "Phase: train. Epoch: 11. Loss: 0.08717779815196991\n",
      "11 train 26 324\n",
      "Phase: train. Epoch: 11. Loss: 0.13256613910198212\n",
      "11 train 27 336\n",
      "Phase: train. Epoch: 11. Loss: 0.1038503646850586\n",
      "11 train 28 348\n",
      "Phase: train. Epoch: 11. Loss: 0.08140084147453308\n",
      "11 train 29 360\n",
      "Phase: train. Epoch: 11. Loss: 0.07710547745227814\n",
      "11 train 30 372\n",
      "Phase: train. Epoch: 11. Loss: 0.07393785566091537\n",
      "11 train 31 384\n",
      "Phase: train. Epoch: 11. Loss: 0.10010986030101776\n",
      "11 train 32 396\n",
      "Phase: train. Epoch: 11. Loss: 0.1199224442243576\n",
      "11 train 33 408\n",
      "Phase: train. Epoch: 11. Loss: 0.07109804451465607\n",
      "11 train 34 420\n",
      "Phase: train. Epoch: 11. Loss: 0.07595090568065643\n",
      "11 train 35 432\n",
      "Phase: train. Epoch: 11. Loss: 0.08545094728469849\n",
      "11 train 36 444\n",
      "Phase: train. Epoch: 11. Loss: 0.08161729574203491\n",
      "11 train 37 456\n",
      "Phase: train. Epoch: 11. Loss: 0.09571096301078796\n",
      "11 train 38 468\n",
      "Phase: train. Epoch: 11. Loss: 0.0743158757686615\n",
      "11 train 39 480\n",
      "Phase: train. Epoch: 11. Loss: 0.06937727332115173\n",
      "11 train 40 492\n",
      "Phase: train. Epoch: 11. Loss: 0.08638153970241547\n",
      "11 train 41 504\n",
      "Phase: train. Epoch: 11. Loss: 0.09967093169689178\n",
      "11 train 42 516\n",
      "Phase: train. Epoch: 11. Loss: 0.08600567281246185\n",
      "11 train 43 528\n",
      "Phase: train. Epoch: 11. Loss: 0.1154046505689621\n",
      "11 train 44 540\n",
      "Phase: train. Epoch: 11. Loss: 0.08276765048503876\n",
      "11 train 45 552\n",
      "Phase: train. Epoch: 11. Loss: 0.1201709732413292\n",
      "11 train 46 564\n",
      "Phase: train. Epoch: 11. Loss: 0.15341053903102875\n",
      "11 train 47 576\n",
      "Phase: train. Epoch: 11. Loss: 0.09002485126256943\n",
      "11 train 48 588\n",
      "Phase: train. Epoch: 11. Loss: 0.08867844194173813\n",
      "11 train 49 600\n",
      "Phase: train. Epoch: 11. Loss: 0.1335773915052414\n",
      "11 train 50 612\n",
      "Phase: train. Epoch: 11. Loss: 0.08090968430042267\n",
      "11 train 51 624\n",
      "Phase: train. Epoch: 11. Loss: 0.10792253911495209\n",
      "11 train 52 636\n",
      "Phase: train. Epoch: 11. Loss: 0.11354605853557587\n",
      "11 train 53 648\n",
      "Phase: train. Epoch: 11. Loss: 0.09551939368247986\n",
      "11 train 54 660\n",
      "Phase: train. Epoch: 11. Loss: 0.10410591214895248\n",
      "11 train 55 672\n",
      "Phase: train. Epoch: 11. Loss: 0.11988602578639984\n",
      "11 train 56 684\n",
      "Phase: train. Epoch: 11. Loss: 0.10236424207687378\n",
      "11 train 57 696\n",
      "Phase: train. Epoch: 11. Loss: 0.10943438112735748\n",
      "11 train 58 708\n",
      "Phase: train. Epoch: 11. Loss: 0.08104883134365082\n",
      "11 train 59 720\n",
      "Phase: train. Epoch: 11. Loss: 0.07976096123456955\n",
      "11 train 60 732\n",
      "Phase: train. Epoch: 11. Loss: 0.0993133932352066\n",
      "11 train 61 744\n",
      "Phase: train. Epoch: 11. Loss: 0.09057873487472534\n",
      "11 train 62 751\n",
      "Phase: train. Epoch: 11. Loss: 0.07868298143148422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 val 0 763\n",
      "Phase: val. Epoch: 11. Loss: 0.09554492682218552\n",
      "11 val 1 775\n",
      "Phase: val. Epoch: 11. Loss: 0.09312300384044647\n",
      "11 val 2 787\n",
      "Phase: val. Epoch: 11. Loss: 0.09520123898983002\n",
      "11 val 3 799\n",
      "Phase: val. Epoch: 11. Loss: 0.11489099264144897\n",
      "11 val 4 811\n",
      "Phase: val. Epoch: 11. Loss: 0.07798007130622864\n",
      "11 val 5 823\n",
      "Phase: val. Epoch: 11. Loss: 0.08171406388282776\n",
      "11 val 6 835\n",
      "Phase: val. Epoch: 11. Loss: 0.11134862899780273\n",
      "11 val 7 847\n",
      "Phase: val. Epoch: 11. Loss: 0.08010371029376984\n",
      "11 val 8 859\n",
      "Phase: val. Epoch: 11. Loss: 0.08869791030883789\n",
      "11 val 9 871\n",
      "Phase: val. Epoch: 11. Loss: 0.07971176505088806\n",
      "11 val 10 883\n",
      "Phase: val. Epoch: 11. Loss: 0.10297900438308716\n",
      "11 val 11 884\n",
      "Phase: val. Epoch: 11. Loss: 0.11916275322437286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 train 0 12\n",
      "Phase: train. Epoch: 12. Loss: 0.1397317498922348\n",
      "12 train 1 24\n",
      "Phase: train. Epoch: 12. Loss: 0.10300727933645248\n",
      "12 train 2 36\n",
      "Phase: train. Epoch: 12. Loss: 0.1034805178642273\n",
      "12 train 3 48\n",
      "Phase: train. Epoch: 12. Loss: 0.07498350739479065\n",
      "12 train 4 60\n",
      "Phase: train. Epoch: 12. Loss: 0.08766575902700424\n",
      "12 train 5 72\n",
      "Phase: train. Epoch: 12. Loss: 0.07507100701332092\n",
      "12 train 6 84\n",
      "Phase: train. Epoch: 12. Loss: 0.10199593007564545\n",
      "12 train 7 96\n",
      "Phase: train. Epoch: 12. Loss: 0.11064419150352478\n",
      "12 train 8 108\n",
      "Phase: train. Epoch: 12. Loss: 0.10124939680099487\n",
      "12 train 9 120\n",
      "Phase: train. Epoch: 12. Loss: 0.08313454687595367\n",
      "12 train 10 132\n",
      "Phase: train. Epoch: 12. Loss: 0.10658776760101318\n",
      "12 train 11 144\n",
      "Phase: train. Epoch: 12. Loss: 0.08840641379356384\n",
      "12 train 12 156\n",
      "Phase: train. Epoch: 12. Loss: 0.09750016778707504\n",
      "12 train 13 168\n",
      "Phase: train. Epoch: 12. Loss: 0.07474441826343536\n",
      "12 train 14 180\n",
      "Phase: train. Epoch: 12. Loss: 0.09847639501094818\n",
      "12 train 15 192\n",
      "Phase: train. Epoch: 12. Loss: 0.10479836165904999\n",
      "12 train 16 204\n",
      "Phase: train. Epoch: 12. Loss: 0.10418667644262314\n",
      "12 train 17 216\n",
      "Phase: train. Epoch: 12. Loss: 0.08817380666732788\n",
      "12 train 18 228\n",
      "Phase: train. Epoch: 12. Loss: 0.09528760612010956\n",
      "12 train 19 240\n",
      "Phase: train. Epoch: 12. Loss: 0.10533498227596283\n",
      "12 train 20 252\n",
      "Phase: train. Epoch: 12. Loss: 0.09548018872737885\n",
      "12 train 21 264\n",
      "Phase: train. Epoch: 12. Loss: 0.08698093146085739\n",
      "12 train 22 276\n",
      "Phase: train. Epoch: 12. Loss: 0.10910108685493469\n",
      "12 train 23 288\n",
      "Phase: train. Epoch: 12. Loss: 0.0990762710571289\n",
      "12 train 24 300\n",
      "Phase: train. Epoch: 12. Loss: 0.08274140954017639\n",
      "12 train 25 312\n",
      "Phase: train. Epoch: 12. Loss: 0.08841469138860703\n",
      "12 train 26 324\n",
      "Phase: train. Epoch: 12. Loss: 0.1334930956363678\n",
      "12 train 27 336\n",
      "Phase: train. Epoch: 12. Loss: 0.07932783663272858\n",
      "12 train 28 348\n",
      "Phase: train. Epoch: 12. Loss: 0.09682642668485641\n",
      "12 train 29 360\n",
      "Phase: train. Epoch: 12. Loss: 0.08959601819515228\n",
      "12 train 30 372\n",
      "Phase: train. Epoch: 12. Loss: 0.08880166709423065\n",
      "12 train 31 384\n",
      "Phase: train. Epoch: 12. Loss: 0.1074150949716568\n",
      "12 train 32 396\n",
      "Phase: train. Epoch: 12. Loss: 0.09682273119688034\n",
      "12 train 33 408\n",
      "Phase: train. Epoch: 12. Loss: 0.09119679778814316\n",
      "12 train 34 420\n",
      "Phase: train. Epoch: 12. Loss: 0.10277809202671051\n",
      "12 train 35 432\n",
      "Phase: train. Epoch: 12. Loss: 0.11204823106527328\n",
      "12 train 36 444\n",
      "Phase: train. Epoch: 12. Loss: 0.11064492166042328\n",
      "12 train 37 456\n",
      "Phase: train. Epoch: 12. Loss: 0.08159986883401871\n",
      "12 train 38 468\n",
      "Phase: train. Epoch: 12. Loss: 0.1145702451467514\n",
      "12 train 39 480\n",
      "Phase: train. Epoch: 12. Loss: 0.08647489547729492\n",
      "12 train 40 492\n",
      "Phase: train. Epoch: 12. Loss: 0.1123712882399559\n",
      "12 train 41 504\n",
      "Phase: train. Epoch: 12. Loss: 0.09605809301137924\n",
      "12 train 42 516\n",
      "Phase: train. Epoch: 12. Loss: 0.10442819446325302\n",
      "12 train 43 528\n",
      "Phase: train. Epoch: 12. Loss: 0.09317675232887268\n",
      "12 train 44 540\n",
      "Phase: train. Epoch: 12. Loss: 0.09890873730182648\n",
      "12 train 45 552\n",
      "Phase: train. Epoch: 12. Loss: 0.12739013135433197\n",
      "12 train 46 564\n",
      "Phase: train. Epoch: 12. Loss: 0.09018783271312714\n",
      "12 train 47 576\n",
      "Phase: train. Epoch: 12. Loss: 0.08156036585569382\n",
      "12 train 48 588\n",
      "Phase: train. Epoch: 12. Loss: 0.07922592014074326\n",
      "12 train 49 600\n",
      "Phase: train. Epoch: 12. Loss: 0.09300795197486877\n",
      "12 train 50 612\n",
      "Phase: train. Epoch: 12. Loss: 0.09590266644954681\n",
      "12 train 51 624\n",
      "Phase: train. Epoch: 12. Loss: 0.0926467627286911\n",
      "12 train 52 636\n",
      "Phase: train. Epoch: 12. Loss: 0.10148316621780396\n",
      "12 train 53 648\n",
      "Phase: train. Epoch: 12. Loss: 0.10130219161510468\n",
      "12 train 54 660\n",
      "Phase: train. Epoch: 12. Loss: 0.10825514793395996\n",
      "12 train 55 672\n",
      "Phase: train. Epoch: 12. Loss: 0.0947677493095398\n",
      "12 train 56 684\n",
      "Phase: train. Epoch: 12. Loss: 0.07800252735614777\n",
      "12 train 57 696\n",
      "Phase: train. Epoch: 12. Loss: 0.10542496293783188\n",
      "12 train 58 708\n",
      "Phase: train. Epoch: 12. Loss: 0.09619975090026855\n",
      "12 train 59 720\n",
      "Phase: train. Epoch: 12. Loss: 0.08123983442783356\n",
      "12 train 60 732\n",
      "Phase: train. Epoch: 12. Loss: 0.08186174929141998\n",
      "12 train 61 744\n",
      "Phase: train. Epoch: 12. Loss: 0.10297292470932007\n",
      "12 train 62 751\n",
      "Phase: train. Epoch: 12. Loss: 0.06602779030799866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 val 0 763\n",
      "Phase: val. Epoch: 12. Loss: 0.10395016521215439\n",
      "12 val 1 775\n",
      "Phase: val. Epoch: 12. Loss: 0.10383429378271103\n",
      "12 val 2 787\n",
      "Phase: val. Epoch: 12. Loss: 0.09546753764152527\n",
      "12 val 3 799\n",
      "Phase: val. Epoch: 12. Loss: 0.08111999928951263\n",
      "12 val 4 811\n",
      "Phase: val. Epoch: 12. Loss: 0.07493177801370621\n",
      "12 val 5 823\n",
      "Phase: val. Epoch: 12. Loss: 0.09164528548717499\n",
      "12 val 6 835\n",
      "Phase: val. Epoch: 12. Loss: 0.09959718585014343\n",
      "12 val 7 847\n",
      "Phase: val. Epoch: 12. Loss: 0.08720290660858154\n",
      "12 val 8 859\n",
      "Phase: val. Epoch: 12. Loss: 0.088990718126297\n",
      "12 val 9 871\n",
      "Phase: val. Epoch: 12. Loss: 0.09521937370300293\n",
      "12 val 10 883\n",
      "Phase: val. Epoch: 12. Loss: 0.08699142187833786\n",
      "12 val 11 884\n",
      "Phase: val. Epoch: 12. Loss: 0.03526727855205536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 train 0 12\n",
      "Phase: train. Epoch: 13. Loss: 0.0823633223772049\n",
      "13 train 1 24\n",
      "Phase: train. Epoch: 13. Loss: 0.09380356967449188\n",
      "13 train 2 36\n",
      "Phase: train. Epoch: 13. Loss: 0.08305105566978455\n",
      "13 train 3 48\n",
      "Phase: train. Epoch: 13. Loss: 0.0895942971110344\n",
      "13 train 4 60\n",
      "Phase: train. Epoch: 13. Loss: 0.08204633742570877\n",
      "13 train 5 72\n",
      "Phase: train. Epoch: 13. Loss: 0.0675709992647171\n",
      "13 train 6 84\n",
      "Phase: train. Epoch: 13. Loss: 0.07723800837993622\n",
      "13 train 7 96\n",
      "Phase: train. Epoch: 13. Loss: 0.06616773456335068\n",
      "13 train 8 108\n",
      "Phase: train. Epoch: 13. Loss: 0.09309116005897522\n",
      "13 train 9 120\n",
      "Phase: train. Epoch: 13. Loss: 0.09381835907697678\n",
      "13 train 10 132\n",
      "Phase: train. Epoch: 13. Loss: 0.09591497480869293\n",
      "13 train 11 144\n",
      "Phase: train. Epoch: 13. Loss: 0.07947815954685211\n",
      "13 train 12 156\n",
      "Phase: train. Epoch: 13. Loss: 0.10410481691360474\n",
      "13 train 13 168\n",
      "Phase: train. Epoch: 13. Loss: 0.10053297132253647\n",
      "13 train 14 180\n",
      "Phase: train. Epoch: 13. Loss: 0.09000419080257416\n",
      "13 train 15 192\n",
      "Phase: train. Epoch: 13. Loss: 0.11851255595684052\n",
      "13 train 16 204\n",
      "Phase: train. Epoch: 13. Loss: 0.10366760194301605\n",
      "13 train 17 216\n",
      "Phase: train. Epoch: 13. Loss: 0.10867588222026825\n",
      "13 train 18 228\n",
      "Phase: train. Epoch: 13. Loss: 0.12345036119222641\n",
      "13 train 19 240\n",
      "Phase: train. Epoch: 13. Loss: 0.08151227235794067\n",
      "13 train 20 252\n",
      "Phase: train. Epoch: 13. Loss: 0.09912975877523422\n",
      "13 train 21 264\n",
      "Phase: train. Epoch: 13. Loss: 0.12072061747312546\n",
      "13 train 22 276\n",
      "Phase: train. Epoch: 13. Loss: 0.08314196765422821\n",
      "13 train 23 288\n",
      "Phase: train. Epoch: 13. Loss: 0.10970041155815125\n",
      "13 train 24 300\n",
      "Phase: train. Epoch: 13. Loss: 0.12410050630569458\n",
      "13 train 25 312\n",
      "Phase: train. Epoch: 13. Loss: 0.10626276582479477\n",
      "13 train 26 324\n",
      "Phase: train. Epoch: 13. Loss: 0.1069752499461174\n",
      "13 train 27 336\n",
      "Phase: train. Epoch: 13. Loss: 0.08183151483535767\n",
      "13 train 28 348\n",
      "Phase: train. Epoch: 13. Loss: 0.08444526791572571\n",
      "13 train 29 360\n",
      "Phase: train. Epoch: 13. Loss: 0.10641215741634369\n",
      "13 train 30 372\n",
      "Phase: train. Epoch: 13. Loss: 0.109848253428936\n",
      "13 train 31 384\n",
      "Phase: train. Epoch: 13. Loss: 0.1026567816734314\n",
      "13 train 32 396\n",
      "Phase: train. Epoch: 13. Loss: 0.13074010610580444\n",
      "13 train 33 408\n",
      "Phase: train. Epoch: 13. Loss: 0.11104826629161835\n",
      "13 train 34 420\n",
      "Phase: train. Epoch: 13. Loss: 0.08514540642499924\n",
      "13 train 35 432\n",
      "Phase: train. Epoch: 13. Loss: 0.10449621081352234\n",
      "13 train 36 444\n",
      "Phase: train. Epoch: 13. Loss: 0.07989969104528427\n",
      "13 train 37 456\n",
      "Phase: train. Epoch: 13. Loss: 0.10166898369789124\n",
      "13 train 38 468\n",
      "Phase: train. Epoch: 13. Loss: 0.08716770261526108\n",
      "13 train 39 480\n",
      "Phase: train. Epoch: 13. Loss: 0.1139921247959137\n",
      "13 train 40 492\n",
      "Phase: train. Epoch: 13. Loss: 0.08524968475103378\n",
      "13 train 41 504\n",
      "Phase: train. Epoch: 13. Loss: 0.09733785688877106\n",
      "13 train 42 516\n",
      "Phase: train. Epoch: 13. Loss: 0.11464488506317139\n",
      "13 train 43 528\n",
      "Phase: train. Epoch: 13. Loss: 0.09534735977649689\n",
      "13 train 44 540\n",
      "Phase: train. Epoch: 13. Loss: 0.08144187182188034\n",
      "13 train 45 552\n",
      "Phase: train. Epoch: 13. Loss: 0.08945547044277191\n",
      "13 train 46 564\n",
      "Phase: train. Epoch: 13. Loss: 0.09023791551589966\n",
      "13 train 47 576\n",
      "Phase: train. Epoch: 13. Loss: 0.10953738540410995\n",
      "13 train 48 588\n",
      "Phase: train. Epoch: 13. Loss: 0.10379727929830551\n",
      "13 train 49 600\n",
      "Phase: train. Epoch: 13. Loss: 0.08335854858160019\n",
      "13 train 50 612\n",
      "Phase: train. Epoch: 13. Loss: 0.08969040960073471\n",
      "13 train 51 624\n",
      "Phase: train. Epoch: 13. Loss: 0.09248702228069305\n",
      "13 train 52 636\n",
      "Phase: train. Epoch: 13. Loss: 0.10559600591659546\n",
      "13 train 53 648\n",
      "Phase: train. Epoch: 13. Loss: 0.11349436640739441\n",
      "13 train 54 660\n",
      "Phase: train. Epoch: 13. Loss: 0.08493775129318237\n",
      "13 train 55 672\n",
      "Phase: train. Epoch: 13. Loss: 0.10714602470397949\n",
      "13 train 56 684\n",
      "Phase: train. Epoch: 13. Loss: 0.11266063153743744\n",
      "13 train 57 696\n",
      "Phase: train. Epoch: 13. Loss: 0.1083674430847168\n",
      "13 train 58 708\n",
      "Phase: train. Epoch: 13. Loss: 0.09196551889181137\n",
      "13 train 59 720\n",
      "Phase: train. Epoch: 13. Loss: 0.07745110988616943\n",
      "13 train 60 732\n",
      "Phase: train. Epoch: 13. Loss: 0.09854497015476227\n",
      "13 train 61 744\n",
      "Phase: train. Epoch: 13. Loss: 0.12718847393989563\n",
      "13 train 62 751\n",
      "Phase: train. Epoch: 13. Loss: 0.1044856533408165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 val 0 763\n",
      "Phase: val. Epoch: 13. Loss: 0.0830957293510437\n",
      "13 val 1 775\n",
      "Phase: val. Epoch: 13. Loss: 0.116246297955513\n",
      "13 val 2 787\n",
      "Phase: val. Epoch: 13. Loss: 0.09281828254461288\n",
      "13 val 3 799\n",
      "Phase: val. Epoch: 13. Loss: 0.09346304833889008\n",
      "13 val 4 811\n",
      "Phase: val. Epoch: 13. Loss: 0.06943047046661377\n",
      "13 val 5 823\n",
      "Phase: val. Epoch: 13. Loss: 0.08435468375682831\n",
      "13 val 6 835\n",
      "Phase: val. Epoch: 13. Loss: 0.08597852289676666\n",
      "13 val 7 847\n",
      "Phase: val. Epoch: 13. Loss: 0.10767656564712524\n",
      "13 val 8 859\n",
      "Phase: val. Epoch: 13. Loss: 0.09561464190483093\n",
      "13 val 9 871\n",
      "Phase: val. Epoch: 13. Loss: 0.10609135031700134\n",
      "13 val 10 883\n",
      "Phase: val. Epoch: 13. Loss: 0.08349915593862534\n",
      "13 val 11 884\n",
      "Phase: val. Epoch: 13. Loss: 0.03437330573797226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 train 0 12\n",
      "Phase: train. Epoch: 14. Loss: 0.09024721384048462\n",
      "14 train 1 24\n",
      "Phase: train. Epoch: 14. Loss: 0.08033545315265656\n",
      "14 train 2 36\n",
      "Phase: train. Epoch: 14. Loss: 0.09138472378253937\n",
      "14 train 3 48\n",
      "Phase: train. Epoch: 14. Loss: 0.09930291771888733\n",
      "14 train 4 60\n",
      "Phase: train. Epoch: 14. Loss: 0.07020813226699829\n",
      "14 train 5 72\n",
      "Phase: train. Epoch: 14. Loss: 0.0778612270951271\n",
      "14 train 6 84\n",
      "Phase: train. Epoch: 14. Loss: 0.10138390958309174\n",
      "14 train 7 96\n",
      "Phase: train. Epoch: 14. Loss: 0.0940518006682396\n",
      "14 train 8 108\n",
      "Phase: train. Epoch: 14. Loss: 0.07872331142425537\n",
      "14 train 9 120\n",
      "Phase: train. Epoch: 14. Loss: 0.08651386201381683\n",
      "14 train 10 132\n",
      "Phase: train. Epoch: 14. Loss: 0.09942589700222015\n",
      "14 train 11 144\n",
      "Phase: train. Epoch: 14. Loss: 0.07366638630628586\n",
      "14 train 12 156\n",
      "Phase: train. Epoch: 14. Loss: 0.09238070249557495\n",
      "14 train 13 168\n",
      "Phase: train. Epoch: 14. Loss: 0.08620594441890717\n",
      "14 train 14 180\n",
      "Phase: train. Epoch: 14. Loss: 0.10405360162258148\n",
      "14 train 15 192\n",
      "Phase: train. Epoch: 14. Loss: 0.09627149999141693\n",
      "14 train 16 204\n",
      "Phase: train. Epoch: 14. Loss: 0.15728594362735748\n",
      "14 train 17 216\n",
      "Phase: train. Epoch: 14. Loss: 0.11406829208135605\n",
      "14 train 18 228\n",
      "Phase: train. Epoch: 14. Loss: 0.07827456295490265\n",
      "14 train 19 240\n",
      "Phase: train. Epoch: 14. Loss: 0.09315678477287292\n",
      "14 train 20 252\n",
      "Phase: train. Epoch: 14. Loss: 0.06897951662540436\n",
      "14 train 21 264\n",
      "Phase: train. Epoch: 14. Loss: 0.1070108637213707\n",
      "14 train 22 276\n",
      "Phase: train. Epoch: 14. Loss: 0.09580504149198532\n",
      "14 train 23 288\n",
      "Phase: train. Epoch: 14. Loss: 0.09842762351036072\n",
      "14 train 24 300\n",
      "Phase: train. Epoch: 14. Loss: 0.10378479957580566\n",
      "14 train 25 312\n",
      "Phase: train. Epoch: 14. Loss: 0.08753477036952972\n",
      "14 train 26 324\n",
      "Phase: train. Epoch: 14. Loss: 0.09060442447662354\n",
      "14 train 27 336\n",
      "Phase: train. Epoch: 14. Loss: 0.09104766696691513\n",
      "14 train 28 348\n",
      "Phase: train. Epoch: 14. Loss: 0.09372099488973618\n",
      "14 train 29 360\n",
      "Phase: train. Epoch: 14. Loss: 0.07345429807901382\n",
      "14 train 30 372\n",
      "Phase: train. Epoch: 14. Loss: 0.10559581220149994\n",
      "14 train 31 384\n",
      "Phase: train. Epoch: 14. Loss: 0.08962412923574448\n",
      "14 train 32 396\n",
      "Phase: train. Epoch: 14. Loss: 0.07717765867710114\n",
      "14 train 33 408\n",
      "Phase: train. Epoch: 14. Loss: 0.09830920398235321\n",
      "14 train 34 420\n",
      "Phase: train. Epoch: 14. Loss: 0.08043330907821655\n",
      "14 train 35 432\n",
      "Phase: train. Epoch: 14. Loss: 0.10587143898010254\n",
      "14 train 36 444\n",
      "Phase: train. Epoch: 14. Loss: 0.10840539634227753\n",
      "14 train 37 456\n",
      "Phase: train. Epoch: 14. Loss: 0.09411659091711044\n",
      "14 train 38 468\n",
      "Phase: train. Epoch: 14. Loss: 0.12292622774839401\n",
      "14 train 39 480\n",
      "Phase: train. Epoch: 14. Loss: 0.08636309206485748\n",
      "14 train 40 492\n",
      "Phase: train. Epoch: 14. Loss: 0.09449007362127304\n",
      "14 train 41 504\n",
      "Phase: train. Epoch: 14. Loss: 0.10114032030105591\n",
      "14 train 42 516\n",
      "Phase: train. Epoch: 14. Loss: 0.09947274625301361\n",
      "14 train 43 528\n",
      "Phase: train. Epoch: 14. Loss: 0.10527056455612183\n",
      "14 train 44 540\n",
      "Phase: train. Epoch: 14. Loss: 0.0955103263258934\n",
      "14 train 45 552\n",
      "Phase: train. Epoch: 14. Loss: 0.08647549152374268\n",
      "14 train 46 564\n",
      "Phase: train. Epoch: 14. Loss: 0.12078496813774109\n",
      "14 train 47 576\n",
      "Phase: train. Epoch: 14. Loss: 0.08513986319303513\n",
      "14 train 48 588\n",
      "Phase: train. Epoch: 14. Loss: 0.09466852992773056\n",
      "14 train 49 600\n",
      "Phase: train. Epoch: 14. Loss: 0.09088633954524994\n",
      "14 train 50 612\n",
      "Phase: train. Epoch: 14. Loss: 0.10027486085891724\n",
      "14 train 51 624\n",
      "Phase: train. Epoch: 14. Loss: 0.10975323617458344\n",
      "14 train 52 636\n",
      "Phase: train. Epoch: 14. Loss: 0.07813076674938202\n",
      "14 train 53 648\n",
      "Phase: train. Epoch: 14. Loss: 0.10210766643285751\n",
      "14 train 54 660\n",
      "Phase: train. Epoch: 14. Loss: 0.08990626037120819\n",
      "14 train 55 672\n",
      "Phase: train. Epoch: 14. Loss: 0.11087309569120407\n",
      "14 train 56 684\n",
      "Phase: train. Epoch: 14. Loss: 0.08795784413814545\n",
      "14 train 57 696\n",
      "Phase: train. Epoch: 14. Loss: 0.09215663373470306\n",
      "14 train 58 708\n",
      "Phase: train. Epoch: 14. Loss: 0.10397303104400635\n",
      "14 train 59 720\n",
      "Phase: train. Epoch: 14. Loss: 0.11906234920024872\n",
      "14 train 60 732\n",
      "Phase: train. Epoch: 14. Loss: 0.08501249551773071\n",
      "14 train 61 744\n",
      "Phase: train. Epoch: 14. Loss: 0.1029614806175232\n",
      "14 train 62 751\n",
      "Phase: train. Epoch: 14. Loss: 0.1371365189552307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 val 0 763\n",
      "Phase: val. Epoch: 14. Loss: 0.11091490834951401\n",
      "14 val 1 775\n",
      "Phase: val. Epoch: 14. Loss: 0.10177239030599594\n",
      "14 val 2 787\n",
      "Phase: val. Epoch: 14. Loss: 0.09322451800107956\n",
      "14 val 3 799\n",
      "Phase: val. Epoch: 14. Loss: 0.07448162883520126\n",
      "14 val 4 811\n",
      "Phase: val. Epoch: 14. Loss: 0.09010746330022812\n",
      "14 val 5 823\n",
      "Phase: val. Epoch: 14. Loss: 0.0633414089679718\n",
      "14 val 6 835\n",
      "Phase: val. Epoch: 14. Loss: 0.11798180639743805\n",
      "14 val 7 847\n",
      "Phase: val. Epoch: 14. Loss: 0.05962563306093216\n",
      "14 val 8 859\n",
      "Phase: val. Epoch: 14. Loss: 0.07982636243104935\n",
      "14 val 9 871\n",
      "Phase: val. Epoch: 14. Loss: 0.088937908411026\n",
      "14 val 10 883\n",
      "Phase: val. Epoch: 14. Loss: 0.11234723031520844\n",
      "14 val 11 884\n",
      "Phase: val. Epoch: 14. Loss: 0.11543919146060944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 train 0 12\n",
      "Phase: train. Epoch: 15. Loss: 0.10557147115468979\n",
      "15 train 1 24\n",
      "Phase: train. Epoch: 15. Loss: 0.10776911675930023\n",
      "15 train 2 36\n",
      "Phase: train. Epoch: 15. Loss: 0.08939146995544434\n",
      "15 train 3 48\n",
      "Phase: train. Epoch: 15. Loss: 0.12926599383354187\n",
      "15 train 4 60\n",
      "Phase: train. Epoch: 15. Loss: 0.07452305406332016\n",
      "15 train 5 72\n",
      "Phase: train. Epoch: 15. Loss: 0.09138286113739014\n",
      "15 train 6 84\n",
      "Phase: train. Epoch: 15. Loss: 0.07006444036960602\n",
      "15 train 7 96\n",
      "Phase: train. Epoch: 15. Loss: 0.0860261619091034\n",
      "15 train 8 108\n",
      "Phase: train. Epoch: 15. Loss: 0.09775936603546143\n",
      "15 train 9 120\n",
      "Phase: train. Epoch: 15. Loss: 0.08271800726652145\n",
      "15 train 10 132\n",
      "Phase: train. Epoch: 15. Loss: 0.11068804562091827\n",
      "15 train 11 144\n",
      "Phase: train. Epoch: 15. Loss: 0.07948499172925949\n",
      "15 train 12 156\n",
      "Phase: train. Epoch: 15. Loss: 0.0888851135969162\n",
      "15 train 13 168\n",
      "Phase: train. Epoch: 15. Loss: 0.09122457355260849\n",
      "15 train 14 180\n",
      "Phase: train. Epoch: 15. Loss: 0.07847251743078232\n",
      "15 train 15 192\n",
      "Phase: train. Epoch: 15. Loss: 0.0968737006187439\n",
      "15 train 16 204\n",
      "Phase: train. Epoch: 15. Loss: 0.08028092235326767\n",
      "15 train 17 216\n",
      "Phase: train. Epoch: 15. Loss: 0.09845328330993652\n",
      "15 train 18 228\n",
      "Phase: train. Epoch: 15. Loss: 0.10553176701068878\n",
      "15 train 19 240\n",
      "Phase: train. Epoch: 15. Loss: 0.08957221359014511\n",
      "15 train 20 252\n",
      "Phase: train. Epoch: 15. Loss: 0.09642466902732849\n",
      "15 train 21 264\n",
      "Phase: train. Epoch: 15. Loss: 0.08776875585317612\n",
      "15 train 22 276\n",
      "Phase: train. Epoch: 15. Loss: 0.08754252642393112\n",
      "15 train 23 288\n",
      "Phase: train. Epoch: 15. Loss: 0.08307305723428726\n",
      "15 train 24 300\n",
      "Phase: train. Epoch: 15. Loss: 0.08423038572072983\n",
      "15 train 25 312\n",
      "Phase: train. Epoch: 15. Loss: 0.09428976476192474\n",
      "15 train 26 324\n",
      "Phase: train. Epoch: 15. Loss: 0.1067965030670166\n",
      "15 train 27 336\n",
      "Phase: train. Epoch: 15. Loss: 0.09603270143270493\n",
      "15 train 28 348\n",
      "Phase: train. Epoch: 15. Loss: 0.13129909336566925\n",
      "15 train 29 360\n",
      "Phase: train. Epoch: 15. Loss: 0.09176754951477051\n",
      "15 train 30 372\n",
      "Phase: train. Epoch: 15. Loss: 0.07833223044872284\n",
      "15 train 31 384\n",
      "Phase: train. Epoch: 15. Loss: 0.10200759768486023\n",
      "15 train 32 396\n",
      "Phase: train. Epoch: 15. Loss: 0.08018563687801361\n",
      "15 train 33 408\n",
      "Phase: train. Epoch: 15. Loss: 0.06891664862632751\n",
      "15 train 34 420\n",
      "Phase: train. Epoch: 15. Loss: 0.082417331635952\n",
      "15 train 35 432\n",
      "Phase: train. Epoch: 15. Loss: 0.11242465674877167\n",
      "15 train 36 444\n",
      "Phase: train. Epoch: 15. Loss: 0.0904930830001831\n",
      "15 train 37 456\n",
      "Phase: train. Epoch: 15. Loss: 0.10530796647071838\n",
      "15 train 38 468\n",
      "Phase: train. Epoch: 15. Loss: 0.08557921648025513\n",
      "15 train 39 480\n",
      "Phase: train. Epoch: 15. Loss: 0.12320351600646973\n",
      "15 train 40 492\n",
      "Phase: train. Epoch: 15. Loss: 0.105894073843956\n",
      "15 train 41 504\n",
      "Phase: train. Epoch: 15. Loss: 0.087418332695961\n",
      "15 train 42 516\n",
      "Phase: train. Epoch: 15. Loss: 0.09927427023649216\n",
      "15 train 43 528\n",
      "Phase: train. Epoch: 15. Loss: 0.09353572130203247\n",
      "15 train 44 540\n",
      "Phase: train. Epoch: 15. Loss: 0.09898869693279266\n",
      "15 train 45 552\n",
      "Phase: train. Epoch: 15. Loss: 0.09929893910884857\n",
      "15 train 46 564\n",
      "Phase: train. Epoch: 15. Loss: 0.09623199701309204\n",
      "15 train 47 576\n",
      "Phase: train. Epoch: 15. Loss: 0.08778124302625656\n",
      "15 train 48 588\n",
      "Phase: train. Epoch: 15. Loss: 0.09000517427921295\n",
      "15 train 49 600\n",
      "Phase: train. Epoch: 15. Loss: 0.09843947738409042\n",
      "15 train 50 612\n",
      "Phase: train. Epoch: 15. Loss: 0.08162200450897217\n",
      "15 train 51 624\n",
      "Phase: train. Epoch: 15. Loss: 0.09506174176931381\n",
      "15 train 52 636\n",
      "Phase: train. Epoch: 15. Loss: 0.10364007949829102\n",
      "15 train 53 648\n",
      "Phase: train. Epoch: 15. Loss: 0.07597632706165314\n",
      "15 train 54 660\n",
      "Phase: train. Epoch: 15. Loss: 0.10275037586688995\n",
      "15 train 55 672\n",
      "Phase: train. Epoch: 15. Loss: 0.08693691343069077\n",
      "15 train 56 684\n",
      "Phase: train. Epoch: 15. Loss: 0.11368590593338013\n",
      "15 train 57 696\n",
      "Phase: train. Epoch: 15. Loss: 0.10220231860876083\n",
      "15 train 58 708\n",
      "Phase: train. Epoch: 15. Loss: 0.13011637330055237\n",
      "15 train 59 720\n",
      "Phase: train. Epoch: 15. Loss: 0.07310213148593903\n",
      "15 train 60 732\n",
      "Phase: train. Epoch: 15. Loss: 0.12332068383693695\n",
      "15 train 61 744\n",
      "Phase: train. Epoch: 15. Loss: 0.10621173679828644\n",
      "15 train 62 751\n",
      "Phase: train. Epoch: 15. Loss: 0.11136461049318314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 val 0 763\n",
      "Phase: val. Epoch: 15. Loss: 0.05947098135948181\n",
      "15 val 1 775\n",
      "Phase: val. Epoch: 15. Loss: 0.09685604274272919\n",
      "15 val 2 787\n",
      "Phase: val. Epoch: 15. Loss: 0.10343160480260849\n",
      "15 val 3 799\n",
      "Phase: val. Epoch: 15. Loss: 0.1537874937057495\n",
      "15 val 4 811\n",
      "Phase: val. Epoch: 15. Loss: 0.08913945406675339\n",
      "15 val 5 823\n",
      "Phase: val. Epoch: 15. Loss: 0.07535742968320847\n",
      "15 val 6 835\n",
      "Phase: val. Epoch: 15. Loss: 0.08934476226568222\n",
      "15 val 7 847\n",
      "Phase: val. Epoch: 15. Loss: 0.09185732901096344\n",
      "15 val 8 859\n",
      "Phase: val. Epoch: 15. Loss: 0.0669708177447319\n",
      "15 val 9 871\n",
      "Phase: val. Epoch: 15. Loss: 0.06230786442756653\n",
      "15 val 10 883\n",
      "Phase: val. Epoch: 15. Loss: 0.08128847926855087\n",
      "15 val 11 884\n",
      "Phase: val. Epoch: 15. Loss: 0.11715652048587799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 train 0 12\n",
      "Phase: train. Epoch: 16. Loss: 0.13177061080932617\n",
      "16 train 1 24\n",
      "Phase: train. Epoch: 16. Loss: 0.15243999660015106\n",
      "16 train 2 36\n",
      "Phase: train. Epoch: 16. Loss: 0.089137963950634\n",
      "16 train 3 48\n",
      "Phase: train. Epoch: 16. Loss: 0.09680315852165222\n",
      "16 train 4 60\n",
      "Phase: train. Epoch: 16. Loss: 0.09693913906812668\n",
      "16 train 5 72\n",
      "Phase: train. Epoch: 16. Loss: 0.08631257712841034\n",
      "16 train 6 84\n",
      "Phase: train. Epoch: 16. Loss: 0.09331993758678436\n",
      "16 train 7 96\n",
      "Phase: train. Epoch: 16. Loss: 0.09525161981582642\n",
      "16 train 8 108\n",
      "Phase: train. Epoch: 16. Loss: 0.08831285685300827\n",
      "16 train 9 120\n",
      "Phase: train. Epoch: 16. Loss: 0.08904828131198883\n",
      "16 train 10 132\n",
      "Phase: train. Epoch: 16. Loss: 0.09556339681148529\n",
      "16 train 11 144\n",
      "Phase: train. Epoch: 16. Loss: 0.10591160506010056\n",
      "16 train 12 156\n",
      "Phase: train. Epoch: 16. Loss: 0.09571421146392822\n",
      "16 train 13 168\n",
      "Phase: train. Epoch: 16. Loss: 0.12105017155408859\n",
      "16 train 14 180\n",
      "Phase: train. Epoch: 16. Loss: 0.08212026208639145\n",
      "16 train 15 192\n",
      "Phase: train. Epoch: 16. Loss: 0.086319200694561\n",
      "16 train 16 204\n",
      "Phase: train. Epoch: 16. Loss: 0.09566213935613632\n",
      "16 train 17 216\n",
      "Phase: train. Epoch: 16. Loss: 0.1024949848651886\n",
      "16 train 18 228\n",
      "Phase: train. Epoch: 16. Loss: 0.11136960983276367\n",
      "16 train 19 240\n",
      "Phase: train. Epoch: 16. Loss: 0.09492699801921844\n",
      "16 train 20 252\n",
      "Phase: train. Epoch: 16. Loss: 0.08161181956529617\n",
      "16 train 21 264\n",
      "Phase: train. Epoch: 16. Loss: 0.10773588716983795\n",
      "16 train 22 276\n",
      "Phase: train. Epoch: 16. Loss: 0.09501133859157562\n",
      "16 train 23 288\n",
      "Phase: train. Epoch: 16. Loss: 0.08203616738319397\n",
      "16 train 24 300\n",
      "Phase: train. Epoch: 16. Loss: 0.09401395916938782\n",
      "16 train 25 312\n",
      "Phase: train. Epoch: 16. Loss: 0.07887916266918182\n",
      "16 train 26 324\n",
      "Phase: train. Epoch: 16. Loss: 0.07874947041273117\n",
      "16 train 27 336\n",
      "Phase: train. Epoch: 16. Loss: 0.08352487534284592\n",
      "16 train 28 348\n",
      "Phase: train. Epoch: 16. Loss: 0.08311858028173447\n",
      "16 train 29 360\n",
      "Phase: train. Epoch: 16. Loss: 0.09133437275886536\n",
      "16 train 30 372\n",
      "Phase: train. Epoch: 16. Loss: 0.09637445211410522\n",
      "16 train 31 384\n",
      "Phase: train. Epoch: 16. Loss: 0.08491333574056625\n",
      "16 train 32 396\n",
      "Phase: train. Epoch: 16. Loss: 0.08425440639257431\n",
      "16 train 33 408\n",
      "Phase: train. Epoch: 16. Loss: 0.11041954159736633\n",
      "16 train 34 420\n",
      "Phase: train. Epoch: 16. Loss: 0.08139407634735107\n",
      "16 train 35 432\n",
      "Phase: train. Epoch: 16. Loss: 0.08711603283882141\n",
      "16 train 36 444\n",
      "Phase: train. Epoch: 16. Loss: 0.10533623397350311\n",
      "16 train 37 456\n",
      "Phase: train. Epoch: 16. Loss: 0.08429412543773651\n",
      "16 train 38 468\n",
      "Phase: train. Epoch: 16. Loss: 0.08196017146110535\n",
      "16 train 39 480\n",
      "Phase: train. Epoch: 16. Loss: 0.07517875730991364\n",
      "16 train 40 492\n",
      "Phase: train. Epoch: 16. Loss: 0.08854904025793076\n",
      "16 train 41 504\n",
      "Phase: train. Epoch: 16. Loss: 0.10867622494697571\n",
      "16 train 42 516\n",
      "Phase: train. Epoch: 16. Loss: 0.09647166728973389\n",
      "16 train 43 528\n",
      "Phase: train. Epoch: 16. Loss: 0.09736152738332748\n",
      "16 train 44 540\n",
      "Phase: train. Epoch: 16. Loss: 0.08408858627080917\n",
      "16 train 45 552\n",
      "Phase: train. Epoch: 16. Loss: 0.09040343761444092\n",
      "16 train 46 564\n",
      "Phase: train. Epoch: 16. Loss: 0.07479429990053177\n",
      "16 train 47 576\n",
      "Phase: train. Epoch: 16. Loss: 0.07601549476385117\n",
      "16 train 48 588\n",
      "Phase: train. Epoch: 16. Loss: 0.07577979564666748\n",
      "16 train 49 600\n",
      "Phase: train. Epoch: 16. Loss: 0.0733058974146843\n",
      "16 train 50 612\n",
      "Phase: train. Epoch: 16. Loss: 0.10956649482250214\n",
      "16 train 51 624\n",
      "Phase: train. Epoch: 16. Loss: 0.09172260016202927\n",
      "16 train 52 636\n",
      "Phase: train. Epoch: 16. Loss: 0.07087170332670212\n",
      "16 train 53 648\n",
      "Phase: train. Epoch: 16. Loss: 0.0864119678735733\n",
      "16 train 54 660\n",
      "Phase: train. Epoch: 16. Loss: 0.07356555759906769\n",
      "16 train 55 672\n",
      "Phase: train. Epoch: 16. Loss: 0.09070193022489548\n",
      "16 train 56 684\n",
      "Phase: train. Epoch: 16. Loss: 0.1010408028960228\n",
      "16 train 57 696\n",
      "Phase: train. Epoch: 16. Loss: 0.09260906279087067\n",
      "16 train 58 708\n",
      "Phase: train. Epoch: 16. Loss: 0.09513401985168457\n",
      "16 train 59 720\n",
      "Phase: train. Epoch: 16. Loss: 0.09745520353317261\n",
      "16 train 60 732\n",
      "Phase: train. Epoch: 16. Loss: 0.06999894231557846\n",
      "16 train 61 744\n",
      "Phase: train. Epoch: 16. Loss: 0.08367234468460083\n",
      "16 train 62 751\n",
      "Phase: train. Epoch: 16. Loss: 0.10626629739999771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 val 0 763\n",
      "Phase: val. Epoch: 16. Loss: 0.13747327029705048\n",
      "16 val 1 775\n",
      "Phase: val. Epoch: 16. Loss: 0.104814313352108\n",
      "16 val 2 787\n",
      "Phase: val. Epoch: 16. Loss: 0.08214780688285828\n",
      "16 val 3 799\n",
      "Phase: val. Epoch: 16. Loss: 0.07437136024236679\n",
      "16 val 4 811\n",
      "Phase: val. Epoch: 16. Loss: 0.11614834517240524\n",
      "16 val 5 823\n",
      "Phase: val. Epoch: 16. Loss: 0.08112030476331711\n",
      "16 val 6 835\n",
      "Phase: val. Epoch: 16. Loss: 0.12867483496665955\n",
      "16 val 7 847\n",
      "Phase: val. Epoch: 16. Loss: 0.10529589653015137\n",
      "16 val 8 859\n",
      "Phase: val. Epoch: 16. Loss: 0.13038486242294312\n",
      "16 val 9 871\n",
      "Phase: val. Epoch: 16. Loss: 0.10662601888179779\n",
      "16 val 10 883\n",
      "Phase: val. Epoch: 16. Loss: 0.050063565373420715\n",
      "16 val 11 884\n",
      "Phase: val. Epoch: 16. Loss: 0.032148730009794235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 train 0 12\n",
      "Phase: train. Epoch: 17. Loss: 0.10102371871471405\n",
      "17 train 1 24\n",
      "Phase: train. Epoch: 17. Loss: 0.06713168323040009\n",
      "17 train 2 36\n",
      "Phase: train. Epoch: 17. Loss: 0.07957357168197632\n",
      "17 train 3 48\n",
      "Phase: train. Epoch: 17. Loss: 0.08245900273323059\n",
      "17 train 4 60\n",
      "Phase: train. Epoch: 17. Loss: 0.10374811291694641\n",
      "17 train 5 72\n",
      "Phase: train. Epoch: 17. Loss: 0.09998640418052673\n",
      "17 train 6 84\n",
      "Phase: train. Epoch: 17. Loss: 0.09331782907247543\n",
      "17 train 7 96\n",
      "Phase: train. Epoch: 17. Loss: 0.1287815272808075\n",
      "17 train 8 108\n",
      "Phase: train. Epoch: 17. Loss: 0.08735646307468414\n",
      "17 train 9 120\n",
      "Phase: train. Epoch: 17. Loss: 0.12153515219688416\n",
      "17 train 10 132\n",
      "Phase: train. Epoch: 17. Loss: 0.08625560998916626\n",
      "17 train 11 144\n",
      "Phase: train. Epoch: 17. Loss: 0.07397821545600891\n",
      "17 train 12 156\n",
      "Phase: train. Epoch: 17. Loss: 0.10804295539855957\n",
      "17 train 13 168\n",
      "Phase: train. Epoch: 17. Loss: 0.08353729546070099\n",
      "17 train 14 180\n",
      "Phase: train. Epoch: 17. Loss: 0.08350178599357605\n",
      "17 train 15 192\n",
      "Phase: train. Epoch: 17. Loss: 0.11866708844900131\n",
      "17 train 16 204\n",
      "Phase: train. Epoch: 17. Loss: 0.09125486016273499\n",
      "17 train 17 216\n",
      "Phase: train. Epoch: 17. Loss: 0.09227760136127472\n",
      "17 train 18 228\n",
      "Phase: train. Epoch: 17. Loss: 0.1051245927810669\n",
      "17 train 19 240\n",
      "Phase: train. Epoch: 17. Loss: 0.09668891876935959\n",
      "17 train 20 252\n",
      "Phase: train. Epoch: 17. Loss: 0.07362079620361328\n",
      "17 train 21 264\n",
      "Phase: train. Epoch: 17. Loss: 0.07592375576496124\n",
      "17 train 22 276\n",
      "Phase: train. Epoch: 17. Loss: 0.10858067125082016\n",
      "17 train 23 288\n",
      "Phase: train. Epoch: 17. Loss: 0.09758253395557404\n",
      "17 train 24 300\n",
      "Phase: train. Epoch: 17. Loss: 0.12839952111244202\n",
      "17 train 25 312\n",
      "Phase: train. Epoch: 17. Loss: 0.10122095793485641\n",
      "17 train 26 324\n",
      "Phase: train. Epoch: 17. Loss: 0.09868261218070984\n",
      "17 train 27 336\n",
      "Phase: train. Epoch: 17. Loss: 0.08314040303230286\n",
      "17 train 28 348\n",
      "Phase: train. Epoch: 17. Loss: 0.07906489074230194\n",
      "17 train 29 360\n",
      "Phase: train. Epoch: 17. Loss: 0.10896578431129456\n",
      "17 train 30 372\n",
      "Phase: train. Epoch: 17. Loss: 0.09335170686244965\n",
      "17 train 31 384\n",
      "Phase: train. Epoch: 17. Loss: 0.09969135373830795\n",
      "17 train 32 396\n",
      "Phase: train. Epoch: 17. Loss: 0.13121667504310608\n",
      "17 train 33 408\n",
      "Phase: train. Epoch: 17. Loss: 0.09547637403011322\n",
      "17 train 34 420\n",
      "Phase: train. Epoch: 17. Loss: 0.10555221140384674\n",
      "17 train 35 432\n",
      "Phase: train. Epoch: 17. Loss: 0.08783942461013794\n",
      "17 train 36 444\n",
      "Phase: train. Epoch: 17. Loss: 0.0806117057800293\n",
      "17 train 37 456\n",
      "Phase: train. Epoch: 17. Loss: 0.10657459497451782\n",
      "17 train 38 468\n",
      "Phase: train. Epoch: 17. Loss: 0.10295987129211426\n",
      "17 train 39 480\n",
      "Phase: train. Epoch: 17. Loss: 0.08530797809362411\n",
      "17 train 40 492\n",
      "Phase: train. Epoch: 17. Loss: 0.10226850211620331\n",
      "17 train 41 504\n",
      "Phase: train. Epoch: 17. Loss: 0.08207908272743225\n",
      "17 train 42 516\n",
      "Phase: train. Epoch: 17. Loss: 0.09628735482692719\n",
      "17 train 43 528\n",
      "Phase: train. Epoch: 17. Loss: 0.11382293701171875\n",
      "17 train 44 540\n",
      "Phase: train. Epoch: 17. Loss: 0.10961385071277618\n",
      "17 train 45 552\n",
      "Phase: train. Epoch: 17. Loss: 0.06487955152988434\n",
      "17 train 46 564\n",
      "Phase: train. Epoch: 17. Loss: 0.08215467631816864\n",
      "17 train 47 576\n",
      "Phase: train. Epoch: 17. Loss: 0.0816393569111824\n",
      "17 train 48 588\n",
      "Phase: train. Epoch: 17. Loss: 0.0796031653881073\n",
      "17 train 49 600\n",
      "Phase: train. Epoch: 17. Loss: 0.09794482588768005\n",
      "17 train 50 612\n",
      "Phase: train. Epoch: 17. Loss: 0.0787229984998703\n",
      "17 train 51 624\n",
      "Phase: train. Epoch: 17. Loss: 0.08542148768901825\n",
      "17 train 52 636\n",
      "Phase: train. Epoch: 17. Loss: 0.07531990110874176\n",
      "17 train 53 648\n",
      "Phase: train. Epoch: 17. Loss: 0.0785355493426323\n",
      "17 train 54 660\n",
      "Phase: train. Epoch: 17. Loss: 0.08892615884542465\n",
      "17 train 55 672\n",
      "Phase: train. Epoch: 17. Loss: 0.10955427587032318\n",
      "17 train 56 684\n",
      "Phase: train. Epoch: 17. Loss: 0.08169341087341309\n",
      "17 train 57 696\n",
      "Phase: train. Epoch: 17. Loss: 0.08167615532875061\n",
      "17 train 58 708\n",
      "Phase: train. Epoch: 17. Loss: 0.08596424758434296\n",
      "17 train 59 720\n",
      "Phase: train. Epoch: 17. Loss: 0.09562911838293076\n",
      "17 train 60 732\n",
      "Phase: train. Epoch: 17. Loss: 0.08236649632453918\n",
      "17 train 61 744\n",
      "Phase: train. Epoch: 17. Loss: 0.10427733510732651\n",
      "17 train 62 751\n",
      "Phase: train. Epoch: 17. Loss: 0.07856752723455429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 val 0 763\n",
      "Phase: val. Epoch: 17. Loss: 0.10877418518066406\n",
      "17 val 1 775\n",
      "Phase: val. Epoch: 17. Loss: 0.10223393142223358\n",
      "17 val 2 787\n",
      "Phase: val. Epoch: 17. Loss: 0.07586639374494553\n",
      "17 val 3 799\n",
      "Phase: val. Epoch: 17. Loss: 0.11242620646953583\n",
      "17 val 4 811\n",
      "Phase: val. Epoch: 17. Loss: 0.0810571014881134\n",
      "17 val 5 823\n",
      "Phase: val. Epoch: 17. Loss: 0.07666052877902985\n",
      "17 val 6 835\n",
      "Phase: val. Epoch: 17. Loss: 0.06673264503479004\n",
      "17 val 7 847\n",
      "Phase: val. Epoch: 17. Loss: 0.08273386210203171\n",
      "17 val 8 859\n",
      "Phase: val. Epoch: 17. Loss: 0.09360744804143906\n",
      "17 val 9 871\n",
      "Phase: val. Epoch: 17. Loss: 0.07950156182050705\n",
      "17 val 10 883\n",
      "Phase: val. Epoch: 17. Loss: 0.09016763418912888\n",
      "17 val 11 884\n",
      "Phase: val. Epoch: 17. Loss: 0.03248827904462814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 train 0 12\n",
      "Phase: train. Epoch: 18. Loss: 0.0920603796839714\n",
      "18 train 1 24\n",
      "Phase: train. Epoch: 18. Loss: 0.08906515687704086\n",
      "18 train 2 36\n",
      "Phase: train. Epoch: 18. Loss: 0.07724147289991379\n",
      "18 train 3 48\n",
      "Phase: train. Epoch: 18. Loss: 0.07278994470834732\n",
      "18 train 4 60\n",
      "Phase: train. Epoch: 18. Loss: 0.0999838337302208\n",
      "18 train 5 72\n",
      "Phase: train. Epoch: 18. Loss: 0.1495455801486969\n",
      "18 train 6 84\n",
      "Phase: train. Epoch: 18. Loss: 0.1008666455745697\n",
      "18 train 7 96\n",
      "Phase: train. Epoch: 18. Loss: 0.08347617089748383\n",
      "18 train 8 108\n",
      "Phase: train. Epoch: 18. Loss: 0.09754610061645508\n",
      "18 train 9 120\n",
      "Phase: train. Epoch: 18. Loss: 0.10720996558666229\n",
      "18 train 10 132\n",
      "Phase: train. Epoch: 18. Loss: 0.08204849064350128\n",
      "18 train 11 144\n",
      "Phase: train. Epoch: 18. Loss: 0.0845479667186737\n",
      "18 train 12 156\n",
      "Phase: train. Epoch: 18. Loss: 0.09918009489774704\n",
      "18 train 13 168\n",
      "Phase: train. Epoch: 18. Loss: 0.08000540733337402\n",
      "18 train 14 180\n",
      "Phase: train. Epoch: 18. Loss: 0.09516257047653198\n",
      "18 train 15 192\n",
      "Phase: train. Epoch: 18. Loss: 0.08658512681722641\n",
      "18 train 16 204\n",
      "Phase: train. Epoch: 18. Loss: 0.09127142280340195\n",
      "18 train 17 216\n",
      "Phase: train. Epoch: 18. Loss: 0.12303133308887482\n",
      "18 train 18 228\n",
      "Phase: train. Epoch: 18. Loss: 0.09645776450634003\n",
      "18 train 19 240\n",
      "Phase: train. Epoch: 18. Loss: 0.08691336959600449\n",
      "18 train 20 252\n",
      "Phase: train. Epoch: 18. Loss: 0.0928032249212265\n",
      "18 train 21 264\n",
      "Phase: train. Epoch: 18. Loss: 0.08239129930734634\n",
      "18 train 22 276\n",
      "Phase: train. Epoch: 18. Loss: 0.08961062133312225\n",
      "18 train 23 288\n",
      "Phase: train. Epoch: 18. Loss: 0.09737427532672882\n",
      "18 train 24 300\n",
      "Phase: train. Epoch: 18. Loss: 0.08831126987934113\n",
      "18 train 25 312\n",
      "Phase: train. Epoch: 18. Loss: 0.09457486867904663\n",
      "18 train 26 324\n",
      "Phase: train. Epoch: 18. Loss: 0.10814549773931503\n",
      "18 train 27 336\n",
      "Phase: train. Epoch: 18. Loss: 0.0987696573138237\n",
      "18 train 28 348\n",
      "Phase: train. Epoch: 18. Loss: 0.09093266725540161\n",
      "18 train 29 360\n",
      "Phase: train. Epoch: 18. Loss: 0.08200794458389282\n",
      "18 train 30 372\n",
      "Phase: train. Epoch: 18. Loss: 0.09700523316860199\n",
      "18 train 31 384\n",
      "Phase: train. Epoch: 18. Loss: 0.1030815988779068\n",
      "18 train 32 396\n",
      "Phase: train. Epoch: 18. Loss: 0.08982227742671967\n",
      "18 train 33 408\n",
      "Phase: train. Epoch: 18. Loss: 0.0811077281832695\n",
      "18 train 34 420\n",
      "Phase: train. Epoch: 18. Loss: 0.09093104302883148\n",
      "18 train 35 432\n",
      "Phase: train. Epoch: 18. Loss: 0.09661509096622467\n",
      "18 train 36 444\n",
      "Phase: train. Epoch: 18. Loss: 0.07991363853216171\n",
      "18 train 37 456\n",
      "Phase: train. Epoch: 18. Loss: 0.06717988103628159\n",
      "18 train 38 468\n",
      "Phase: train. Epoch: 18. Loss: 0.08534292876720428\n",
      "18 train 39 480\n",
      "Phase: train. Epoch: 18. Loss: 0.07840777933597565\n",
      "18 train 40 492\n",
      "Phase: train. Epoch: 18. Loss: 0.08292040228843689\n",
      "18 train 41 504\n",
      "Phase: train. Epoch: 18. Loss: 0.07991757988929749\n",
      "18 train 42 516\n",
      "Phase: train. Epoch: 18. Loss: 0.11207155883312225\n",
      "18 train 43 528\n",
      "Phase: train. Epoch: 18. Loss: 0.09539693593978882\n",
      "18 train 44 540\n",
      "Phase: train. Epoch: 18. Loss: 0.12347404658794403\n",
      "18 train 45 552\n",
      "Phase: train. Epoch: 18. Loss: 0.10204589366912842\n",
      "18 train 46 564\n",
      "Phase: train. Epoch: 18. Loss: 0.0911603569984436\n",
      "18 train 47 576\n",
      "Phase: train. Epoch: 18. Loss: 0.07278640568256378\n",
      "18 train 48 588\n",
      "Phase: train. Epoch: 18. Loss: 0.08139897137880325\n",
      "18 train 49 600\n",
      "Phase: train. Epoch: 18. Loss: 0.10311337560415268\n",
      "18 train 50 612\n",
      "Phase: train. Epoch: 18. Loss: 0.08062373101711273\n",
      "18 train 51 624\n",
      "Phase: train. Epoch: 18. Loss: 0.0981428474187851\n",
      "18 train 52 636\n",
      "Phase: train. Epoch: 18. Loss: 0.07513615489006042\n",
      "18 train 53 648\n",
      "Phase: train. Epoch: 18. Loss: 0.08203452825546265\n",
      "18 train 54 660\n",
      "Phase: train. Epoch: 18. Loss: 0.08408842980861664\n",
      "18 train 55 672\n",
      "Phase: train. Epoch: 18. Loss: 0.07955686002969742\n",
      "18 train 56 684\n",
      "Phase: train. Epoch: 18. Loss: 0.08463089168071747\n",
      "18 train 57 696\n",
      "Phase: train. Epoch: 18. Loss: 0.09574036300182343\n",
      "18 train 58 708\n",
      "Phase: train. Epoch: 18. Loss: 0.09370572865009308\n",
      "18 train 59 720\n",
      "Phase: train. Epoch: 18. Loss: 0.127395361661911\n",
      "18 train 60 732\n",
      "Phase: train. Epoch: 18. Loss: 0.08478501439094543\n",
      "18 train 61 744\n",
      "Phase: train. Epoch: 18. Loss: 0.08105866611003876\n",
      "18 train 62 751\n",
      "Phase: train. Epoch: 18. Loss: 0.0937163382768631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 val 0 763\n",
      "Phase: val. Epoch: 18. Loss: 0.05296223983168602\n",
      "18 val 1 775\n",
      "Phase: val. Epoch: 18. Loss: 0.09945595264434814\n",
      "18 val 2 787\n",
      "Phase: val. Epoch: 18. Loss: 0.07283224910497665\n",
      "18 val 3 799\n",
      "Phase: val. Epoch: 18. Loss: 0.0828460305929184\n",
      "18 val 4 811\n",
      "Phase: val. Epoch: 18. Loss: 0.07490231096744537\n",
      "18 val 5 823\n",
      "Phase: val. Epoch: 18. Loss: 0.1017603725194931\n",
      "18 val 6 835\n",
      "Phase: val. Epoch: 18. Loss: 0.09620922803878784\n",
      "18 val 7 847\n",
      "Phase: val. Epoch: 18. Loss: 0.07323205471038818\n",
      "18 val 8 859\n",
      "Phase: val. Epoch: 18. Loss: 0.07205590605735779\n",
      "18 val 9 871\n",
      "Phase: val. Epoch: 18. Loss: 0.12210547924041748\n",
      "18 val 10 883\n",
      "Phase: val. Epoch: 18. Loss: 0.10296141356229782\n",
      "18 val 11 884\n",
      "Phase: val. Epoch: 18. Loss: 0.03596553951501846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 train 0 12\n",
      "Phase: train. Epoch: 19. Loss: 0.07773752510547638\n",
      "19 train 1 24\n",
      "Phase: train. Epoch: 19. Loss: 0.0903996154665947\n",
      "19 train 2 36\n",
      "Phase: train. Epoch: 19. Loss: 0.08886347711086273\n",
      "19 train 3 48\n",
      "Phase: train. Epoch: 19. Loss: 0.08968451619148254\n",
      "19 train 4 60\n",
      "Phase: train. Epoch: 19. Loss: 0.11467685550451279\n",
      "19 train 5 72\n",
      "Phase: train. Epoch: 19. Loss: 0.13898548483848572\n",
      "19 train 6 84\n",
      "Phase: train. Epoch: 19. Loss: 0.08399691432714462\n",
      "19 train 7 96\n",
      "Phase: train. Epoch: 19. Loss: 0.10062222182750702\n",
      "19 train 8 108\n",
      "Phase: train. Epoch: 19. Loss: 0.09704261273145676\n",
      "19 train 9 120\n",
      "Phase: train. Epoch: 19. Loss: 0.1270826905965805\n",
      "19 train 10 132\n",
      "Phase: train. Epoch: 19. Loss: 0.10850896686315536\n",
      "19 train 11 144\n",
      "Phase: train. Epoch: 19. Loss: 0.09041740000247955\n",
      "19 train 12 156\n",
      "Phase: train. Epoch: 19. Loss: 0.0834663137793541\n",
      "19 train 13 168\n",
      "Phase: train. Epoch: 19. Loss: 0.08029872179031372\n",
      "19 train 14 180\n",
      "Phase: train. Epoch: 19. Loss: 0.12563815712928772\n",
      "19 train 15 192\n",
      "Phase: train. Epoch: 19. Loss: 0.085151806473732\n",
      "19 train 16 204\n",
      "Phase: train. Epoch: 19. Loss: 0.06361989676952362\n",
      "19 train 17 216\n",
      "Phase: train. Epoch: 19. Loss: 0.084307961165905\n",
      "19 train 18 228\n",
      "Phase: train. Epoch: 19. Loss: 0.08727477490901947\n",
      "19 train 19 240\n",
      "Phase: train. Epoch: 19. Loss: 0.10574613511562347\n",
      "19 train 20 252\n",
      "Phase: train. Epoch: 19. Loss: 0.07939013093709946\n",
      "19 train 21 264\n",
      "Phase: train. Epoch: 19. Loss: 0.08827243745326996\n",
      "19 train 22 276\n",
      "Phase: train. Epoch: 19. Loss: 0.10214906930923462\n",
      "19 train 23 288\n",
      "Phase: train. Epoch: 19. Loss: 0.07032276690006256\n",
      "19 train 24 300\n",
      "Phase: train. Epoch: 19. Loss: 0.0818653404712677\n",
      "19 train 25 312\n",
      "Phase: train. Epoch: 19. Loss: 0.10868138074874878\n",
      "19 train 26 324\n",
      "Phase: train. Epoch: 19. Loss: 0.09113843739032745\n",
      "19 train 27 336\n",
      "Phase: train. Epoch: 19. Loss: 0.08666013181209564\n",
      "19 train 28 348\n",
      "Phase: train. Epoch: 19. Loss: 0.10159578919410706\n",
      "19 train 29 360\n",
      "Phase: train. Epoch: 19. Loss: 0.08235500752925873\n",
      "19 train 30 372\n",
      "Phase: train. Epoch: 19. Loss: 0.09304120391607285\n",
      "19 train 31 384\n",
      "Phase: train. Epoch: 19. Loss: 0.07824859023094177\n",
      "19 train 32 396\n",
      "Phase: train. Epoch: 19. Loss: 0.06345100700855255\n",
      "19 train 33 408\n",
      "Phase: train. Epoch: 19. Loss: 0.08645212650299072\n",
      "19 train 34 420\n",
      "Phase: train. Epoch: 19. Loss: 0.11087241768836975\n",
      "19 train 35 432\n",
      "Phase: train. Epoch: 19. Loss: 0.11471445858478546\n",
      "19 train 36 444\n",
      "Phase: train. Epoch: 19. Loss: 0.1421196460723877\n",
      "19 train 37 456\n",
      "Phase: train. Epoch: 19. Loss: 0.0715378150343895\n",
      "19 train 38 468\n",
      "Phase: train. Epoch: 19. Loss: 0.07527294754981995\n",
      "19 train 39 480\n",
      "Phase: train. Epoch: 19. Loss: 0.07525479793548584\n",
      "19 train 40 492\n",
      "Phase: train. Epoch: 19. Loss: 0.07779732346534729\n",
      "19 train 41 504\n",
      "Phase: train. Epoch: 19. Loss: 0.09566982835531235\n",
      "19 train 42 516\n",
      "Phase: train. Epoch: 19. Loss: 0.12058242410421371\n",
      "19 train 43 528\n",
      "Phase: train. Epoch: 19. Loss: 0.10590197890996933\n",
      "19 train 44 540\n",
      "Phase: train. Epoch: 19. Loss: 0.10103821009397507\n",
      "19 train 45 552\n",
      "Phase: train. Epoch: 19. Loss: 0.06595363467931747\n",
      "19 train 46 564\n",
      "Phase: train. Epoch: 19. Loss: 0.07686407119035721\n",
      "19 train 47 576\n",
      "Phase: train. Epoch: 19. Loss: 0.10828091949224472\n",
      "19 train 48 588\n",
      "Phase: train. Epoch: 19. Loss: 0.07346443831920624\n",
      "19 train 49 600\n",
      "Phase: train. Epoch: 19. Loss: 0.09282368421554565\n",
      "19 train 50 612\n",
      "Phase: train. Epoch: 19. Loss: 0.11117763817310333\n",
      "19 train 51 624\n",
      "Phase: train. Epoch: 19. Loss: 0.06746480613946915\n",
      "19 train 52 636\n",
      "Phase: train. Epoch: 19. Loss: 0.07564514130353928\n",
      "19 train 53 648\n",
      "Phase: train. Epoch: 19. Loss: 0.08489924669265747\n",
      "19 train 54 660\n",
      "Phase: train. Epoch: 19. Loss: 0.11375907063484192\n",
      "19 train 55 672\n",
      "Phase: train. Epoch: 19. Loss: 0.07861342281103134\n",
      "19 train 56 684\n",
      "Phase: train. Epoch: 19. Loss: 0.1016799658536911\n",
      "19 train 57 696\n",
      "Phase: train. Epoch: 19. Loss: 0.10329395532608032\n",
      "19 train 58 708\n",
      "Phase: train. Epoch: 19. Loss: 0.07339181005954742\n",
      "19 train 59 720\n",
      "Phase: train. Epoch: 19. Loss: 0.09499943256378174\n",
      "19 train 60 732\n",
      "Phase: train. Epoch: 19. Loss: 0.10527651011943817\n",
      "19 train 61 744\n",
      "Phase: train. Epoch: 19. Loss: 0.08130784332752228\n",
      "19 train 62 751\n",
      "Phase: train. Epoch: 19. Loss: 0.06724821031093597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 val 0 763\n",
      "Phase: val. Epoch: 19. Loss: 0.07955050468444824\n",
      "19 val 1 775\n",
      "Phase: val. Epoch: 19. Loss: 0.08064283430576324\n",
      "19 val 2 787\n",
      "Phase: val. Epoch: 19. Loss: 0.10316593945026398\n",
      "19 val 3 799\n",
      "Phase: val. Epoch: 19. Loss: 0.10776315629482269\n",
      "19 val 4 811\n",
      "Phase: val. Epoch: 19. Loss: 0.10056126117706299\n",
      "19 val 5 823\n",
      "Phase: val. Epoch: 19. Loss: 0.07984621822834015\n",
      "19 val 6 835\n",
      "Phase: val. Epoch: 19. Loss: 0.08478492498397827\n",
      "19 val 7 847\n",
      "Phase: val. Epoch: 19. Loss: 0.07754353433847427\n",
      "19 val 8 859\n",
      "Phase: val. Epoch: 19. Loss: 0.07009901106357574\n",
      "19 val 9 871\n",
      "Phase: val. Epoch: 19. Loss: 0.09138791263103485\n",
      "19 val 10 883\n",
      "Phase: val. Epoch: 19. Loss: 0.11257050931453705\n",
      "19 val 11 884\n",
      "Phase: val. Epoch: 19. Loss: 0.031073033809661865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 train 0 12\n",
      "Phase: train. Epoch: 20. Loss: 0.10707354545593262\n",
      "20 train 1 24\n",
      "Phase: train. Epoch: 20. Loss: 0.08382990211248398\n",
      "20 train 2 36\n",
      "Phase: train. Epoch: 20. Loss: 0.11062949150800705\n",
      "20 train 3 48\n",
      "Phase: train. Epoch: 20. Loss: 0.10285697877407074\n",
      "20 train 4 60\n",
      "Phase: train. Epoch: 20. Loss: 0.072357177734375\n",
      "20 train 5 72\n",
      "Phase: train. Epoch: 20. Loss: 0.0840674564242363\n",
      "20 train 6 84\n",
      "Phase: train. Epoch: 20. Loss: 0.08943387120962143\n",
      "20 train 7 96\n",
      "Phase: train. Epoch: 20. Loss: 0.08721031993627548\n",
      "20 train 8 108\n",
      "Phase: train. Epoch: 20. Loss: 0.10128355026245117\n",
      "20 train 9 120\n",
      "Phase: train. Epoch: 20. Loss: 0.07473476231098175\n",
      "20 train 10 132\n",
      "Phase: train. Epoch: 20. Loss: 0.09065382182598114\n",
      "20 train 11 144\n",
      "Phase: train. Epoch: 20. Loss: 0.06782795488834381\n",
      "20 train 12 156\n",
      "Phase: train. Epoch: 20. Loss: 0.1024952381849289\n",
      "20 train 13 168\n",
      "Phase: train. Epoch: 20. Loss: 0.09576267004013062\n",
      "20 train 14 180\n",
      "Phase: train. Epoch: 20. Loss: 0.11438572406768799\n",
      "20 train 15 192\n",
      "Phase: train. Epoch: 20. Loss: 0.07712175697088242\n",
      "20 train 16 204\n",
      "Phase: train. Epoch: 20. Loss: 0.10543827712535858\n",
      "20 train 17 216\n",
      "Phase: train. Epoch: 20. Loss: 0.10995998978614807\n",
      "20 train 18 228\n",
      "Phase: train. Epoch: 20. Loss: 0.09000056982040405\n",
      "20 train 19 240\n",
      "Phase: train. Epoch: 20. Loss: 0.08507289737462997\n",
      "20 train 20 252\n",
      "Phase: train. Epoch: 20. Loss: 0.08331923186779022\n",
      "20 train 21 264\n",
      "Phase: train. Epoch: 20. Loss: 0.11161236464977264\n",
      "20 train 22 276\n",
      "Phase: train. Epoch: 20. Loss: 0.06912069022655487\n",
      "20 train 23 288\n",
      "Phase: train. Epoch: 20. Loss: 0.09212376177310944\n",
      "20 train 24 300\n",
      "Phase: train. Epoch: 20. Loss: 0.08614636957645416\n",
      "20 train 25 312\n",
      "Phase: train. Epoch: 20. Loss: 0.09638705104589462\n",
      "20 train 26 324\n",
      "Phase: train. Epoch: 20. Loss: 0.10082101076841354\n",
      "20 train 27 336\n",
      "Phase: train. Epoch: 20. Loss: 0.12219972908496857\n",
      "20 train 28 348\n",
      "Phase: train. Epoch: 20. Loss: 0.0780128687620163\n",
      "20 train 29 360\n",
      "Phase: train. Epoch: 20. Loss: 0.08379632234573364\n",
      "20 train 30 372\n",
      "Phase: train. Epoch: 20. Loss: 0.1043340414762497\n",
      "20 train 31 384\n",
      "Phase: train. Epoch: 20. Loss: 0.09890761971473694\n",
      "20 train 32 396\n",
      "Phase: train. Epoch: 20. Loss: 0.06657953560352325\n",
      "20 train 33 408\n",
      "Phase: train. Epoch: 20. Loss: 0.08189782500267029\n",
      "20 train 34 420\n",
      "Phase: train. Epoch: 20. Loss: 0.0658428817987442\n",
      "20 train 35 432\n",
      "Phase: train. Epoch: 20. Loss: 0.09415701031684875\n",
      "20 train 36 444\n",
      "Phase: train. Epoch: 20. Loss: 0.0703485906124115\n",
      "20 train 37 456\n",
      "Phase: train. Epoch: 20. Loss: 0.07852828502655029\n",
      "20 train 38 468\n",
      "Phase: train. Epoch: 20. Loss: 0.07028371840715408\n",
      "20 train 39 480\n",
      "Phase: train. Epoch: 20. Loss: 0.0882941484451294\n",
      "20 train 40 492\n",
      "Phase: train. Epoch: 20. Loss: 0.08106902986764908\n",
      "20 train 41 504\n",
      "Phase: train. Epoch: 20. Loss: 0.09285178035497665\n",
      "20 train 42 516\n",
      "Phase: train. Epoch: 20. Loss: 0.09778374433517456\n",
      "20 train 43 528\n",
      "Phase: train. Epoch: 20. Loss: 0.07871618121862411\n",
      "20 train 44 540\n",
      "Phase: train. Epoch: 20. Loss: 0.07589620351791382\n",
      "20 train 45 552\n",
      "Phase: train. Epoch: 20. Loss: 0.07677791267633438\n",
      "20 train 46 564\n",
      "Phase: train. Epoch: 20. Loss: 0.08765722811222076\n",
      "20 train 47 576\n",
      "Phase: train. Epoch: 20. Loss: 0.09548662602901459\n",
      "20 train 48 588\n",
      "Phase: train. Epoch: 20. Loss: 0.08329011499881744\n",
      "20 train 49 600\n",
      "Phase: train. Epoch: 20. Loss: 0.09680682420730591\n",
      "20 train 50 612\n",
      "Phase: train. Epoch: 20. Loss: 0.09385573863983154\n",
      "20 train 51 624\n",
      "Phase: train. Epoch: 20. Loss: 0.09170150011777878\n",
      "20 train 52 636\n",
      "Phase: train. Epoch: 20. Loss: 0.10740941017866135\n",
      "20 train 53 648\n",
      "Phase: train. Epoch: 20. Loss: 0.10328756272792816\n",
      "20 train 54 660\n",
      "Phase: train. Epoch: 20. Loss: 0.07936970889568329\n",
      "20 train 55 672\n",
      "Phase: train. Epoch: 20. Loss: 0.0848744660615921\n",
      "20 train 56 684\n",
      "Phase: train. Epoch: 20. Loss: 0.08489064872264862\n",
      "20 train 57 696\n",
      "Phase: train. Epoch: 20. Loss: 0.08206120133399963\n",
      "20 train 58 708\n",
      "Phase: train. Epoch: 20. Loss: 0.10413278639316559\n",
      "20 train 59 720\n",
      "Phase: train. Epoch: 20. Loss: 0.10875820368528366\n",
      "20 train 60 732\n",
      "Phase: train. Epoch: 20. Loss: 0.07312487065792084\n",
      "20 train 61 744\n",
      "Phase: train. Epoch: 20. Loss: 0.08635450899600983\n",
      "20 train 62 751\n",
      "Phase: train. Epoch: 20. Loss: 0.10222750157117844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 val 0 763\n",
      "Phase: val. Epoch: 20. Loss: 0.1000610738992691\n",
      "20 val 1 775\n",
      "Phase: val. Epoch: 20. Loss: 0.09900205582380295\n",
      "20 val 2 787\n",
      "Phase: val. Epoch: 20. Loss: 0.07452167570590973\n",
      "20 val 3 799\n",
      "Phase: val. Epoch: 20. Loss: 0.10595810413360596\n",
      "20 val 4 811\n",
      "Phase: val. Epoch: 20. Loss: 0.05038893222808838\n",
      "20 val 5 823\n",
      "Phase: val. Epoch: 20. Loss: 0.06336501240730286\n",
      "20 val 6 835\n",
      "Phase: val. Epoch: 20. Loss: 0.06489463895559311\n",
      "20 val 7 847\n",
      "Phase: val. Epoch: 20. Loss: 0.10173432528972626\n",
      "20 val 8 859\n",
      "Phase: val. Epoch: 20. Loss: 0.06365245580673218\n",
      "20 val 9 871\n",
      "Phase: val. Epoch: 20. Loss: 0.09702140092849731\n",
      "20 val 10 883\n",
      "Phase: val. Epoch: 20. Loss: 0.1051313728094101\n",
      "20 val 11 884\n",
      "Phase: val. Epoch: 20. Loss: 0.23877131938934326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 train 0 12\n",
      "Phase: train. Epoch: 21. Loss: 0.07733052968978882\n",
      "21 train 1 24\n",
      "Phase: train. Epoch: 21. Loss: 0.08142082393169403\n",
      "21 train 2 36\n",
      "Phase: train. Epoch: 21. Loss: 0.08434902131557465\n",
      "21 train 3 48\n",
      "Phase: train. Epoch: 21. Loss: 0.06135011836886406\n",
      "21 train 4 60\n",
      "Phase: train. Epoch: 21. Loss: 0.08394738286733627\n",
      "21 train 5 72\n",
      "Phase: train. Epoch: 21. Loss: 0.07949980348348618\n",
      "21 train 6 84\n",
      "Phase: train. Epoch: 21. Loss: 0.07856693863868713\n",
      "21 train 7 96\n",
      "Phase: train. Epoch: 21. Loss: 0.06589827686548233\n",
      "21 train 8 108\n",
      "Phase: train. Epoch: 21. Loss: 0.09086185693740845\n",
      "21 train 9 120\n",
      "Phase: train. Epoch: 21. Loss: 0.07559016346931458\n",
      "21 train 10 132\n",
      "Phase: train. Epoch: 21. Loss: 0.10612915456295013\n",
      "21 train 11 144\n",
      "Phase: train. Epoch: 21. Loss: 0.08376643061637878\n",
      "21 train 12 156\n",
      "Phase: train. Epoch: 21. Loss: 0.0932372659444809\n",
      "21 train 13 168\n",
      "Phase: train. Epoch: 21. Loss: 0.10016711801290512\n",
      "21 train 14 180\n",
      "Phase: train. Epoch: 21. Loss: 0.08690036833286285\n",
      "21 train 15 192\n",
      "Phase: train. Epoch: 21. Loss: 0.13437974452972412\n",
      "21 train 16 204\n",
      "Phase: train. Epoch: 21. Loss: 0.13787305355072021\n",
      "21 train 17 216\n",
      "Phase: train. Epoch: 21. Loss: 0.0996720939874649\n",
      "21 train 18 228\n",
      "Phase: train. Epoch: 21. Loss: 0.06849519908428192\n",
      "21 train 19 240\n",
      "Phase: train. Epoch: 21. Loss: 0.0827876478433609\n",
      "21 train 20 252\n",
      "Phase: train. Epoch: 21. Loss: 0.08742259442806244\n",
      "21 train 21 264\n",
      "Phase: train. Epoch: 21. Loss: 0.10572628676891327\n",
      "21 train 22 276\n",
      "Phase: train. Epoch: 21. Loss: 0.10096035897731781\n",
      "21 train 23 288\n",
      "Phase: train. Epoch: 21. Loss: 0.0783563107252121\n",
      "21 train 24 300\n",
      "Phase: train. Epoch: 21. Loss: 0.07498465478420258\n",
      "21 train 25 312\n",
      "Phase: train. Epoch: 21. Loss: 0.12151448428630829\n",
      "21 train 26 324\n",
      "Phase: train. Epoch: 21. Loss: 0.07147251814603806\n",
      "21 train 27 336\n",
      "Phase: train. Epoch: 21. Loss: 0.07529553771018982\n",
      "21 train 28 348\n",
      "Phase: train. Epoch: 21. Loss: 0.092826709151268\n",
      "21 train 29 360\n",
      "Phase: train. Epoch: 21. Loss: 0.08846433460712433\n",
      "21 train 30 372\n",
      "Phase: train. Epoch: 21. Loss: 0.08772750943899155\n",
      "21 train 31 384\n",
      "Phase: train. Epoch: 21. Loss: 0.09648959338665009\n",
      "21 train 32 396\n",
      "Phase: train. Epoch: 21. Loss: 0.08446241915225983\n",
      "21 train 33 408\n",
      "Phase: train. Epoch: 21. Loss: 0.09096433967351913\n",
      "21 train 34 420\n",
      "Phase: train. Epoch: 21. Loss: 0.08428364247083664\n",
      "21 train 35 432\n",
      "Phase: train. Epoch: 21. Loss: 0.114541195333004\n",
      "21 train 36 444\n",
      "Phase: train. Epoch: 21. Loss: 0.09894625842571259\n",
      "21 train 37 456\n",
      "Phase: train. Epoch: 21. Loss: 0.09654756635427475\n",
      "21 train 38 468\n",
      "Phase: train. Epoch: 21. Loss: 0.08714743703603745\n",
      "21 train 39 480\n",
      "Phase: train. Epoch: 21. Loss: 0.09452911466360092\n",
      "21 train 40 492\n",
      "Phase: train. Epoch: 21. Loss: 0.1327832192182541\n",
      "21 train 41 504\n",
      "Phase: train. Epoch: 21. Loss: 0.07353510707616806\n",
      "21 train 42 516\n",
      "Phase: train. Epoch: 21. Loss: 0.06689178198575974\n",
      "21 train 43 528\n",
      "Phase: train. Epoch: 21. Loss: 0.08012355118989944\n",
      "21 train 44 540\n",
      "Phase: train. Epoch: 21. Loss: 0.08019988238811493\n",
      "21 train 45 552\n",
      "Phase: train. Epoch: 21. Loss: 0.11428379267454147\n",
      "21 train 46 564\n",
      "Phase: train. Epoch: 21. Loss: 0.08075394481420517\n",
      "21 train 47 576\n",
      "Phase: train. Epoch: 21. Loss: 0.07988807559013367\n",
      "21 train 48 588\n",
      "Phase: train. Epoch: 21. Loss: 0.07219304144382477\n",
      "21 train 49 600\n",
      "Phase: train. Epoch: 21. Loss: 0.09793418645858765\n",
      "21 train 50 612\n",
      "Phase: train. Epoch: 21. Loss: 0.07673396170139313\n",
      "21 train 51 624\n",
      "Phase: train. Epoch: 21. Loss: 0.07601328194141388\n",
      "21 train 52 636\n",
      "Phase: train. Epoch: 21. Loss: 0.1202380508184433\n",
      "21 train 53 648\n",
      "Phase: train. Epoch: 21. Loss: 0.10826344788074493\n",
      "21 train 54 660\n",
      "Phase: train. Epoch: 21. Loss: 0.07745024561882019\n",
      "21 train 55 672\n",
      "Phase: train. Epoch: 21. Loss: 0.08890768885612488\n",
      "21 train 56 684\n",
      "Phase: train. Epoch: 21. Loss: 0.07970322668552399\n",
      "21 train 57 696\n",
      "Phase: train. Epoch: 21. Loss: 0.08576571196317673\n",
      "21 train 58 708\n",
      "Phase: train. Epoch: 21. Loss: 0.07389308512210846\n",
      "21 train 59 720\n",
      "Phase: train. Epoch: 21. Loss: 0.0792614221572876\n",
      "21 train 60 732\n",
      "Phase: train. Epoch: 21. Loss: 0.08499410003423691\n",
      "21 train 61 744\n",
      "Phase: train. Epoch: 21. Loss: 0.09373301267623901\n",
      "21 train 62 751\n",
      "Phase: train. Epoch: 21. Loss: 0.11825933307409286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 val 0 763\n",
      "Phase: val. Epoch: 21. Loss: 0.09991659224033356\n",
      "21 val 1 775\n",
      "Phase: val. Epoch: 21. Loss: 0.08547578752040863\n",
      "21 val 2 787\n",
      "Phase: val. Epoch: 21. Loss: 0.07466995716094971\n",
      "21 val 3 799\n",
      "Phase: val. Epoch: 21. Loss: 0.09156956523656845\n",
      "21 val 4 811\n",
      "Phase: val. Epoch: 21. Loss: 0.08889635652303696\n",
      "21 val 5 823\n",
      "Phase: val. Epoch: 21. Loss: 0.08749159425497055\n",
      "21 val 6 835\n",
      "Phase: val. Epoch: 21. Loss: 0.10461810231208801\n",
      "21 val 7 847\n",
      "Phase: val. Epoch: 21. Loss: 0.06622108072042465\n",
      "21 val 8 859\n",
      "Phase: val. Epoch: 21. Loss: 0.06634537130594254\n",
      "21 val 9 871\n",
      "Phase: val. Epoch: 21. Loss: 0.07996761798858643\n",
      "21 val 10 883\n",
      "Phase: val. Epoch: 21. Loss: 0.08218389749526978\n",
      "21 val 11 884\n",
      "Phase: val. Epoch: 21. Loss: 0.16687937080860138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 train 0 12\n",
      "Phase: train. Epoch: 22. Loss: 0.07695114612579346\n",
      "22 train 1 24\n",
      "Phase: train. Epoch: 22. Loss: 0.07265695929527283\n",
      "22 train 2 36\n",
      "Phase: train. Epoch: 22. Loss: 0.09570270031690598\n",
      "22 train 3 48\n",
      "Phase: train. Epoch: 22. Loss: 0.06749361753463745\n",
      "22 train 4 60\n",
      "Phase: train. Epoch: 22. Loss: 0.10863994807004929\n",
      "22 train 5 72\n",
      "Phase: train. Epoch: 22. Loss: 0.08771362155675888\n",
      "22 train 6 84\n",
      "Phase: train. Epoch: 22. Loss: 0.09487106651067734\n",
      "22 train 7 96\n",
      "Phase: train. Epoch: 22. Loss: 0.07643391191959381\n",
      "22 train 8 108\n",
      "Phase: train. Epoch: 22. Loss: 0.112718366086483\n",
      "22 train 9 120\n",
      "Phase: train. Epoch: 22. Loss: 0.10408284515142441\n",
      "22 train 10 132\n",
      "Phase: train. Epoch: 22. Loss: 0.07165645807981491\n",
      "22 train 11 144\n",
      "Phase: train. Epoch: 22. Loss: 0.07892526686191559\n",
      "22 train 12 156\n",
      "Phase: train. Epoch: 22. Loss: 0.08784471452236176\n",
      "22 train 13 168\n",
      "Phase: train. Epoch: 22. Loss: 0.07629597187042236\n",
      "22 train 14 180\n",
      "Phase: train. Epoch: 22. Loss: 0.07982736080884933\n",
      "22 train 15 192\n",
      "Phase: train. Epoch: 22. Loss: 0.10740232467651367\n",
      "22 train 16 204\n",
      "Phase: train. Epoch: 22. Loss: 0.08537130802869797\n",
      "22 train 17 216\n",
      "Phase: train. Epoch: 22. Loss: 0.09840919822454453\n",
      "22 train 18 228\n",
      "Phase: train. Epoch: 22. Loss: 0.08060810714960098\n",
      "22 train 19 240\n",
      "Phase: train. Epoch: 22. Loss: 0.09784112870693207\n",
      "22 train 20 252\n",
      "Phase: train. Epoch: 22. Loss: 0.07504048198461533\n",
      "22 train 21 264\n",
      "Phase: train. Epoch: 22. Loss: 0.08157621324062347\n",
      "22 train 22 276\n",
      "Phase: train. Epoch: 22. Loss: 0.12976738810539246\n",
      "22 train 23 288\n",
      "Phase: train. Epoch: 22. Loss: 0.09922657907009125\n",
      "22 train 24 300\n",
      "Phase: train. Epoch: 22. Loss: 0.09327034652233124\n",
      "22 train 25 312\n",
      "Phase: train. Epoch: 22. Loss: 0.0928860530257225\n",
      "22 train 26 324\n",
      "Phase: train. Epoch: 22. Loss: 0.08349277824163437\n",
      "22 train 27 336\n",
      "Phase: train. Epoch: 22. Loss: 0.07742202281951904\n",
      "22 train 28 348\n",
      "Phase: train. Epoch: 22. Loss: 0.09028677642345428\n",
      "22 train 29 360\n",
      "Phase: train. Epoch: 22. Loss: 0.10032318532466888\n",
      "22 train 30 372\n",
      "Phase: train. Epoch: 22. Loss: 0.10496106743812561\n",
      "22 train 31 384\n",
      "Phase: train. Epoch: 22. Loss: 0.10773134231567383\n",
      "22 train 32 396\n",
      "Phase: train. Epoch: 22. Loss: 0.08124562352895737\n",
      "22 train 33 408\n",
      "Phase: train. Epoch: 22. Loss: 0.10316194593906403\n",
      "22 train 34 420\n",
      "Phase: train. Epoch: 22. Loss: 0.06908441334962845\n",
      "22 train 35 432\n",
      "Phase: train. Epoch: 22. Loss: 0.08656241744756699\n",
      "22 train 36 444\n",
      "Phase: train. Epoch: 22. Loss: 0.07547734677791595\n",
      "22 train 37 456\n",
      "Phase: train. Epoch: 22. Loss: 0.09668058156967163\n",
      "22 train 38 468\n",
      "Phase: train. Epoch: 22. Loss: 0.08639201521873474\n",
      "22 train 39 480\n",
      "Phase: train. Epoch: 22. Loss: 0.0640466958284378\n",
      "22 train 40 492\n",
      "Phase: train. Epoch: 22. Loss: 0.10605119913816452\n",
      "22 train 41 504\n",
      "Phase: train. Epoch: 22. Loss: 0.08093427866697311\n",
      "22 train 42 516\n",
      "Phase: train. Epoch: 22. Loss: 0.09941719472408295\n",
      "22 train 43 528\n",
      "Phase: train. Epoch: 22. Loss: 0.07056507468223572\n",
      "22 train 44 540\n",
      "Phase: train. Epoch: 22. Loss: 0.08435232937335968\n",
      "22 train 45 552\n",
      "Phase: train. Epoch: 22. Loss: 0.09742724150419235\n",
      "22 train 46 564\n",
      "Phase: train. Epoch: 22. Loss: 0.10829657316207886\n",
      "22 train 47 576\n",
      "Phase: train. Epoch: 22. Loss: 0.07759164273738861\n",
      "22 train 48 588\n",
      "Phase: train. Epoch: 22. Loss: 0.09843458235263824\n",
      "22 train 49 600\n",
      "Phase: train. Epoch: 22. Loss: 0.06965374946594238\n",
      "22 train 50 612\n",
      "Phase: train. Epoch: 22. Loss: 0.10258059203624725\n",
      "22 train 51 624\n",
      "Phase: train. Epoch: 22. Loss: 0.0782138854265213\n",
      "22 train 52 636\n",
      "Phase: train. Epoch: 22. Loss: 0.08131935447454453\n",
      "22 train 53 648\n",
      "Phase: train. Epoch: 22. Loss: 0.10524927824735641\n",
      "22 train 54 660\n",
      "Phase: train. Epoch: 22. Loss: 0.08564476668834686\n",
      "22 train 55 672\n",
      "Phase: train. Epoch: 22. Loss: 0.1231965646147728\n",
      "22 train 56 684\n",
      "Phase: train. Epoch: 22. Loss: 0.09355568885803223\n",
      "22 train 57 696\n",
      "Phase: train. Epoch: 22. Loss: 0.07976092398166656\n",
      "22 train 58 708\n",
      "Phase: train. Epoch: 22. Loss: 0.09923594444990158\n",
      "22 train 59 720\n",
      "Phase: train. Epoch: 22. Loss: 0.0757477805018425\n",
      "22 train 60 732\n",
      "Phase: train. Epoch: 22. Loss: 0.09100606292486191\n",
      "22 train 61 744\n",
      "Phase: train. Epoch: 22. Loss: 0.07887062430381775\n",
      "22 train 62 751\n",
      "Phase: train. Epoch: 22. Loss: 0.10958695411682129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 val 0 763\n",
      "Phase: val. Epoch: 22. Loss: 0.08086273819208145\n",
      "22 val 1 775\n",
      "Phase: val. Epoch: 22. Loss: 0.05999407172203064\n",
      "22 val 2 787\n",
      "Phase: val. Epoch: 22. Loss: 0.10917111486196518\n",
      "22 val 3 799\n",
      "Phase: val. Epoch: 22. Loss: 0.08927534520626068\n",
      "22 val 4 811\n",
      "Phase: val. Epoch: 22. Loss: 0.0860651433467865\n",
      "22 val 5 823\n",
      "Phase: val. Epoch: 22. Loss: 0.08558498322963715\n",
      "22 val 6 835\n",
      "Phase: val. Epoch: 22. Loss: 0.06688087433576584\n",
      "22 val 7 847\n",
      "Phase: val. Epoch: 22. Loss: 0.08799934387207031\n",
      "22 val 8 859\n",
      "Phase: val. Epoch: 22. Loss: 0.09392811357975006\n",
      "22 val 9 871\n",
      "Phase: val. Epoch: 22. Loss: 0.0767943263053894\n",
      "22 val 10 883\n",
      "Phase: val. Epoch: 22. Loss: 0.08546045422554016\n",
      "22 val 11 884\n",
      "Phase: val. Epoch: 22. Loss: 0.0645245611667633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 train 0 12\n",
      "Phase: train. Epoch: 23. Loss: 0.0952690988779068\n",
      "23 train 1 24\n",
      "Phase: train. Epoch: 23. Loss: 0.07508575171232224\n",
      "23 train 2 36\n",
      "Phase: train. Epoch: 23. Loss: 0.09661124646663666\n",
      "23 train 3 48\n",
      "Phase: train. Epoch: 23. Loss: 0.08462691307067871\n",
      "23 train 4 60\n",
      "Phase: train. Epoch: 23. Loss: 0.06955409795045853\n",
      "23 train 5 72\n",
      "Phase: train. Epoch: 23. Loss: 0.10268281400203705\n",
      "23 train 6 84\n",
      "Phase: train. Epoch: 23. Loss: 0.08566032350063324\n",
      "23 train 7 96\n",
      "Phase: train. Epoch: 23. Loss: 0.11885597556829453\n",
      "23 train 8 108\n",
      "Phase: train. Epoch: 23. Loss: 0.08022686094045639\n",
      "23 train 9 120\n",
      "Phase: train. Epoch: 23. Loss: 0.07271216064691544\n",
      "23 train 10 132\n",
      "Phase: train. Epoch: 23. Loss: 0.08745231479406357\n",
      "23 train 11 144\n",
      "Phase: train. Epoch: 23. Loss: 0.08800999820232391\n",
      "23 train 12 156\n",
      "Phase: train. Epoch: 23. Loss: 0.09043142199516296\n",
      "23 train 13 168\n",
      "Phase: train. Epoch: 23. Loss: 0.06114759296178818\n",
      "23 train 14 180\n",
      "Phase: train. Epoch: 23. Loss: 0.06653934717178345\n",
      "23 train 15 192\n",
      "Phase: train. Epoch: 23. Loss: 0.07344523817300797\n",
      "23 train 16 204\n",
      "Phase: train. Epoch: 23. Loss: 0.12098115682601929\n",
      "23 train 17 216\n",
      "Phase: train. Epoch: 23. Loss: 0.078896664083004\n",
      "23 train 18 228\n",
      "Phase: train. Epoch: 23. Loss: 0.08564253896474838\n",
      "23 train 19 240\n",
      "Phase: train. Epoch: 23. Loss: 0.07542244344949722\n",
      "23 train 20 252\n",
      "Phase: train. Epoch: 23. Loss: 0.08515971899032593\n",
      "23 train 21 264\n",
      "Phase: train. Epoch: 23. Loss: 0.07595284283161163\n",
      "23 train 22 276\n",
      "Phase: train. Epoch: 23. Loss: 0.09284339845180511\n",
      "23 train 23 288\n",
      "Phase: train. Epoch: 23. Loss: 0.09944476187229156\n",
      "23 train 24 300\n",
      "Phase: train. Epoch: 23. Loss: 0.08081410080194473\n",
      "23 train 25 312\n",
      "Phase: train. Epoch: 23. Loss: 0.09487594664096832\n",
      "23 train 26 324\n",
      "Phase: train. Epoch: 23. Loss: 0.09400813281536102\n",
      "23 train 27 336\n",
      "Phase: train. Epoch: 23. Loss: 0.1048138365149498\n",
      "23 train 28 348\n",
      "Phase: train. Epoch: 23. Loss: 0.07777640223503113\n",
      "23 train 29 360\n",
      "Phase: train. Epoch: 23. Loss: 0.09798870980739594\n",
      "23 train 30 372\n",
      "Phase: train. Epoch: 23. Loss: 0.08647069334983826\n",
      "23 train 31 384\n",
      "Phase: train. Epoch: 23. Loss: 0.07286091893911362\n",
      "23 train 32 396\n",
      "Phase: train. Epoch: 23. Loss: 0.09326835721731186\n",
      "23 train 33 408\n",
      "Phase: train. Epoch: 23. Loss: 0.0723467618227005\n",
      "23 train 34 420\n",
      "Phase: train. Epoch: 23. Loss: 0.10723541676998138\n",
      "23 train 35 432\n",
      "Phase: train. Epoch: 23. Loss: 0.06967954337596893\n",
      "23 train 36 444\n",
      "Phase: train. Epoch: 23. Loss: 0.08499275147914886\n",
      "23 train 37 456\n",
      "Phase: train. Epoch: 23. Loss: 0.07249514758586884\n",
      "23 train 38 468\n",
      "Phase: train. Epoch: 23. Loss: 0.09437225759029388\n",
      "23 train 39 480\n",
      "Phase: train. Epoch: 23. Loss: 0.10235743969678879\n",
      "23 train 40 492\n",
      "Phase: train. Epoch: 23. Loss: 0.10655266046524048\n",
      "23 train 41 504\n",
      "Phase: train. Epoch: 23. Loss: 0.06989893317222595\n",
      "23 train 42 516\n",
      "Phase: train. Epoch: 23. Loss: 0.10222690552473068\n",
      "23 train 43 528\n",
      "Phase: train. Epoch: 23. Loss: 0.09642487019300461\n",
      "23 train 44 540\n",
      "Phase: train. Epoch: 23. Loss: 0.11343594640493393\n",
      "23 train 45 552\n",
      "Phase: train. Epoch: 23. Loss: 0.0884341225028038\n",
      "23 train 46 564\n",
      "Phase: train. Epoch: 23. Loss: 0.10892304033041\n",
      "23 train 47 576\n",
      "Phase: train. Epoch: 23. Loss: 0.08457078784704208\n",
      "23 train 48 588\n",
      "Phase: train. Epoch: 23. Loss: 0.08304700255393982\n",
      "23 train 49 600\n",
      "Phase: train. Epoch: 23. Loss: 0.08358582109212875\n",
      "23 train 50 612\n",
      "Phase: train. Epoch: 23. Loss: 0.07506398111581802\n",
      "23 train 51 624\n",
      "Phase: train. Epoch: 23. Loss: 0.09575682878494263\n",
      "23 train 52 636\n",
      "Phase: train. Epoch: 23. Loss: 0.08655476570129395\n",
      "23 train 53 648\n",
      "Phase: train. Epoch: 23. Loss: 0.10679116100072861\n",
      "23 train 54 660\n",
      "Phase: train. Epoch: 23. Loss: 0.08526264876127243\n",
      "23 train 55 672\n",
      "Phase: train. Epoch: 23. Loss: 0.06955382227897644\n",
      "23 train 56 684\n",
      "Phase: train. Epoch: 23. Loss: 0.10475917160511017\n",
      "23 train 57 696\n",
      "Phase: train. Epoch: 23. Loss: 0.09304963052272797\n",
      "23 train 58 708\n",
      "Phase: train. Epoch: 23. Loss: 0.12589460611343384\n",
      "23 train 59 720\n",
      "Phase: train. Epoch: 23. Loss: 0.09289084374904633\n",
      "23 train 60 732\n",
      "Phase: train. Epoch: 23. Loss: 0.1308297961950302\n",
      "23 train 61 744\n",
      "Phase: train. Epoch: 23. Loss: 0.11058910191059113\n",
      "23 train 62 751\n",
      "Phase: train. Epoch: 23. Loss: 0.09184585511684418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 val 0 763\n",
      "Phase: val. Epoch: 23. Loss: 0.07694463431835175\n",
      "23 val 1 775\n",
      "Phase: val. Epoch: 23. Loss: 0.09882169961929321\n",
      "23 val 2 787\n",
      "Phase: val. Epoch: 23. Loss: 0.07458774745464325\n",
      "23 val 3 799\n",
      "Phase: val. Epoch: 23. Loss: 0.06855320930480957\n",
      "23 val 4 811\n",
      "Phase: val. Epoch: 23. Loss: 0.08295519649982452\n",
      "23 val 5 823\n",
      "Phase: val. Epoch: 23. Loss: 0.08617673814296722\n",
      "23 val 6 835\n",
      "Phase: val. Epoch: 23. Loss: 0.09793542325496674\n",
      "23 val 7 847\n",
      "Phase: val. Epoch: 23. Loss: 0.11365766823291779\n",
      "23 val 8 859\n",
      "Phase: val. Epoch: 23. Loss: 0.06546355783939362\n",
      "23 val 9 871\n",
      "Phase: val. Epoch: 23. Loss: 0.09360890090465546\n",
      "23 val 10 883\n",
      "Phase: val. Epoch: 23. Loss: 0.08798706531524658\n",
      "23 val 11 884\n",
      "Phase: val. Epoch: 23. Loss: 0.09865155816078186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 train 0 12\n",
      "Phase: train. Epoch: 24. Loss: 0.07763555645942688\n",
      "24 train 1 24\n",
      "Phase: train. Epoch: 24. Loss: 0.09289313852787018\n",
      "24 train 2 36\n",
      "Phase: train. Epoch: 24. Loss: 0.07566645741462708\n",
      "24 train 3 48\n",
      "Phase: train. Epoch: 24. Loss: 0.09712564945220947\n",
      "24 train 4 60\n",
      "Phase: train. Epoch: 24. Loss: 0.07535496354103088\n",
      "24 train 5 72\n",
      "Phase: train. Epoch: 24. Loss: 0.09044532477855682\n",
      "24 train 6 84\n",
      "Phase: train. Epoch: 24. Loss: 0.07103362679481506\n",
      "24 train 7 96\n",
      "Phase: train. Epoch: 24. Loss: 0.08907522261142731\n",
      "24 train 8 108\n",
      "Phase: train. Epoch: 24. Loss: 0.07598830759525299\n",
      "24 train 9 120\n",
      "Phase: train. Epoch: 24. Loss: 0.08114151656627655\n",
      "24 train 10 132\n",
      "Phase: train. Epoch: 24. Loss: 0.07484878599643707\n",
      "24 train 11 144\n",
      "Phase: train. Epoch: 24. Loss: 0.08844410628080368\n",
      "24 train 12 156\n",
      "Phase: train. Epoch: 24. Loss: 0.07661004364490509\n",
      "24 train 13 168\n",
      "Phase: train. Epoch: 24. Loss: 0.09076178073883057\n",
      "24 train 14 180\n",
      "Phase: train. Epoch: 24. Loss: 0.11373597383499146\n",
      "24 train 15 192\n",
      "Phase: train. Epoch: 24. Loss: 0.09481396526098251\n",
      "24 train 16 204\n",
      "Phase: train. Epoch: 24. Loss: 0.09758855402469635\n",
      "24 train 17 216\n",
      "Phase: train. Epoch: 24. Loss: 0.08675588667392731\n",
      "24 train 18 228\n",
      "Phase: train. Epoch: 24. Loss: 0.08334432542324066\n",
      "24 train 19 240\n",
      "Phase: train. Epoch: 24. Loss: 0.0883294939994812\n",
      "24 train 20 252\n",
      "Phase: train. Epoch: 24. Loss: 0.07911635935306549\n",
      "24 train 21 264\n",
      "Phase: train. Epoch: 24. Loss: 0.0952044278383255\n",
      "24 train 22 276\n",
      "Phase: train. Epoch: 24. Loss: 0.11904450505971909\n",
      "24 train 23 288\n",
      "Phase: train. Epoch: 24. Loss: 0.08541636914014816\n",
      "24 train 24 300\n",
      "Phase: train. Epoch: 24. Loss: 0.12054209411144257\n",
      "24 train 25 312\n",
      "Phase: train. Epoch: 24. Loss: 0.09892098605632782\n",
      "24 train 26 324\n",
      "Phase: train. Epoch: 24. Loss: 0.09988667815923691\n",
      "24 train 27 336\n",
      "Phase: train. Epoch: 24. Loss: 0.1183696836233139\n",
      "24 train 28 348\n",
      "Phase: train. Epoch: 24. Loss: 0.06781060993671417\n",
      "24 train 29 360\n",
      "Phase: train. Epoch: 24. Loss: 0.10594558715820312\n",
      "24 train 30 372\n",
      "Phase: train. Epoch: 24. Loss: 0.1423797607421875\n",
      "24 train 31 384\n",
      "Phase: train. Epoch: 24. Loss: 0.09997688233852386\n",
      "24 train 32 396\n",
      "Phase: train. Epoch: 24. Loss: 0.11620090901851654\n",
      "24 train 33 408\n",
      "Phase: train. Epoch: 24. Loss: 0.08852434158325195\n",
      "24 train 34 420\n",
      "Phase: train. Epoch: 24. Loss: 0.08129020780324936\n",
      "24 train 35 432\n",
      "Phase: train. Epoch: 24. Loss: 0.06830232590436935\n",
      "24 train 36 444\n",
      "Phase: train. Epoch: 24. Loss: 0.08849695324897766\n",
      "24 train 37 456\n",
      "Phase: train. Epoch: 24. Loss: 0.10223172605037689\n",
      "24 train 38 468\n",
      "Phase: train. Epoch: 24. Loss: 0.08717241138219833\n",
      "24 train 39 480\n",
      "Phase: train. Epoch: 24. Loss: 0.084174245595932\n",
      "24 train 40 492\n",
      "Phase: train. Epoch: 24. Loss: 0.10809902846813202\n",
      "24 train 41 504\n",
      "Phase: train. Epoch: 24. Loss: 0.08675235509872437\n",
      "24 train 42 516\n",
      "Phase: train. Epoch: 24. Loss: 0.08275274187326431\n",
      "24 train 43 528\n",
      "Phase: train. Epoch: 24. Loss: 0.08715082705020905\n",
      "24 train 44 540\n",
      "Phase: train. Epoch: 24. Loss: 0.09687608480453491\n",
      "24 train 45 552\n",
      "Phase: train. Epoch: 24. Loss: 0.07690417021512985\n",
      "24 train 46 564\n",
      "Phase: train. Epoch: 24. Loss: 0.07420957833528519\n",
      "24 train 47 576\n",
      "Phase: train. Epoch: 24. Loss: 0.10008524358272552\n",
      "24 train 48 588\n",
      "Phase: train. Epoch: 24. Loss: 0.08099213242530823\n",
      "24 train 49 600\n",
      "Phase: train. Epoch: 24. Loss: 0.08214718103408813\n",
      "24 train 50 612\n",
      "Phase: train. Epoch: 24. Loss: 0.12010188400745392\n",
      "24 train 51 624\n",
      "Phase: train. Epoch: 24. Loss: 0.08469575643539429\n",
      "24 train 52 636\n",
      "Phase: train. Epoch: 24. Loss: 0.08757879585027695\n",
      "24 train 53 648\n",
      "Phase: train. Epoch: 24. Loss: 0.11245071142911911\n",
      "24 train 54 660\n",
      "Phase: train. Epoch: 24. Loss: 0.07767878472805023\n",
      "24 train 55 672\n",
      "Phase: train. Epoch: 24. Loss: 0.10196565091609955\n",
      "24 train 56 684\n",
      "Phase: train. Epoch: 24. Loss: 0.06249594688415527\n",
      "24 train 57 696\n",
      "Phase: train. Epoch: 24. Loss: 0.08614526689052582\n",
      "24 train 58 708\n",
      "Phase: train. Epoch: 24. Loss: 0.07936837524175644\n",
      "24 train 59 720\n",
      "Phase: train. Epoch: 24. Loss: 0.09938701242208481\n",
      "24 train 60 732\n",
      "Phase: train. Epoch: 24. Loss: 0.07237138599157333\n",
      "24 train 61 744\n",
      "Phase: train. Epoch: 24. Loss: 0.07923728227615356\n",
      "24 train 62 751\n",
      "Phase: train. Epoch: 24. Loss: 0.09593812376260757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 val 0 763\n",
      "Phase: val. Epoch: 24. Loss: 0.09918790310621262\n",
      "24 val 1 775\n",
      "Phase: val. Epoch: 24. Loss: 0.12620730698108673\n",
      "24 val 2 787\n",
      "Phase: val. Epoch: 24. Loss: 0.0975976511836052\n",
      "24 val 3 799\n",
      "Phase: val. Epoch: 24. Loss: 0.07922031730413437\n",
      "24 val 4 811\n",
      "Phase: val. Epoch: 24. Loss: 0.07608455419540405\n",
      "24 val 5 823\n",
      "Phase: val. Epoch: 24. Loss: 0.10648451745510101\n",
      "24 val 6 835\n",
      "Phase: val. Epoch: 24. Loss: 0.07765747606754303\n",
      "24 val 7 847\n",
      "Phase: val. Epoch: 24. Loss: 0.06430329382419586\n",
      "24 val 8 859\n",
      "Phase: val. Epoch: 24. Loss: 0.07343854755163193\n",
      "24 val 9 871\n",
      "Phase: val. Epoch: 24. Loss: 0.07347796112298965\n",
      "24 val 10 883\n",
      "Phase: val. Epoch: 24. Loss: 0.06656341999769211\n",
      "24 val 11 884\n",
      "Phase: val. Epoch: 24. Loss: 0.05024030804634094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 train 0 12\n",
      "Phase: train. Epoch: 25. Loss: 0.11372130364179611\n",
      "25 train 1 24\n",
      "Phase: train. Epoch: 25. Loss: 0.11105813831090927\n",
      "25 train 2 36\n",
      "Phase: train. Epoch: 25. Loss: 0.0877426415681839\n",
      "25 train 3 48\n",
      "Phase: train. Epoch: 25. Loss: 0.09542956203222275\n",
      "25 train 4 60\n",
      "Phase: train. Epoch: 25. Loss: 0.0952405259013176\n",
      "25 train 5 72\n",
      "Phase: train. Epoch: 25. Loss: 0.06620283424854279\n",
      "25 train 6 84\n",
      "Phase: train. Epoch: 25. Loss: 0.08833339065313339\n",
      "25 train 7 96\n",
      "Phase: train. Epoch: 25. Loss: 0.07768392562866211\n",
      "25 train 8 108\n",
      "Phase: train. Epoch: 25. Loss: 0.07235493510961533\n",
      "25 train 9 120\n",
      "Phase: train. Epoch: 25. Loss: 0.084159255027771\n",
      "25 train 10 132\n",
      "Phase: train. Epoch: 25. Loss: 0.0770922601222992\n",
      "25 train 11 144\n",
      "Phase: train. Epoch: 25. Loss: 0.08534588664770126\n",
      "25 train 12 156\n",
      "Phase: train. Epoch: 25. Loss: 0.084599070250988\n",
      "25 train 13 168\n",
      "Phase: train. Epoch: 25. Loss: 0.07185614109039307\n",
      "25 train 14 180\n",
      "Phase: train. Epoch: 25. Loss: 0.08913831412792206\n",
      "25 train 15 192\n",
      "Phase: train. Epoch: 25. Loss: 0.07872302830219269\n",
      "25 train 16 204\n",
      "Phase: train. Epoch: 25. Loss: 0.06978247314691544\n",
      "25 train 17 216\n",
      "Phase: train. Epoch: 25. Loss: 0.08851667493581772\n",
      "25 train 18 228\n",
      "Phase: train. Epoch: 25. Loss: 0.09225940704345703\n",
      "25 train 19 240\n",
      "Phase: train. Epoch: 25. Loss: 0.08144410699605942\n",
      "25 train 20 252\n",
      "Phase: train. Epoch: 25. Loss: 0.10398337244987488\n",
      "25 train 21 264\n",
      "Phase: train. Epoch: 25. Loss: 0.07410494983196259\n",
      "25 train 22 276\n",
      "Phase: train. Epoch: 25. Loss: 0.07761131972074509\n",
      "25 train 23 288\n",
      "Phase: train. Epoch: 25. Loss: 0.09327038377523422\n",
      "25 train 24 300\n",
      "Phase: train. Epoch: 25. Loss: 0.10452138632535934\n",
      "25 train 25 312\n",
      "Phase: train. Epoch: 25. Loss: 0.09287182986736298\n",
      "25 train 26 324\n",
      "Phase: train. Epoch: 25. Loss: 0.07006751000881195\n",
      "25 train 27 336\n",
      "Phase: train. Epoch: 25. Loss: 0.09466149657964706\n",
      "25 train 28 348\n",
      "Phase: train. Epoch: 25. Loss: 0.06764626502990723\n",
      "25 train 29 360\n",
      "Phase: train. Epoch: 25. Loss: 0.08593633025884628\n",
      "25 train 30 372\n",
      "Phase: train. Epoch: 25. Loss: 0.08726118505001068\n",
      "25 train 31 384\n",
      "Phase: train. Epoch: 25. Loss: 0.0753854364156723\n",
      "25 train 32 396\n",
      "Phase: train. Epoch: 25. Loss: 0.0866742953658104\n",
      "25 train 33 408\n",
      "Phase: train. Epoch: 25. Loss: 0.08438904583454132\n",
      "25 train 34 420\n",
      "Phase: train. Epoch: 25. Loss: 0.11809130758047104\n",
      "25 train 35 432\n",
      "Phase: train. Epoch: 25. Loss: 0.09295031428337097\n",
      "25 train 36 444\n",
      "Phase: train. Epoch: 25. Loss: 0.0996965616941452\n",
      "25 train 37 456\n",
      "Phase: train. Epoch: 25. Loss: 0.08755667507648468\n",
      "25 train 38 468\n",
      "Phase: train. Epoch: 25. Loss: 0.10649065673351288\n",
      "25 train 39 480\n",
      "Phase: train. Epoch: 25. Loss: 0.08521674573421478\n",
      "25 train 40 492\n",
      "Phase: train. Epoch: 25. Loss: 0.1127343401312828\n",
      "25 train 41 504\n",
      "Phase: train. Epoch: 25. Loss: 0.09698283672332764\n",
      "25 train 42 516\n",
      "Phase: train. Epoch: 25. Loss: 0.11214806884527206\n",
      "25 train 43 528\n",
      "Phase: train. Epoch: 25. Loss: 0.08194240182638168\n",
      "25 train 44 540\n",
      "Phase: train. Epoch: 25. Loss: 0.08866264671087265\n",
      "25 train 45 552\n",
      "Phase: train. Epoch: 25. Loss: 0.08001276850700378\n",
      "25 train 46 564\n",
      "Phase: train. Epoch: 25. Loss: 0.07839605212211609\n",
      "25 train 47 576\n",
      "Phase: train. Epoch: 25. Loss: 0.09371519088745117\n",
      "25 train 48 588\n",
      "Phase: train. Epoch: 25. Loss: 0.09502965211868286\n",
      "25 train 49 600\n",
      "Phase: train. Epoch: 25. Loss: 0.07927742600440979\n",
      "25 train 50 612\n",
      "Phase: train. Epoch: 25. Loss: 0.11161935329437256\n",
      "25 train 51 624\n",
      "Phase: train. Epoch: 25. Loss: 0.11860021203756332\n",
      "25 train 52 636\n",
      "Phase: train. Epoch: 25. Loss: 0.07699136435985565\n",
      "25 train 53 648\n",
      "Phase: train. Epoch: 25. Loss: 0.08508104085922241\n",
      "25 train 54 660\n",
      "Phase: train. Epoch: 25. Loss: 0.08417778462171555\n",
      "25 train 55 672\n",
      "Phase: train. Epoch: 25. Loss: 0.10017304122447968\n",
      "25 train 56 684\n",
      "Phase: train. Epoch: 25. Loss: 0.08734216541051865\n",
      "25 train 57 696\n",
      "Phase: train. Epoch: 25. Loss: 0.08626242727041245\n",
      "25 train 58 708\n",
      "Phase: train. Epoch: 25. Loss: 0.08838238567113876\n",
      "25 train 59 720\n",
      "Phase: train. Epoch: 25. Loss: 0.09342559427022934\n",
      "25 train 60 732\n",
      "Phase: train. Epoch: 25. Loss: 0.06906728446483612\n",
      "25 train 61 744\n",
      "Phase: train. Epoch: 25. Loss: 0.08732067048549652\n",
      "25 train 62 751\n",
      "Phase: train. Epoch: 25. Loss: 0.11176062375307083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 val 0 763\n",
      "Phase: val. Epoch: 25. Loss: 0.07288762927055359\n",
      "25 val 1 775\n",
      "Phase: val. Epoch: 25. Loss: 0.06912660598754883\n",
      "25 val 2 787\n",
      "Phase: val. Epoch: 25. Loss: 0.07961159944534302\n",
      "25 val 3 799\n",
      "Phase: val. Epoch: 25. Loss: 0.09231406450271606\n",
      "25 val 4 811\n",
      "Phase: val. Epoch: 25. Loss: 0.07252009958028793\n",
      "25 val 5 823\n",
      "Phase: val. Epoch: 25. Loss: 0.10978393256664276\n",
      "25 val 6 835\n",
      "Phase: val. Epoch: 25. Loss: 0.09265555441379547\n",
      "25 val 7 847\n",
      "Phase: val. Epoch: 25. Loss: 0.06785624474287033\n",
      "25 val 8 859\n",
      "Phase: val. Epoch: 25. Loss: 0.08526607602834702\n",
      "25 val 9 871\n",
      "Phase: val. Epoch: 25. Loss: 0.0824723020195961\n",
      "25 val 10 883\n",
      "Phase: val. Epoch: 25. Loss: 0.08006028831005096\n",
      "25 val 11 884\n",
      "Phase: val. Epoch: 25. Loss: 0.060352079570293427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 train 0 12\n",
      "Phase: train. Epoch: 26. Loss: 0.061792127788066864\n",
      "26 train 1 24\n",
      "Phase: train. Epoch: 26. Loss: 0.0637655109167099\n",
      "26 train 2 36\n",
      "Phase: train. Epoch: 26. Loss: 0.07851274311542511\n",
      "26 train 3 48\n",
      "Phase: train. Epoch: 26. Loss: 0.10327908396720886\n",
      "26 train 4 60\n",
      "Phase: train. Epoch: 26. Loss: 0.07502315193414688\n",
      "26 train 5 72\n",
      "Phase: train. Epoch: 26. Loss: 0.08941340446472168\n",
      "26 train 6 84\n",
      "Phase: train. Epoch: 26. Loss: 0.06497912108898163\n",
      "26 train 7 96\n",
      "Phase: train. Epoch: 26. Loss: 0.09569957107305527\n",
      "26 train 8 108\n",
      "Phase: train. Epoch: 26. Loss: 0.07932786643505096\n",
      "26 train 9 120\n",
      "Phase: train. Epoch: 26. Loss: 0.0918935239315033\n",
      "26 train 10 132\n",
      "Phase: train. Epoch: 26. Loss: 0.09346811473369598\n",
      "26 train 11 144\n",
      "Phase: train. Epoch: 26. Loss: 0.09583082050085068\n",
      "26 train 12 156\n",
      "Phase: train. Epoch: 26. Loss: 0.08552245795726776\n",
      "26 train 13 168\n",
      "Phase: train. Epoch: 26. Loss: 0.0972101241350174\n",
      "26 train 14 180\n",
      "Phase: train. Epoch: 26. Loss: 0.08538534492254257\n",
      "26 train 15 192\n",
      "Phase: train. Epoch: 26. Loss: 0.1106911301612854\n",
      "26 train 16 204\n",
      "Phase: train. Epoch: 26. Loss: 0.08143287897109985\n",
      "26 train 17 216\n",
      "Phase: train. Epoch: 26. Loss: 0.10206323117017746\n",
      "26 train 18 228\n",
      "Phase: train. Epoch: 26. Loss: 0.07744672149419785\n",
      "26 train 19 240\n",
      "Phase: train. Epoch: 26. Loss: 0.08546357601881027\n",
      "26 train 20 252\n",
      "Phase: train. Epoch: 26. Loss: 0.10237301886081696\n",
      "26 train 21 264\n",
      "Phase: train. Epoch: 26. Loss: 0.10917650908231735\n",
      "26 train 22 276\n",
      "Phase: train. Epoch: 26. Loss: 0.10743771493434906\n",
      "26 train 23 288\n",
      "Phase: train. Epoch: 26. Loss: 0.06779679656028748\n",
      "26 train 24 300\n",
      "Phase: train. Epoch: 26. Loss: 0.09707051515579224\n",
      "26 train 25 312\n",
      "Phase: train. Epoch: 26. Loss: 0.07507482916116714\n",
      "26 train 26 324\n",
      "Phase: train. Epoch: 26. Loss: 0.10614080727100372\n",
      "26 train 27 336\n",
      "Phase: train. Epoch: 26. Loss: 0.06299453973770142\n",
      "26 train 28 348\n",
      "Phase: train. Epoch: 26. Loss: 0.08898782730102539\n",
      "26 train 29 360\n",
      "Phase: train. Epoch: 26. Loss: 0.08313564956188202\n",
      "26 train 30 372\n",
      "Phase: train. Epoch: 26. Loss: 0.09098570793867111\n",
      "26 train 31 384\n",
      "Phase: train. Epoch: 26. Loss: 0.06275242567062378\n",
      "26 train 32 396\n",
      "Phase: train. Epoch: 26. Loss: 0.09279867261648178\n",
      "26 train 33 408\n",
      "Phase: train. Epoch: 26. Loss: 0.07776959240436554\n",
      "26 train 34 420\n",
      "Phase: train. Epoch: 26. Loss: 0.09678629040718079\n",
      "26 train 35 432\n",
      "Phase: train. Epoch: 26. Loss: 0.06122398376464844\n",
      "26 train 36 444\n",
      "Phase: train. Epoch: 26. Loss: 0.08767654746770859\n",
      "26 train 37 456\n",
      "Phase: train. Epoch: 26. Loss: 0.10902997106313705\n",
      "26 train 38 468\n",
      "Phase: train. Epoch: 26. Loss: 0.07417267560958862\n",
      "26 train 39 480\n",
      "Phase: train. Epoch: 26. Loss: 0.09855719655752182\n",
      "26 train 40 492\n",
      "Phase: train. Epoch: 26. Loss: 0.07902991026639938\n",
      "26 train 41 504\n",
      "Phase: train. Epoch: 26. Loss: 0.08737164735794067\n",
      "26 train 42 516\n",
      "Phase: train. Epoch: 26. Loss: 0.10833925008773804\n",
      "26 train 43 528\n",
      "Phase: train. Epoch: 26. Loss: 0.1326742321252823\n",
      "26 train 44 540\n",
      "Phase: train. Epoch: 26. Loss: 0.07689204066991806\n",
      "26 train 45 552\n",
      "Phase: train. Epoch: 26. Loss: 0.08044548332691193\n",
      "26 train 46 564\n",
      "Phase: train. Epoch: 26. Loss: 0.1072603389620781\n",
      "26 train 47 576\n",
      "Phase: train. Epoch: 26. Loss: 0.10799365490674973\n",
      "26 train 48 588\n",
      "Phase: train. Epoch: 26. Loss: 0.09288470447063446\n",
      "26 train 49 600\n",
      "Phase: train. Epoch: 26. Loss: 0.06967675685882568\n",
      "26 train 50 612\n",
      "Phase: train. Epoch: 26. Loss: 0.09180261194705963\n",
      "26 train 51 624\n",
      "Phase: train. Epoch: 26. Loss: 0.07612598687410355\n",
      "26 train 52 636\n",
      "Phase: train. Epoch: 26. Loss: 0.12913326919078827\n",
      "26 train 53 648\n",
      "Phase: train. Epoch: 26. Loss: 0.10750880837440491\n",
      "26 train 54 660\n",
      "Phase: train. Epoch: 26. Loss: 0.1057620495557785\n",
      "26 train 55 672\n",
      "Phase: train. Epoch: 26. Loss: 0.1075504943728447\n",
      "26 train 56 684\n",
      "Phase: train. Epoch: 26. Loss: 0.07962563633918762\n",
      "26 train 57 696\n",
      "Phase: train. Epoch: 26. Loss: 0.09611304849386215\n",
      "26 train 58 708\n",
      "Phase: train. Epoch: 26. Loss: 0.10430358350276947\n",
      "26 train 59 720\n",
      "Phase: train. Epoch: 26. Loss: 0.11012392491102219\n",
      "26 train 60 732\n",
      "Phase: train. Epoch: 26. Loss: 0.07737016677856445\n",
      "26 train 61 744\n",
      "Phase: train. Epoch: 26. Loss: 0.10365445911884308\n",
      "26 train 62 751\n",
      "Phase: train. Epoch: 26. Loss: 0.1048915833234787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 val 0 763\n",
      "Phase: val. Epoch: 26. Loss: 0.1108083426952362\n",
      "26 val 1 775\n",
      "Phase: val. Epoch: 26. Loss: 0.0846966952085495\n",
      "26 val 2 787\n",
      "Phase: val. Epoch: 26. Loss: 0.09435876458883286\n",
      "26 val 3 799\n",
      "Phase: val. Epoch: 26. Loss: 0.08558680862188339\n",
      "26 val 4 811\n",
      "Phase: val. Epoch: 26. Loss: 0.09328265488147736\n",
      "26 val 5 823\n",
      "Phase: val. Epoch: 26. Loss: 0.07343156635761261\n",
      "26 val 6 835\n",
      "Phase: val. Epoch: 26. Loss: 0.084215447306633\n",
      "26 val 7 847\n",
      "Phase: val. Epoch: 26. Loss: 0.07681143283843994\n",
      "26 val 8 859\n",
      "Phase: val. Epoch: 26. Loss: 0.07181359082460403\n",
      "26 val 9 871\n",
      "Phase: val. Epoch: 26. Loss: 0.08444790542125702\n",
      "26 val 10 883\n",
      "Phase: val. Epoch: 26. Loss: 0.08223550021648407\n",
      "26 val 11 884\n",
      "Phase: val. Epoch: 26. Loss: 0.20608913898468018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 train 0 12\n",
      "Phase: train. Epoch: 27. Loss: 0.07709082961082458\n",
      "27 train 1 24\n",
      "Phase: train. Epoch: 27. Loss: 0.0851449966430664\n",
      "27 train 2 36\n",
      "Phase: train. Epoch: 27. Loss: 0.07928311079740524\n",
      "27 train 3 48\n",
      "Phase: train. Epoch: 27. Loss: 0.09287191927433014\n",
      "27 train 4 60\n",
      "Phase: train. Epoch: 27. Loss: 0.06944813579320908\n",
      "27 train 5 72\n",
      "Phase: train. Epoch: 27. Loss: 0.10203301906585693\n",
      "27 train 6 84\n",
      "Phase: train. Epoch: 27. Loss: 0.08571569621562958\n",
      "27 train 7 96\n",
      "Phase: train. Epoch: 27. Loss: 0.08827795833349228\n",
      "27 train 8 108\n",
      "Phase: train. Epoch: 27. Loss: 0.0926276445388794\n",
      "27 train 9 120\n",
      "Phase: train. Epoch: 27. Loss: 0.0640304759144783\n",
      "27 train 10 132\n",
      "Phase: train. Epoch: 27. Loss: 0.09023159742355347\n",
      "27 train 11 144\n",
      "Phase: train. Epoch: 27. Loss: 0.07492522150278091\n",
      "27 train 12 156\n",
      "Phase: train. Epoch: 27. Loss: 0.13294164836406708\n",
      "27 train 13 168\n",
      "Phase: train. Epoch: 27. Loss: 0.08764944970607758\n",
      "27 train 14 180\n",
      "Phase: train. Epoch: 27. Loss: 0.07696282118558884\n",
      "27 train 15 192\n",
      "Phase: train. Epoch: 27. Loss: 0.07435169816017151\n",
      "27 train 16 204\n",
      "Phase: train. Epoch: 27. Loss: 0.09219041466712952\n",
      "27 train 17 216\n",
      "Phase: train. Epoch: 27. Loss: 0.07438969612121582\n",
      "27 train 18 228\n",
      "Phase: train. Epoch: 27. Loss: 0.0939791202545166\n",
      "27 train 19 240\n",
      "Phase: train. Epoch: 27. Loss: 0.1104249581694603\n",
      "27 train 20 252\n",
      "Phase: train. Epoch: 27. Loss: 0.07002478837966919\n",
      "27 train 21 264\n",
      "Phase: train. Epoch: 27. Loss: 0.09199774265289307\n",
      "27 train 22 276\n",
      "Phase: train. Epoch: 27. Loss: 0.07995273172855377\n",
      "27 train 23 288\n",
      "Phase: train. Epoch: 27. Loss: 0.0953068658709526\n",
      "27 train 24 300\n",
      "Phase: train. Epoch: 27. Loss: 0.08595821261405945\n",
      "27 train 25 312\n",
      "Phase: train. Epoch: 27. Loss: 0.07942813634872437\n",
      "27 train 26 324\n",
      "Phase: train. Epoch: 27. Loss: 0.09077735990285873\n",
      "27 train 27 336\n",
      "Phase: train. Epoch: 27. Loss: 0.11901579797267914\n",
      "27 train 28 348\n",
      "Phase: train. Epoch: 27. Loss: 0.10955934971570969\n",
      "27 train 29 360\n",
      "Phase: train. Epoch: 27. Loss: 0.08374567329883575\n",
      "27 train 30 372\n",
      "Phase: train. Epoch: 27. Loss: 0.11390665173530579\n",
      "27 train 31 384\n",
      "Phase: train. Epoch: 27. Loss: 0.12707874178886414\n",
      "27 train 32 396\n",
      "Phase: train. Epoch: 27. Loss: 0.08064984530210495\n",
      "27 train 33 408\n",
      "Phase: train. Epoch: 27. Loss: 0.06581062078475952\n",
      "27 train 34 420\n",
      "Phase: train. Epoch: 27. Loss: 0.07636407017707825\n",
      "27 train 35 432\n",
      "Phase: train. Epoch: 27. Loss: 0.09050723165273666\n",
      "27 train 36 444\n",
      "Phase: train. Epoch: 27. Loss: 0.08915911614894867\n",
      "27 train 37 456\n",
      "Phase: train. Epoch: 27. Loss: 0.08172782510519028\n",
      "27 train 38 468\n",
      "Phase: train. Epoch: 27. Loss: 0.10182158648967743\n",
      "27 train 39 480\n",
      "Phase: train. Epoch: 27. Loss: 0.10525526106357574\n",
      "27 train 40 492\n",
      "Phase: train. Epoch: 27. Loss: 0.0667811781167984\n",
      "27 train 41 504\n",
      "Phase: train. Epoch: 27. Loss: 0.088035449385643\n",
      "27 train 42 516\n",
      "Phase: train. Epoch: 27. Loss: 0.08514093607664108\n",
      "27 train 43 528\n",
      "Phase: train. Epoch: 27. Loss: 0.0967022180557251\n",
      "27 train 44 540\n",
      "Phase: train. Epoch: 27. Loss: 0.08459548652172089\n",
      "27 train 45 552\n",
      "Phase: train. Epoch: 27. Loss: 0.06939444690942764\n",
      "27 train 46 564\n",
      "Phase: train. Epoch: 27. Loss: 0.09691416472196579\n",
      "27 train 47 576\n",
      "Phase: train. Epoch: 27. Loss: 0.08574287593364716\n",
      "27 train 48 588\n",
      "Phase: train. Epoch: 27. Loss: 0.10721467435359955\n",
      "27 train 49 600\n",
      "Phase: train. Epoch: 27. Loss: 0.0894843190908432\n",
      "27 train 50 612\n",
      "Phase: train. Epoch: 27. Loss: 0.07949840277433395\n",
      "27 train 51 624\n",
      "Phase: train. Epoch: 27. Loss: 0.08832698315382004\n",
      "27 train 52 636\n",
      "Phase: train. Epoch: 27. Loss: 0.08165128529071808\n",
      "27 train 53 648\n",
      "Phase: train. Epoch: 27. Loss: 0.09499874711036682\n",
      "27 train 54 660\n",
      "Phase: train. Epoch: 27. Loss: 0.09673603624105453\n",
      "27 train 55 672\n",
      "Phase: train. Epoch: 27. Loss: 0.07674173265695572\n",
      "27 train 56 684\n",
      "Phase: train. Epoch: 27. Loss: 0.08623241633176804\n",
      "27 train 57 696\n",
      "Phase: train. Epoch: 27. Loss: 0.08703009784221649\n",
      "27 train 58 708\n",
      "Phase: train. Epoch: 27. Loss: 0.0930107980966568\n",
      "27 train 59 720\n",
      "Phase: train. Epoch: 27. Loss: 0.07903159409761429\n",
      "27 train 60 732\n",
      "Phase: train. Epoch: 27. Loss: 0.10290473699569702\n",
      "27 train 61 744\n",
      "Phase: train. Epoch: 27. Loss: 0.10391631722450256\n",
      "27 train 62 751\n",
      "Phase: train. Epoch: 27. Loss: 0.09089795500040054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 val 0 763\n",
      "Phase: val. Epoch: 27. Loss: 0.09540705382823944\n",
      "27 val 1 775\n",
      "Phase: val. Epoch: 27. Loss: 0.09329619258642197\n",
      "27 val 2 787\n",
      "Phase: val. Epoch: 27. Loss: 0.08349328488111496\n",
      "27 val 3 799\n",
      "Phase: val. Epoch: 27. Loss: 0.06692790985107422\n",
      "27 val 4 811\n",
      "Phase: val. Epoch: 27. Loss: 0.10893933475017548\n",
      "27 val 5 823\n",
      "Phase: val. Epoch: 27. Loss: 0.10531286895275116\n",
      "27 val 6 835\n",
      "Phase: val. Epoch: 27. Loss: 0.07073749601840973\n",
      "27 val 7 847\n",
      "Phase: val. Epoch: 27. Loss: 0.09946875274181366\n",
      "27 val 8 859\n",
      "Phase: val. Epoch: 27. Loss: 0.06816872209310532\n",
      "27 val 9 871\n",
      "Phase: val. Epoch: 27. Loss: 0.0728350281715393\n",
      "27 val 10 883\n",
      "Phase: val. Epoch: 27. Loss: 0.0902719497680664\n",
      "27 val 11 884\n",
      "Phase: val. Epoch: 27. Loss: 0.11849953979253769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 train 0 12\n",
      "Phase: train. Epoch: 28. Loss: 0.1383407562971115\n",
      "28 train 1 24\n",
      "Phase: train. Epoch: 28. Loss: 0.11456000059843063\n",
      "28 train 2 36\n",
      "Phase: train. Epoch: 28. Loss: 0.07894545793533325\n",
      "28 train 3 48\n",
      "Phase: train. Epoch: 28. Loss: 0.07814522087574005\n",
      "28 train 4 60\n",
      "Phase: train. Epoch: 28. Loss: 0.06642910093069077\n",
      "28 train 5 72\n",
      "Phase: train. Epoch: 28. Loss: 0.08707668632268906\n",
      "28 train 6 84\n",
      "Phase: train. Epoch: 28. Loss: 0.10271768271923065\n",
      "28 train 7 96\n",
      "Phase: train. Epoch: 28. Loss: 0.08979909121990204\n",
      "28 train 8 108\n",
      "Phase: train. Epoch: 28. Loss: 0.09906448423862457\n",
      "28 train 9 120\n",
      "Phase: train. Epoch: 28. Loss: 0.07785477489233017\n",
      "28 train 10 132\n",
      "Phase: train. Epoch: 28. Loss: 0.09384851157665253\n",
      "28 train 11 144\n",
      "Phase: train. Epoch: 28. Loss: 0.08733614534139633\n",
      "28 train 12 156\n",
      "Phase: train. Epoch: 28. Loss: 0.07469750940799713\n",
      "28 train 13 168\n",
      "Phase: train. Epoch: 28. Loss: 0.07350825518369675\n",
      "28 train 14 180\n",
      "Phase: train. Epoch: 28. Loss: 0.0778958797454834\n",
      "28 train 15 192\n",
      "Phase: train. Epoch: 28. Loss: 0.12334086745977402\n",
      "28 train 16 204\n",
      "Phase: train. Epoch: 28. Loss: 0.09459303319454193\n",
      "28 train 17 216\n",
      "Phase: train. Epoch: 28. Loss: 0.11797074973583221\n",
      "28 train 18 228\n",
      "Phase: train. Epoch: 28. Loss: 0.09536590427160263\n",
      "28 train 19 240\n",
      "Phase: train. Epoch: 28. Loss: 0.0973496064543724\n",
      "28 train 20 252\n",
      "Phase: train. Epoch: 28. Loss: 0.06662913411855698\n",
      "28 train 21 264\n",
      "Phase: train. Epoch: 28. Loss: 0.09074761718511581\n",
      "28 train 22 276\n",
      "Phase: train. Epoch: 28. Loss: 0.08108963072299957\n",
      "28 train 23 288\n",
      "Phase: train. Epoch: 28. Loss: 0.07776766270399094\n",
      "28 train 24 300\n",
      "Phase: train. Epoch: 28. Loss: 0.07953915745019913\n",
      "28 train 25 312\n",
      "Phase: train. Epoch: 28. Loss: 0.07670017331838608\n",
      "28 train 26 324\n",
      "Phase: train. Epoch: 28. Loss: 0.0782429501414299\n",
      "28 train 27 336\n",
      "Phase: train. Epoch: 28. Loss: 0.06889882683753967\n",
      "28 train 28 348\n",
      "Phase: train. Epoch: 28. Loss: 0.11348681151866913\n",
      "28 train 29 360\n",
      "Phase: train. Epoch: 28. Loss: 0.10430936515331268\n",
      "28 train 30 372\n",
      "Phase: train. Epoch: 28. Loss: 0.0912853330373764\n",
      "28 train 31 384\n",
      "Phase: train. Epoch: 28. Loss: 0.11466248333454132\n",
      "28 train 32 396\n",
      "Phase: train. Epoch: 28. Loss: 0.08642236888408661\n",
      "28 train 33 408\n",
      "Phase: train. Epoch: 28. Loss: 0.11723252385854721\n",
      "28 train 34 420\n",
      "Phase: train. Epoch: 28. Loss: 0.07555928081274033\n",
      "28 train 35 432\n",
      "Phase: train. Epoch: 28. Loss: 0.07566633820533752\n",
      "28 train 36 444\n",
      "Phase: train. Epoch: 28. Loss: 0.10031993687152863\n",
      "28 train 37 456\n",
      "Phase: train. Epoch: 28. Loss: 0.10267771780490875\n",
      "28 train 38 468\n",
      "Phase: train. Epoch: 28. Loss: 0.06588776409626007\n",
      "28 train 39 480\n",
      "Phase: train. Epoch: 28. Loss: 0.119748555123806\n",
      "28 train 40 492\n",
      "Phase: train. Epoch: 28. Loss: 0.11953570693731308\n",
      "28 train 41 504\n",
      "Phase: train. Epoch: 28. Loss: 0.07280310988426208\n",
      "28 train 42 516\n",
      "Phase: train. Epoch: 28. Loss: 0.12237399071455002\n",
      "28 train 43 528\n",
      "Phase: train. Epoch: 28. Loss: 0.06433656811714172\n",
      "28 train 44 540\n",
      "Phase: train. Epoch: 28. Loss: 0.12766778469085693\n",
      "28 train 45 552\n",
      "Phase: train. Epoch: 28. Loss: 0.10899809002876282\n",
      "28 train 46 564\n",
      "Phase: train. Epoch: 28. Loss: 0.07042278349399567\n",
      "28 train 47 576\n",
      "Phase: train. Epoch: 28. Loss: 0.09917409718036652\n",
      "28 train 48 588\n",
      "Phase: train. Epoch: 28. Loss: 0.10456252098083496\n",
      "28 train 49 600\n",
      "Phase: train. Epoch: 28. Loss: 0.08402850478887558\n",
      "28 train 50 612\n",
      "Phase: train. Epoch: 28. Loss: 0.08220812678337097\n",
      "28 train 51 624\n",
      "Phase: train. Epoch: 28. Loss: 0.07553236186504364\n",
      "28 train 52 636\n",
      "Phase: train. Epoch: 28. Loss: 0.07624475657939911\n",
      "28 train 53 648\n",
      "Phase: train. Epoch: 28. Loss: 0.09801006317138672\n",
      "28 train 54 660\n",
      "Phase: train. Epoch: 28. Loss: 0.0615963451564312\n",
      "28 train 55 672\n",
      "Phase: train. Epoch: 28. Loss: 0.09366132318973541\n",
      "28 train 56 684\n",
      "Phase: train. Epoch: 28. Loss: 0.08210048079490662\n",
      "28 train 57 696\n",
      "Phase: train. Epoch: 28. Loss: 0.061204373836517334\n",
      "28 train 58 708\n",
      "Phase: train. Epoch: 28. Loss: 0.07912902534008026\n",
      "28 train 59 720\n",
      "Phase: train. Epoch: 28. Loss: 0.0751345157623291\n",
      "28 train 60 732\n",
      "Phase: train. Epoch: 28. Loss: 0.0648910328745842\n",
      "28 train 61 744\n",
      "Phase: train. Epoch: 28. Loss: 0.08815015852451324\n",
      "28 train 62 751\n",
      "Phase: train. Epoch: 28. Loss: 0.06912024319171906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 val 0 763\n",
      "Phase: val. Epoch: 28. Loss: 0.06393389403820038\n",
      "28 val 1 775\n",
      "Phase: val. Epoch: 28. Loss: 0.1051129400730133\n",
      "28 val 2 787\n",
      "Phase: val. Epoch: 28. Loss: 0.07943015545606613\n",
      "28 val 3 799\n",
      "Phase: val. Epoch: 28. Loss: 0.09121163934469223\n",
      "28 val 4 811\n",
      "Phase: val. Epoch: 28. Loss: 0.10922972857952118\n",
      "28 val 5 823\n",
      "Phase: val. Epoch: 28. Loss: 0.06118132919073105\n",
      "28 val 6 835\n",
      "Phase: val. Epoch: 28. Loss: 0.117439866065979\n",
      "28 val 7 847\n",
      "Phase: val. Epoch: 28. Loss: 0.09988071769475937\n",
      "28 val 8 859\n",
      "Phase: val. Epoch: 28. Loss: 0.06260005384683609\n",
      "28 val 9 871\n",
      "Phase: val. Epoch: 28. Loss: 0.07358807325363159\n",
      "28 val 10 883\n",
      "Phase: val. Epoch: 28. Loss: 0.12824317812919617\n",
      "28 val 11 884\n",
      "Phase: val. Epoch: 28. Loss: 0.03040901944041252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 train 0 12\n",
      "Phase: train. Epoch: 29. Loss: 0.07774271070957184\n",
      "29 train 1 24\n",
      "Phase: train. Epoch: 29. Loss: 0.08527233451604843\n",
      "29 train 2 36\n",
      "Phase: train. Epoch: 29. Loss: 0.09050087630748749\n",
      "29 train 3 48\n",
      "Phase: train. Epoch: 29. Loss: 0.11175954341888428\n",
      "29 train 4 60\n",
      "Phase: train. Epoch: 29. Loss: 0.09354475140571594\n",
      "29 train 5 72\n",
      "Phase: train. Epoch: 29. Loss: 0.08246587216854095\n",
      "29 train 6 84\n",
      "Phase: train. Epoch: 29. Loss: 0.07536625862121582\n",
      "29 train 7 96\n",
      "Phase: train. Epoch: 29. Loss: 0.12238394469022751\n",
      "29 train 8 108\n",
      "Phase: train. Epoch: 29. Loss: 0.06972342729568481\n",
      "29 train 9 120\n",
      "Phase: train. Epoch: 29. Loss: 0.09260986745357513\n",
      "29 train 10 132\n",
      "Phase: train. Epoch: 29. Loss: 0.09422474354505539\n",
      "29 train 11 144\n",
      "Phase: train. Epoch: 29. Loss: 0.06927432864904404\n",
      "29 train 12 156\n",
      "Phase: train. Epoch: 29. Loss: 0.0925387293100357\n",
      "29 train 13 168\n",
      "Phase: train. Epoch: 29. Loss: 0.08834707736968994\n",
      "29 train 14 180\n",
      "Phase: train. Epoch: 29. Loss: 0.08622802793979645\n",
      "29 train 15 192\n",
      "Phase: train. Epoch: 29. Loss: 0.10678067058324814\n",
      "29 train 16 204\n",
      "Phase: train. Epoch: 29. Loss: 0.09545053541660309\n",
      "29 train 17 216\n",
      "Phase: train. Epoch: 29. Loss: 0.10772953927516937\n",
      "29 train 18 228\n",
      "Phase: train. Epoch: 29. Loss: 0.09825742244720459\n",
      "29 train 19 240\n",
      "Phase: train. Epoch: 29. Loss: 0.09001301229000092\n",
      "29 train 20 252\n",
      "Phase: train. Epoch: 29. Loss: 0.0775572881102562\n",
      "29 train 21 264\n",
      "Phase: train. Epoch: 29. Loss: 0.07478342205286026\n",
      "29 train 22 276\n",
      "Phase: train. Epoch: 29. Loss: 0.07753686606884003\n",
      "29 train 23 288\n",
      "Phase: train. Epoch: 29. Loss: 0.10052026808261871\n",
      "29 train 24 300\n",
      "Phase: train. Epoch: 29. Loss: 0.09548322856426239\n",
      "29 train 25 312\n",
      "Phase: train. Epoch: 29. Loss: 0.08950629830360413\n",
      "29 train 26 324\n",
      "Phase: train. Epoch: 29. Loss: 0.09614287316799164\n",
      "29 train 27 336\n",
      "Phase: train. Epoch: 29. Loss: 0.08511416614055634\n",
      "29 train 28 348\n",
      "Phase: train. Epoch: 29. Loss: 0.09805667400360107\n",
      "29 train 29 360\n",
      "Phase: train. Epoch: 29. Loss: 0.07387638092041016\n",
      "29 train 30 372\n",
      "Phase: train. Epoch: 29. Loss: 0.08559463918209076\n",
      "29 train 31 384\n",
      "Phase: train. Epoch: 29. Loss: 0.07020211219787598\n",
      "29 train 32 396\n",
      "Phase: train. Epoch: 29. Loss: 0.08461060374975204\n",
      "29 train 33 408\n",
      "Phase: train. Epoch: 29. Loss: 0.10095520317554474\n",
      "29 train 34 420\n",
      "Phase: train. Epoch: 29. Loss: 0.07344900816679001\n",
      "29 train 35 432\n",
      "Phase: train. Epoch: 29. Loss: 0.08918049186468124\n",
      "29 train 36 444\n",
      "Phase: train. Epoch: 29. Loss: 0.08676056563854218\n",
      "29 train 37 456\n",
      "Phase: train. Epoch: 29. Loss: 0.0980258584022522\n",
      "29 train 38 468\n",
      "Phase: train. Epoch: 29. Loss: 0.08562802523374557\n",
      "29 train 39 480\n",
      "Phase: train. Epoch: 29. Loss: 0.08140258491039276\n",
      "29 train 40 492\n",
      "Phase: train. Epoch: 29. Loss: 0.10492607206106186\n",
      "29 train 41 504\n",
      "Phase: train. Epoch: 29. Loss: 0.06667248159646988\n",
      "29 train 42 516\n",
      "Phase: train. Epoch: 29. Loss: 0.093332938849926\n",
      "29 train 43 528\n",
      "Phase: train. Epoch: 29. Loss: 0.07463786005973816\n",
      "29 train 44 540\n",
      "Phase: train. Epoch: 29. Loss: 0.10152681916952133\n",
      "29 train 45 552\n",
      "Phase: train. Epoch: 29. Loss: 0.08055341988801956\n",
      "29 train 46 564\n",
      "Phase: train. Epoch: 29. Loss: 0.08228962123394012\n",
      "29 train 47 576\n",
      "Phase: train. Epoch: 29. Loss: 0.07510053366422653\n",
      "29 train 48 588\n",
      "Phase: train. Epoch: 29. Loss: 0.100103959441185\n",
      "29 train 49 600\n",
      "Phase: train. Epoch: 29. Loss: 0.10316254198551178\n",
      "29 train 50 612\n",
      "Phase: train. Epoch: 29. Loss: 0.08136685192584991\n",
      "29 train 51 624\n",
      "Phase: train. Epoch: 29. Loss: 0.07322146743535995\n",
      "29 train 52 636\n",
      "Phase: train. Epoch: 29. Loss: 0.11276251077651978\n",
      "29 train 53 648\n",
      "Phase: train. Epoch: 29. Loss: 0.11794108152389526\n",
      "29 train 54 660\n",
      "Phase: train. Epoch: 29. Loss: 0.08206648379564285\n",
      "29 train 55 672\n",
      "Phase: train. Epoch: 29. Loss: 0.09446617215871811\n",
      "29 train 56 684\n",
      "Phase: train. Epoch: 29. Loss: 0.09413080662488937\n",
      "29 train 57 696\n",
      "Phase: train. Epoch: 29. Loss: 0.08587782829999924\n",
      "29 train 58 708\n",
      "Phase: train. Epoch: 29. Loss: 0.12836013734340668\n",
      "29 train 59 720\n",
      "Phase: train. Epoch: 29. Loss: 0.07681559026241302\n",
      "29 train 60 732\n",
      "Phase: train. Epoch: 29. Loss: 0.0810249000787735\n",
      "29 train 61 744\n",
      "Phase: train. Epoch: 29. Loss: 0.12113329023122787\n",
      "29 train 62 751\n",
      "Phase: train. Epoch: 29. Loss: 0.061326418071985245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 val 0 763\n",
      "Phase: val. Epoch: 29. Loss: 0.07284954190254211\n",
      "29 val 1 775\n",
      "Phase: val. Epoch: 29. Loss: 0.076329305768013\n",
      "29 val 2 787\n",
      "Phase: val. Epoch: 29. Loss: 0.08491867780685425\n",
      "29 val 3 799\n",
      "Phase: val. Epoch: 29. Loss: 0.06530742347240448\n",
      "29 val 4 811\n",
      "Phase: val. Epoch: 29. Loss: 0.1281384974718094\n",
      "29 val 5 823\n",
      "Phase: val. Epoch: 29. Loss: 0.08389300107955933\n",
      "29 val 6 835\n",
      "Phase: val. Epoch: 29. Loss: 0.08930893987417221\n",
      "29 val 7 847\n",
      "Phase: val. Epoch: 29. Loss: 0.07386712729930878\n",
      "29 val 8 859\n",
      "Phase: val. Epoch: 29. Loss: 0.08290041238069534\n",
      "29 val 9 871\n",
      "Phase: val. Epoch: 29. Loss: 0.09881144762039185\n",
      "29 val 10 883\n",
      "Phase: val. Epoch: 29. Loss: 0.09357001632452011\n",
      "29 val 11 884\n",
      "Phase: val. Epoch: 29. Loss: 0.09101442247629166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 train 0 12\n",
      "Phase: train. Epoch: 30. Loss: 0.08737014979124069\n",
      "30 train 1 24\n",
      "Phase: train. Epoch: 30. Loss: 0.0708160400390625\n",
      "30 train 2 36\n",
      "Phase: train. Epoch: 30. Loss: 0.10554280132055283\n",
      "30 train 3 48\n",
      "Phase: train. Epoch: 30. Loss: 0.08317289501428604\n",
      "30 train 4 60\n",
      "Phase: train. Epoch: 30. Loss: 0.11054609715938568\n",
      "30 train 5 72\n",
      "Phase: train. Epoch: 30. Loss: 0.09027516841888428\n",
      "30 train 6 84\n",
      "Phase: train. Epoch: 30. Loss: 0.07395230978727341\n",
      "30 train 7 96\n",
      "Phase: train. Epoch: 30. Loss: 0.09445852041244507\n",
      "30 train 8 108\n",
      "Phase: train. Epoch: 30. Loss: 0.06979215145111084\n",
      "30 train 9 120\n",
      "Phase: train. Epoch: 30. Loss: 0.13017651438713074\n",
      "30 train 10 132\n",
      "Phase: train. Epoch: 30. Loss: 0.07076498866081238\n",
      "30 train 11 144\n",
      "Phase: train. Epoch: 30. Loss: 0.10133012384176254\n",
      "30 train 12 156\n",
      "Phase: train. Epoch: 30. Loss: 0.07406842708587646\n",
      "30 train 13 168\n",
      "Phase: train. Epoch: 30. Loss: 0.07770851254463196\n",
      "30 train 14 180\n",
      "Phase: train. Epoch: 30. Loss: 0.087775319814682\n",
      "30 train 15 192\n",
      "Phase: train. Epoch: 30. Loss: 0.0647992342710495\n",
      "30 train 16 204\n",
      "Phase: train. Epoch: 30. Loss: 0.08955081552267075\n",
      "30 train 17 216\n",
      "Phase: train. Epoch: 30. Loss: 0.09396447986364365\n",
      "30 train 18 228\n",
      "Phase: train. Epoch: 30. Loss: 0.09482526779174805\n",
      "30 train 19 240\n",
      "Phase: train. Epoch: 30. Loss: 0.10398265719413757\n",
      "30 train 20 252\n",
      "Phase: train. Epoch: 30. Loss: 0.08285820484161377\n",
      "30 train 21 264\n",
      "Phase: train. Epoch: 30. Loss: 0.08761074393987656\n",
      "30 train 22 276\n",
      "Phase: train. Epoch: 30. Loss: 0.10981069505214691\n",
      "30 train 23 288\n",
      "Phase: train. Epoch: 30. Loss: 0.088168203830719\n",
      "30 train 24 300\n",
      "Phase: train. Epoch: 30. Loss: 0.104794442653656\n",
      "30 train 25 312\n",
      "Phase: train. Epoch: 30. Loss: 0.08908345550298691\n",
      "30 train 26 324\n",
      "Phase: train. Epoch: 30. Loss: 0.10428512096405029\n",
      "30 train 27 336\n",
      "Phase: train. Epoch: 30. Loss: 0.07743988186120987\n",
      "30 train 28 348\n",
      "Phase: train. Epoch: 30. Loss: 0.08829506486654282\n",
      "30 train 29 360\n",
      "Phase: train. Epoch: 30. Loss: 0.09174785017967224\n",
      "30 train 30 372\n",
      "Phase: train. Epoch: 30. Loss: 0.08292997628450394\n",
      "30 train 31 384\n",
      "Phase: train. Epoch: 30. Loss: 0.11854803562164307\n",
      "30 train 32 396\n",
      "Phase: train. Epoch: 30. Loss: 0.09851168841123581\n",
      "30 train 33 408\n",
      "Phase: train. Epoch: 30. Loss: 0.06839147210121155\n",
      "30 train 34 420\n",
      "Phase: train. Epoch: 30. Loss: 0.10838133096694946\n",
      "30 train 35 432\n",
      "Phase: train. Epoch: 30. Loss: 0.08271494507789612\n",
      "30 train 36 444\n",
      "Phase: train. Epoch: 30. Loss: 0.08401820063591003\n",
      "30 train 37 456\n",
      "Phase: train. Epoch: 30. Loss: 0.08274365961551666\n",
      "30 train 38 468\n",
      "Phase: train. Epoch: 30. Loss: 0.07161056250333786\n",
      "30 train 39 480\n",
      "Phase: train. Epoch: 30. Loss: 0.07966598868370056\n",
      "30 train 40 492\n",
      "Phase: train. Epoch: 30. Loss: 0.08653154969215393\n",
      "30 train 41 504\n",
      "Phase: train. Epoch: 30. Loss: 0.07789221405982971\n",
      "30 train 42 516\n",
      "Phase: train. Epoch: 30. Loss: 0.10278438776731491\n",
      "30 train 43 528\n",
      "Phase: train. Epoch: 30. Loss: 0.09661424160003662\n",
      "30 train 44 540\n",
      "Phase: train. Epoch: 30. Loss: 0.09931379556655884\n",
      "30 train 45 552\n",
      "Phase: train. Epoch: 30. Loss: 0.07366631925106049\n",
      "30 train 46 564\n",
      "Phase: train. Epoch: 30. Loss: 0.10152687877416611\n",
      "30 train 47 576\n",
      "Phase: train. Epoch: 30. Loss: 0.0776682049036026\n",
      "30 train 48 588\n",
      "Phase: train. Epoch: 30. Loss: 0.08476750552654266\n",
      "30 train 49 600\n",
      "Phase: train. Epoch: 30. Loss: 0.07173280417919159\n",
      "30 train 50 612\n",
      "Phase: train. Epoch: 30. Loss: 0.10899487137794495\n",
      "30 train 51 624\n",
      "Phase: train. Epoch: 30. Loss: 0.08289805054664612\n",
      "30 train 52 636\n",
      "Phase: train. Epoch: 30. Loss: 0.08004949986934662\n",
      "30 train 53 648\n",
      "Phase: train. Epoch: 30. Loss: 0.08638840913772583\n",
      "30 train 54 660\n",
      "Phase: train. Epoch: 30. Loss: 0.09292801469564438\n",
      "30 train 55 672\n",
      "Phase: train. Epoch: 30. Loss: 0.0712621808052063\n",
      "30 train 56 684\n",
      "Phase: train. Epoch: 30. Loss: 0.07395653426647186\n",
      "30 train 57 696\n",
      "Phase: train. Epoch: 30. Loss: 0.08610227704048157\n",
      "30 train 58 708\n",
      "Phase: train. Epoch: 30. Loss: 0.08676064014434814\n",
      "30 train 59 720\n",
      "Phase: train. Epoch: 30. Loss: 0.0951784998178482\n",
      "30 train 60 732\n",
      "Phase: train. Epoch: 30. Loss: 0.0945880264043808\n",
      "30 train 61 744\n",
      "Phase: train. Epoch: 30. Loss: 0.1012093722820282\n",
      "30 train 62 751\n",
      "Phase: train. Epoch: 30. Loss: 0.06297601759433746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 val 0 763\n",
      "Phase: val. Epoch: 30. Loss: 0.11976514756679535\n",
      "30 val 1 775\n",
      "Phase: val. Epoch: 30. Loss: 0.07621580362319946\n",
      "30 val 2 787\n",
      "Phase: val. Epoch: 30. Loss: 0.06755562871694565\n",
      "30 val 3 799\n",
      "Phase: val. Epoch: 30. Loss: 0.09605611860752106\n",
      "30 val 4 811\n",
      "Phase: val. Epoch: 30. Loss: 0.08584332466125488\n",
      "30 val 5 823\n",
      "Phase: val. Epoch: 30. Loss: 0.08867748826742172\n",
      "30 val 6 835\n",
      "Phase: val. Epoch: 30. Loss: 0.08454274386167526\n",
      "30 val 7 847\n",
      "Phase: val. Epoch: 30. Loss: 0.06822358071804047\n",
      "30 val 8 859\n",
      "Phase: val. Epoch: 30. Loss: 0.08068593591451645\n",
      "30 val 9 871\n",
      "Phase: val. Epoch: 30. Loss: 0.11944513022899628\n",
      "30 val 10 883\n",
      "Phase: val. Epoch: 30. Loss: 0.08550547063350677\n",
      "30 val 11 884\n",
      "Phase: val. Epoch: 30. Loss: 0.02918112650513649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 train 0 12\n",
      "Phase: train. Epoch: 31. Loss: 0.08114008605480194\n",
      "31 train 1 24\n",
      "Phase: train. Epoch: 31. Loss: 0.06836982071399689\n",
      "31 train 2 36\n",
      "Phase: train. Epoch: 31. Loss: 0.09160323441028595\n",
      "31 train 3 48\n",
      "Phase: train. Epoch: 31. Loss: 0.09253271669149399\n",
      "31 train 4 60\n",
      "Phase: train. Epoch: 31. Loss: 0.09278057515621185\n",
      "31 train 5 72\n",
      "Phase: train. Epoch: 31. Loss: 0.0764675885438919\n",
      "31 train 6 84\n",
      "Phase: train. Epoch: 31. Loss: 0.09360456466674805\n",
      "31 train 7 96\n",
      "Phase: train. Epoch: 31. Loss: 0.08841206878423691\n",
      "31 train 8 108\n",
      "Phase: train. Epoch: 31. Loss: 0.0763382762670517\n",
      "31 train 9 120\n",
      "Phase: train. Epoch: 31. Loss: 0.08049939572811127\n",
      "31 train 10 132\n",
      "Phase: train. Epoch: 31. Loss: 0.09025045484304428\n",
      "31 train 11 144\n",
      "Phase: train. Epoch: 31. Loss: 0.10792382806539536\n",
      "31 train 12 156\n",
      "Phase: train. Epoch: 31. Loss: 0.09593664109706879\n",
      "31 train 13 168\n",
      "Phase: train. Epoch: 31. Loss: 0.11057260632514954\n",
      "31 train 14 180\n",
      "Phase: train. Epoch: 31. Loss: 0.10595524311065674\n",
      "31 train 15 192\n",
      "Phase: train. Epoch: 31. Loss: 0.10247915983200073\n",
      "31 train 16 204\n",
      "Phase: train. Epoch: 31. Loss: 0.07182512432336807\n",
      "31 train 17 216\n",
      "Phase: train. Epoch: 31. Loss: 0.06552086770534515\n",
      "31 train 18 228\n",
      "Phase: train. Epoch: 31. Loss: 0.08784174919128418\n",
      "31 train 19 240\n",
      "Phase: train. Epoch: 31. Loss: 0.12037903070449829\n",
      "31 train 20 252\n",
      "Phase: train. Epoch: 31. Loss: 0.07952716201543808\n",
      "31 train 21 264\n",
      "Phase: train. Epoch: 31. Loss: 0.07518374174833298\n",
      "31 train 22 276\n",
      "Phase: train. Epoch: 31. Loss: 0.08966298401355743\n",
      "31 train 23 288\n",
      "Phase: train. Epoch: 31. Loss: 0.09381288290023804\n",
      "31 train 24 300\n",
      "Phase: train. Epoch: 31. Loss: 0.08612494170665741\n",
      "31 train 25 312\n",
      "Phase: train. Epoch: 31. Loss: 0.10290568321943283\n",
      "31 train 26 324\n",
      "Phase: train. Epoch: 31. Loss: 0.10042788088321686\n",
      "31 train 27 336\n",
      "Phase: train. Epoch: 31. Loss: 0.09037037938833237\n",
      "31 train 28 348\n",
      "Phase: train. Epoch: 31. Loss: 0.10227646678686142\n",
      "31 train 29 360\n",
      "Phase: train. Epoch: 31. Loss: 0.07965240627527237\n",
      "31 train 30 372\n",
      "Phase: train. Epoch: 31. Loss: 0.0822102278470993\n",
      "31 train 31 384\n",
      "Phase: train. Epoch: 31. Loss: 0.08406151831150055\n",
      "31 train 32 396\n",
      "Phase: train. Epoch: 31. Loss: 0.10078644007444382\n",
      "31 train 33 408\n",
      "Phase: train. Epoch: 31. Loss: 0.06969254463911057\n",
      "31 train 34 420\n",
      "Phase: train. Epoch: 31. Loss: 0.09275525063276291\n",
      "31 train 35 432\n",
      "Phase: train. Epoch: 31. Loss: 0.0805976539850235\n",
      "31 train 36 444\n",
      "Phase: train. Epoch: 31. Loss: 0.08204802870750427\n",
      "31 train 37 456\n",
      "Phase: train. Epoch: 31. Loss: 0.09683184325695038\n",
      "31 train 38 468\n",
      "Phase: train. Epoch: 31. Loss: 0.10299419611692429\n",
      "31 train 39 480\n",
      "Phase: train. Epoch: 31. Loss: 0.08565546572208405\n",
      "31 train 40 492\n",
      "Phase: train. Epoch: 31. Loss: 0.07939724624156952\n",
      "31 train 41 504\n",
      "Phase: train. Epoch: 31. Loss: 0.08948558568954468\n",
      "31 train 42 516\n",
      "Phase: train. Epoch: 31. Loss: 0.07405607402324677\n",
      "31 train 43 528\n",
      "Phase: train. Epoch: 31. Loss: 0.07177956402301788\n",
      "31 train 44 540\n",
      "Phase: train. Epoch: 31. Loss: 0.06857549399137497\n",
      "31 train 45 552\n",
      "Phase: train. Epoch: 31. Loss: 0.08142106980085373\n",
      "31 train 46 564\n",
      "Phase: train. Epoch: 31. Loss: 0.10343743860721588\n",
      "31 train 47 576\n",
      "Phase: train. Epoch: 31. Loss: 0.08140157908201218\n",
      "31 train 48 588\n",
      "Phase: train. Epoch: 31. Loss: 0.12176460027694702\n",
      "31 train 49 600\n",
      "Phase: train. Epoch: 31. Loss: 0.0967218279838562\n",
      "31 train 50 612\n",
      "Phase: train. Epoch: 31. Loss: 0.06710594892501831\n",
      "31 train 51 624\n",
      "Phase: train. Epoch: 31. Loss: 0.07392917573451996\n",
      "31 train 52 636\n",
      "Phase: train. Epoch: 31. Loss: 0.08554212749004364\n",
      "31 train 53 648\n",
      "Phase: train. Epoch: 31. Loss: 0.07153931260108948\n",
      "31 train 54 660\n",
      "Phase: train. Epoch: 31. Loss: 0.0951806902885437\n",
      "31 train 55 672\n",
      "Phase: train. Epoch: 31. Loss: 0.08693372458219528\n",
      "31 train 56 684\n",
      "Phase: train. Epoch: 31. Loss: 0.0913533866405487\n",
      "31 train 57 696\n",
      "Phase: train. Epoch: 31. Loss: 0.1018231138586998\n",
      "31 train 58 708\n",
      "Phase: train. Epoch: 31. Loss: 0.11366050690412521\n",
      "31 train 59 720\n",
      "Phase: train. Epoch: 31. Loss: 0.07790054380893707\n",
      "31 train 60 732\n",
      "Phase: train. Epoch: 31. Loss: 0.08348093926906586\n",
      "31 train 61 744\n",
      "Phase: train. Epoch: 31. Loss: 0.09957264363765717\n",
      "31 train 62 751\n",
      "Phase: train. Epoch: 31. Loss: 0.0781765952706337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 val 0 763\n",
      "Phase: val. Epoch: 31. Loss: 0.09205728769302368\n",
      "31 val 1 775\n",
      "Phase: val. Epoch: 31. Loss: 0.09719358384609222\n",
      "31 val 2 787\n",
      "Phase: val. Epoch: 31. Loss: 0.09204676747322083\n",
      "31 val 3 799\n",
      "Phase: val. Epoch: 31. Loss: 0.0890006572008133\n",
      "31 val 4 811\n",
      "Phase: val. Epoch: 31. Loss: 0.08152332901954651\n",
      "31 val 5 823\n",
      "Phase: val. Epoch: 31. Loss: 0.06538516283035278\n",
      "31 val 6 835\n",
      "Phase: val. Epoch: 31. Loss: 0.0852527916431427\n",
      "31 val 7 847\n",
      "Phase: val. Epoch: 31. Loss: 0.0690840482711792\n",
      "31 val 8 859\n",
      "Phase: val. Epoch: 31. Loss: 0.10686451196670532\n",
      "31 val 9 871\n",
      "Phase: val. Epoch: 31. Loss: 0.0789700299501419\n",
      "31 val 10 883\n",
      "Phase: val. Epoch: 31. Loss: 0.07974541187286377\n",
      "31 val 11 884\n",
      "Phase: val. Epoch: 31. Loss: 0.02959999442100525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 train 0 12\n",
      "Phase: train. Epoch: 32. Loss: 0.0642227753996849\n",
      "32 train 1 24\n",
      "Phase: train. Epoch: 32. Loss: 0.10123944282531738\n",
      "32 train 2 36\n",
      "Phase: train. Epoch: 32. Loss: 0.08079010993242264\n",
      "32 train 3 48\n",
      "Phase: train. Epoch: 32. Loss: 0.10324953496456146\n",
      "32 train 4 60\n",
      "Phase: train. Epoch: 32. Loss: 0.07887141406536102\n",
      "32 train 5 72\n",
      "Phase: train. Epoch: 32. Loss: 0.09744185209274292\n",
      "32 train 6 84\n",
      "Phase: train. Epoch: 32. Loss: 0.09325961768627167\n",
      "32 train 7 96\n",
      "Phase: train. Epoch: 32. Loss: 0.10062279552221298\n",
      "32 train 8 108\n",
      "Phase: train. Epoch: 32. Loss: 0.08746704459190369\n",
      "32 train 9 120\n",
      "Phase: train. Epoch: 32. Loss: 0.09280572831630707\n",
      "32 train 10 132\n",
      "Phase: train. Epoch: 32. Loss: 0.08510491251945496\n",
      "32 train 11 144\n",
      "Phase: train. Epoch: 32. Loss: 0.06779322028160095\n",
      "32 train 12 156\n",
      "Phase: train. Epoch: 32. Loss: 0.08995743095874786\n",
      "32 train 13 168\n",
      "Phase: train. Epoch: 32. Loss: 0.09512338042259216\n",
      "32 train 14 180\n",
      "Phase: train. Epoch: 32. Loss: 0.08097495138645172\n",
      "32 train 15 192\n",
      "Phase: train. Epoch: 32. Loss: 0.0738154947757721\n",
      "32 train 16 204\n",
      "Phase: train. Epoch: 32. Loss: 0.08209608495235443\n",
      "32 train 17 216\n",
      "Phase: train. Epoch: 32. Loss: 0.0752728134393692\n",
      "32 train 18 228\n",
      "Phase: train. Epoch: 32. Loss: 0.10599236935377121\n",
      "32 train 19 240\n",
      "Phase: train. Epoch: 32. Loss: 0.08768460154533386\n",
      "32 train 20 252\n",
      "Phase: train. Epoch: 32. Loss: 0.09023335576057434\n",
      "32 train 21 264\n",
      "Phase: train. Epoch: 32. Loss: 0.10146792232990265\n",
      "32 train 22 276\n",
      "Phase: train. Epoch: 32. Loss: 0.1250535249710083\n",
      "32 train 23 288\n",
      "Phase: train. Epoch: 32. Loss: 0.07364062964916229\n",
      "32 train 24 300\n",
      "Phase: train. Epoch: 32. Loss: 0.08787821233272552\n",
      "32 train 25 312\n",
      "Phase: train. Epoch: 32. Loss: 0.09254980087280273\n",
      "32 train 26 324\n",
      "Phase: train. Epoch: 32. Loss: 0.09641341865062714\n",
      "32 train 27 336\n",
      "Phase: train. Epoch: 32. Loss: 0.08381803333759308\n",
      "32 train 28 348\n",
      "Phase: train. Epoch: 32. Loss: 0.08993886411190033\n",
      "32 train 29 360\n",
      "Phase: train. Epoch: 32. Loss: 0.09608478844165802\n",
      "32 train 30 372\n",
      "Phase: train. Epoch: 32. Loss: 0.08383123576641083\n",
      "32 train 31 384\n",
      "Phase: train. Epoch: 32. Loss: 0.0749378427863121\n",
      "32 train 32 396\n",
      "Phase: train. Epoch: 32. Loss: 0.08308662474155426\n",
      "32 train 33 408\n",
      "Phase: train. Epoch: 32. Loss: 0.06877162307500839\n",
      "32 train 34 420\n",
      "Phase: train. Epoch: 32. Loss: 0.12345313280820847\n",
      "32 train 35 432\n",
      "Phase: train. Epoch: 32. Loss: 0.10076610743999481\n",
      "32 train 36 444\n",
      "Phase: train. Epoch: 32. Loss: 0.0776415467262268\n",
      "32 train 37 456\n",
      "Phase: train. Epoch: 32. Loss: 0.12363561987876892\n",
      "32 train 38 468\n",
      "Phase: train. Epoch: 32. Loss: 0.07441240549087524\n",
      "32 train 39 480\n",
      "Phase: train. Epoch: 32. Loss: 0.0682879313826561\n",
      "32 train 40 492\n",
      "Phase: train. Epoch: 32. Loss: 0.07602773606777191\n",
      "32 train 41 504\n",
      "Phase: train. Epoch: 32. Loss: 0.09636364877223969\n",
      "32 train 42 516\n",
      "Phase: train. Epoch: 32. Loss: 0.07519140839576721\n",
      "32 train 43 528\n",
      "Phase: train. Epoch: 32. Loss: 0.09877877682447433\n",
      "32 train 44 540\n",
      "Phase: train. Epoch: 32. Loss: 0.08413301408290863\n",
      "32 train 45 552\n",
      "Phase: train. Epoch: 32. Loss: 0.09837289154529572\n",
      "32 train 46 564\n",
      "Phase: train. Epoch: 32. Loss: 0.07318542152643204\n",
      "32 train 47 576\n",
      "Phase: train. Epoch: 32. Loss: 0.07882001996040344\n",
      "32 train 48 588\n",
      "Phase: train. Epoch: 32. Loss: 0.09532232582569122\n",
      "32 train 49 600\n",
      "Phase: train. Epoch: 32. Loss: 0.09383589029312134\n",
      "32 train 50 612\n",
      "Phase: train. Epoch: 32. Loss: 0.10727886855602264\n",
      "32 train 51 624\n",
      "Phase: train. Epoch: 32. Loss: 0.09923066943883896\n",
      "32 train 52 636\n",
      "Phase: train. Epoch: 32. Loss: 0.07543422281742096\n",
      "32 train 53 648\n",
      "Phase: train. Epoch: 32. Loss: 0.10257430374622345\n",
      "32 train 54 660\n",
      "Phase: train. Epoch: 32. Loss: 0.07429556548595428\n",
      "32 train 55 672\n",
      "Phase: train. Epoch: 32. Loss: 0.08392338454723358\n",
      "32 train 56 684\n",
      "Phase: train. Epoch: 32. Loss: 0.08533211797475815\n",
      "32 train 57 696\n",
      "Phase: train. Epoch: 32. Loss: 0.07428177446126938\n",
      "32 train 58 708\n",
      "Phase: train. Epoch: 32. Loss: 0.12702247500419617\n",
      "32 train 59 720\n",
      "Phase: train. Epoch: 32. Loss: 0.1078181117773056\n",
      "32 train 60 732\n",
      "Phase: train. Epoch: 32. Loss: 0.08700113743543625\n",
      "32 train 61 744\n",
      "Phase: train. Epoch: 32. Loss: 0.08203868567943573\n",
      "32 train 62 751\n",
      "Phase: train. Epoch: 32. Loss: 0.12183600664138794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 val 0 763\n",
      "Phase: val. Epoch: 32. Loss: 0.07501937448978424\n",
      "32 val 1 775\n",
      "Phase: val. Epoch: 32. Loss: 0.08816039562225342\n",
      "32 val 2 787\n",
      "Phase: val. Epoch: 32. Loss: 0.10270808637142181\n",
      "32 val 3 799\n",
      "Phase: val. Epoch: 32. Loss: 0.08659239113330841\n",
      "32 val 4 811\n",
      "Phase: val. Epoch: 32. Loss: 0.07707752287387848\n",
      "32 val 5 823\n",
      "Phase: val. Epoch: 32. Loss: 0.07720763981342316\n",
      "32 val 6 835\n",
      "Phase: val. Epoch: 32. Loss: 0.07990079373121262\n",
      "32 val 7 847\n",
      "Phase: val. Epoch: 32. Loss: 0.07529334723949432\n",
      "32 val 8 859\n",
      "Phase: val. Epoch: 32. Loss: 0.10134607553482056\n",
      "32 val 9 871\n",
      "Phase: val. Epoch: 32. Loss: 0.09074564278125763\n",
      "32 val 10 883\n",
      "Phase: val. Epoch: 32. Loss: 0.10204726457595825\n",
      "32 val 11 884\n",
      "Phase: val. Epoch: 32. Loss: 0.09820801019668579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 train 0 12\n",
      "Phase: train. Epoch: 33. Loss: 0.0962822437286377\n",
      "33 train 1 24\n",
      "Phase: train. Epoch: 33. Loss: 0.06542526185512543\n",
      "33 train 2 36\n",
      "Phase: train. Epoch: 33. Loss: 0.09788151830434799\n",
      "33 train 3 48\n",
      "Phase: train. Epoch: 33. Loss: 0.09948955476284027\n",
      "33 train 4 60\n",
      "Phase: train. Epoch: 33. Loss: 0.0774395763874054\n",
      "33 train 5 72\n",
      "Phase: train. Epoch: 33. Loss: 0.07512630522251129\n",
      "33 train 6 84\n",
      "Phase: train. Epoch: 33. Loss: 0.08374923467636108\n",
      "33 train 7 96\n",
      "Phase: train. Epoch: 33. Loss: 0.08411824703216553\n",
      "33 train 8 108\n",
      "Phase: train. Epoch: 33. Loss: 0.08928334712982178\n",
      "33 train 9 120\n",
      "Phase: train. Epoch: 33. Loss: 0.08460891246795654\n",
      "33 train 10 132\n",
      "Phase: train. Epoch: 33. Loss: 0.07145549356937408\n",
      "33 train 11 144\n",
      "Phase: train. Epoch: 33. Loss: 0.08844225108623505\n",
      "33 train 12 156\n",
      "Phase: train. Epoch: 33. Loss: 0.07333627343177795\n",
      "33 train 13 168\n",
      "Phase: train. Epoch: 33. Loss: 0.08880149573087692\n",
      "33 train 14 180\n",
      "Phase: train. Epoch: 33. Loss: 0.07930537313222885\n",
      "33 train 15 192\n",
      "Phase: train. Epoch: 33. Loss: 0.08345067501068115\n",
      "33 train 16 204\n",
      "Phase: train. Epoch: 33. Loss: 0.06587238609790802\n",
      "33 train 17 216\n",
      "Phase: train. Epoch: 33. Loss: 0.10173381865024567\n",
      "33 train 18 228\n",
      "Phase: train. Epoch: 33. Loss: 0.07421763241291046\n",
      "33 train 19 240\n",
      "Phase: train. Epoch: 33. Loss: 0.11544107645750046\n",
      "33 train 20 252\n",
      "Phase: train. Epoch: 33. Loss: 0.11017163842916489\n",
      "33 train 21 264\n",
      "Phase: train. Epoch: 33. Loss: 0.08250516653060913\n",
      "33 train 22 276\n",
      "Phase: train. Epoch: 33. Loss: 0.10930909961462021\n",
      "33 train 23 288\n",
      "Phase: train. Epoch: 33. Loss: 0.08645904064178467\n",
      "33 train 24 300\n",
      "Phase: train. Epoch: 33. Loss: 0.0793977677822113\n",
      "33 train 25 312\n",
      "Phase: train. Epoch: 33. Loss: 0.07149096578359604\n",
      "33 train 26 324\n",
      "Phase: train. Epoch: 33. Loss: 0.08988271653652191\n",
      "33 train 27 336\n",
      "Phase: train. Epoch: 33. Loss: 0.07569783926010132\n",
      "33 train 28 348\n",
      "Phase: train. Epoch: 33. Loss: 0.09757527709007263\n",
      "33 train 29 360\n",
      "Phase: train. Epoch: 33. Loss: 0.09085290133953094\n",
      "33 train 30 372\n",
      "Phase: train. Epoch: 33. Loss: 0.09503092616796494\n",
      "33 train 31 384\n",
      "Phase: train. Epoch: 33. Loss: 0.09305095672607422\n",
      "33 train 32 396\n",
      "Phase: train. Epoch: 33. Loss: 0.0972505658864975\n",
      "33 train 33 408\n",
      "Phase: train. Epoch: 33. Loss: 0.07779088616371155\n",
      "33 train 34 420\n",
      "Phase: train. Epoch: 33. Loss: 0.09066669642925262\n",
      "33 train 35 432\n",
      "Phase: train. Epoch: 33. Loss: 0.09487771987915039\n",
      "33 train 36 444\n",
      "Phase: train. Epoch: 33. Loss: 0.09532805532217026\n",
      "33 train 37 456\n",
      "Phase: train. Epoch: 33. Loss: 0.08123184740543365\n",
      "33 train 38 468\n",
      "Phase: train. Epoch: 33. Loss: 0.10786671936511993\n",
      "33 train 39 480\n",
      "Phase: train. Epoch: 33. Loss: 0.12029463052749634\n",
      "33 train 40 492\n",
      "Phase: train. Epoch: 33. Loss: 0.08750727027654648\n",
      "33 train 41 504\n",
      "Phase: train. Epoch: 33. Loss: 0.10793518275022507\n",
      "33 train 42 516\n",
      "Phase: train. Epoch: 33. Loss: 0.12043928354978561\n",
      "33 train 43 528\n",
      "Phase: train. Epoch: 33. Loss: 0.11241989582777023\n",
      "33 train 44 540\n",
      "Phase: train. Epoch: 33. Loss: 0.07681884616613388\n",
      "33 train 45 552\n",
      "Phase: train. Epoch: 33. Loss: 0.09728036820888519\n",
      "33 train 46 564\n",
      "Phase: train. Epoch: 33. Loss: 0.08057522773742676\n",
      "33 train 47 576\n",
      "Phase: train. Epoch: 33. Loss: 0.07825280725955963\n",
      "33 train 48 588\n",
      "Phase: train. Epoch: 33. Loss: 0.06155182048678398\n",
      "33 train 49 600\n",
      "Phase: train. Epoch: 33. Loss: 0.08933841437101364\n",
      "33 train 50 612\n",
      "Phase: train. Epoch: 33. Loss: 0.10737069696187973\n",
      "33 train 51 624\n",
      "Phase: train. Epoch: 33. Loss: 0.09125920385122299\n",
      "33 train 52 636\n",
      "Phase: train. Epoch: 33. Loss: 0.08773289620876312\n",
      "33 train 53 648\n",
      "Phase: train. Epoch: 33. Loss: 0.07220171391963959\n",
      "33 train 54 660\n",
      "Phase: train. Epoch: 33. Loss: 0.08749054372310638\n",
      "33 train 55 672\n",
      "Phase: train. Epoch: 33. Loss: 0.10348054021596909\n",
      "33 train 56 684\n",
      "Phase: train. Epoch: 33. Loss: 0.10034124553203583\n",
      "33 train 57 696\n",
      "Phase: train. Epoch: 33. Loss: 0.09017211198806763\n",
      "33 train 58 708\n",
      "Phase: train. Epoch: 33. Loss: 0.07948541641235352\n",
      "33 train 59 720\n",
      "Phase: train. Epoch: 33. Loss: 0.08556266874074936\n",
      "33 train 60 732\n",
      "Phase: train. Epoch: 33. Loss: 0.09577115625143051\n",
      "33 train 61 744\n",
      "Phase: train. Epoch: 33. Loss: 0.07390492409467697\n",
      "33 train 62 751\n",
      "Phase: train. Epoch: 33. Loss: 0.08655192703008652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 val 0 763\n",
      "Phase: val. Epoch: 33. Loss: 0.05010750889778137\n",
      "33 val 1 775\n",
      "Phase: val. Epoch: 33. Loss: 0.07131309807300568\n",
      "33 val 2 787\n",
      "Phase: val. Epoch: 33. Loss: 0.08547171205282211\n",
      "33 val 3 799\n",
      "Phase: val. Epoch: 33. Loss: 0.0997360497713089\n",
      "33 val 4 811\n",
      "Phase: val. Epoch: 33. Loss: 0.08406176418066025\n",
      "33 val 5 823\n",
      "Phase: val. Epoch: 33. Loss: 0.08749958127737045\n",
      "33 val 6 835\n",
      "Phase: val. Epoch: 33. Loss: 0.06377147138118744\n",
      "33 val 7 847\n",
      "Phase: val. Epoch: 33. Loss: 0.08627250045537949\n",
      "33 val 8 859\n",
      "Phase: val. Epoch: 33. Loss: 0.06754814088344574\n",
      "33 val 9 871\n",
      "Phase: val. Epoch: 33. Loss: 0.08457629382610321\n",
      "33 val 10 883\n",
      "Phase: val. Epoch: 33. Loss: 0.13012313842773438\n",
      "33 val 11 884\n",
      "Phase: val. Epoch: 33. Loss: 0.12056977301836014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 train 0 12\n",
      "Phase: train. Epoch: 34. Loss: 0.08921970427036285\n",
      "34 train 1 24\n",
      "Phase: train. Epoch: 34. Loss: 0.105951689183712\n",
      "34 train 2 36\n",
      "Phase: train. Epoch: 34. Loss: 0.06967276334762573\n",
      "34 train 3 48\n",
      "Phase: train. Epoch: 34. Loss: 0.09281470626592636\n",
      "34 train 4 60\n",
      "Phase: train. Epoch: 34. Loss: 0.079353466629982\n",
      "34 train 5 72\n",
      "Phase: train. Epoch: 34. Loss: 0.08198712766170502\n",
      "34 train 6 84\n",
      "Phase: train. Epoch: 34. Loss: 0.06510802358388901\n",
      "34 train 7 96\n",
      "Phase: train. Epoch: 34. Loss: 0.10137735307216644\n",
      "34 train 8 108\n",
      "Phase: train. Epoch: 34. Loss: 0.10233261436223984\n",
      "34 train 9 120\n",
      "Phase: train. Epoch: 34. Loss: 0.07471951842308044\n",
      "34 train 10 132\n",
      "Phase: train. Epoch: 34. Loss: 0.08755283057689667\n",
      "34 train 11 144\n",
      "Phase: train. Epoch: 34. Loss: 0.05836617946624756\n",
      "34 train 12 156\n",
      "Phase: train. Epoch: 34. Loss: 0.1018633097410202\n",
      "34 train 13 168\n",
      "Phase: train. Epoch: 34. Loss: 0.10968741029500961\n",
      "34 train 14 180\n",
      "Phase: train. Epoch: 34. Loss: 0.09412332624197006\n",
      "34 train 15 192\n",
      "Phase: train. Epoch: 34. Loss: 0.07744516432285309\n",
      "34 train 16 204\n",
      "Phase: train. Epoch: 34. Loss: 0.08076292276382446\n",
      "34 train 17 216\n",
      "Phase: train. Epoch: 34. Loss: 0.09265335649251938\n",
      "34 train 18 228\n",
      "Phase: train. Epoch: 34. Loss: 0.10666805505752563\n",
      "34 train 19 240\n",
      "Phase: train. Epoch: 34. Loss: 0.07632119208574295\n",
      "34 train 20 252\n",
      "Phase: train. Epoch: 34. Loss: 0.08785125613212585\n",
      "34 train 21 264\n",
      "Phase: train. Epoch: 34. Loss: 0.09013822674751282\n",
      "34 train 22 276\n",
      "Phase: train. Epoch: 34. Loss: 0.08603262901306152\n",
      "34 train 23 288\n",
      "Phase: train. Epoch: 34. Loss: 0.1230364665389061\n",
      "34 train 24 300\n",
      "Phase: train. Epoch: 34. Loss: 0.07152852416038513\n",
      "34 train 25 312\n",
      "Phase: train. Epoch: 34. Loss: 0.0821550041437149\n",
      "34 train 26 324\n",
      "Phase: train. Epoch: 34. Loss: 0.06960795819759369\n",
      "34 train 27 336\n",
      "Phase: train. Epoch: 34. Loss: 0.07692258805036545\n",
      "34 train 28 348\n",
      "Phase: train. Epoch: 34. Loss: 0.09862568974494934\n",
      "34 train 29 360\n",
      "Phase: train. Epoch: 34. Loss: 0.08954676240682602\n",
      "34 train 30 372\n",
      "Phase: train. Epoch: 34. Loss: 0.09671586006879807\n",
      "34 train 31 384\n",
      "Phase: train. Epoch: 34. Loss: 0.10276204347610474\n",
      "34 train 32 396\n",
      "Phase: train. Epoch: 34. Loss: 0.09667404741048813\n",
      "34 train 33 408\n",
      "Phase: train. Epoch: 34. Loss: 0.09775598347187042\n",
      "34 train 34 420\n",
      "Phase: train. Epoch: 34. Loss: 0.1162000298500061\n",
      "34 train 35 432\n",
      "Phase: train. Epoch: 34. Loss: 0.07507449388504028\n",
      "34 train 36 444\n",
      "Phase: train. Epoch: 34. Loss: 0.07834528386592865\n",
      "34 train 37 456\n",
      "Phase: train. Epoch: 34. Loss: 0.07398869842290878\n",
      "34 train 38 468\n",
      "Phase: train. Epoch: 34. Loss: 0.0897941142320633\n",
      "34 train 39 480\n",
      "Phase: train. Epoch: 34. Loss: 0.08023461699485779\n",
      "34 train 40 492\n",
      "Phase: train. Epoch: 34. Loss: 0.09671928733587265\n",
      "34 train 41 504\n",
      "Phase: train. Epoch: 34. Loss: 0.09970255196094513\n",
      "34 train 42 516\n",
      "Phase: train. Epoch: 34. Loss: 0.10691547393798828\n",
      "34 train 43 528\n",
      "Phase: train. Epoch: 34. Loss: 0.08206699788570404\n",
      "34 train 44 540\n",
      "Phase: train. Epoch: 34. Loss: 0.1310034841299057\n",
      "34 train 45 552\n",
      "Phase: train. Epoch: 34. Loss: 0.08042898774147034\n",
      "34 train 46 564\n",
      "Phase: train. Epoch: 34. Loss: 0.09661711752414703\n",
      "34 train 47 576\n",
      "Phase: train. Epoch: 34. Loss: 0.08813303709030151\n",
      "34 train 48 588\n",
      "Phase: train. Epoch: 34. Loss: 0.08994793891906738\n",
      "34 train 49 600\n",
      "Phase: train. Epoch: 34. Loss: 0.10022246837615967\n",
      "34 train 50 612\n",
      "Phase: train. Epoch: 34. Loss: 0.07545468956232071\n",
      "34 train 51 624\n",
      "Phase: train. Epoch: 34. Loss: 0.0855952724814415\n",
      "34 train 52 636\n",
      "Phase: train. Epoch: 34. Loss: 0.07054866850376129\n",
      "34 train 53 648\n",
      "Phase: train. Epoch: 34. Loss: 0.09077312052249908\n",
      "34 train 54 660\n",
      "Phase: train. Epoch: 34. Loss: 0.07023380696773529\n",
      "34 train 55 672\n",
      "Phase: train. Epoch: 34. Loss: 0.0933132991194725\n",
      "34 train 56 684\n",
      "Phase: train. Epoch: 34. Loss: 0.07255931198596954\n",
      "34 train 57 696\n",
      "Phase: train. Epoch: 34. Loss: 0.09230099618434906\n",
      "34 train 58 708\n",
      "Phase: train. Epoch: 34. Loss: 0.122189000248909\n",
      "34 train 59 720\n",
      "Phase: train. Epoch: 34. Loss: 0.07470189034938812\n",
      "34 train 60 732\n",
      "Phase: train. Epoch: 34. Loss: 0.09615308046340942\n",
      "34 train 61 744\n",
      "Phase: train. Epoch: 34. Loss: 0.0795711800456047\n",
      "34 train 62 751\n",
      "Phase: train. Epoch: 34. Loss: 0.07501069456338882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 val 0 763\n",
      "Phase: val. Epoch: 34. Loss: 0.12836000323295593\n",
      "34 val 1 775\n",
      "Phase: val. Epoch: 34. Loss: 0.06911805272102356\n",
      "34 val 2 787\n",
      "Phase: val. Epoch: 34. Loss: 0.06941595673561096\n",
      "34 val 3 799\n",
      "Phase: val. Epoch: 34. Loss: 0.06459817290306091\n",
      "34 val 4 811\n",
      "Phase: val. Epoch: 34. Loss: 0.1018783450126648\n",
      "34 val 5 823\n",
      "Phase: val. Epoch: 34. Loss: 0.06550528854131699\n",
      "34 val 6 835\n",
      "Phase: val. Epoch: 34. Loss: 0.06779851019382477\n",
      "34 val 7 847\n",
      "Phase: val. Epoch: 34. Loss: 0.09571848809719086\n",
      "34 val 8 859\n",
      "Phase: val. Epoch: 34. Loss: 0.10116740316152573\n",
      "34 val 9 871\n",
      "Phase: val. Epoch: 34. Loss: 0.06427463889122009\n",
      "34 val 10 883\n",
      "Phase: val. Epoch: 34. Loss: 0.11382047832012177\n",
      "34 val 11 884\n",
      "Phase: val. Epoch: 34. Loss: 0.08961939811706543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 train 0 12\n",
      "Phase: train. Epoch: 35. Loss: 0.07227832078933716\n",
      "35 train 1 24\n",
      "Phase: train. Epoch: 35. Loss: 0.09598096460103989\n",
      "35 train 2 36\n",
      "Phase: train. Epoch: 35. Loss: 0.08967845141887665\n",
      "35 train 3 48\n",
      "Phase: train. Epoch: 35. Loss: 0.08916428685188293\n",
      "35 train 4 60\n",
      "Phase: train. Epoch: 35. Loss: 0.07058074325323105\n",
      "35 train 5 72\n",
      "Phase: train. Epoch: 35. Loss: 0.1206049844622612\n",
      "35 train 6 84\n",
      "Phase: train. Epoch: 35. Loss: 0.07562211155891418\n",
      "35 train 7 96\n",
      "Phase: train. Epoch: 35. Loss: 0.09037746489048004\n",
      "35 train 8 108\n",
      "Phase: train. Epoch: 35. Loss: 0.09722918272018433\n",
      "35 train 9 120\n",
      "Phase: train. Epoch: 35. Loss: 0.07753929495811462\n",
      "35 train 10 132\n",
      "Phase: train. Epoch: 35. Loss: 0.08034403622150421\n",
      "35 train 11 144\n",
      "Phase: train. Epoch: 35. Loss: 0.14366291463375092\n",
      "35 train 12 156\n",
      "Phase: train. Epoch: 35. Loss: 0.07791264355182648\n",
      "35 train 13 168\n",
      "Phase: train. Epoch: 35. Loss: 0.09211846441030502\n",
      "35 train 14 180\n",
      "Phase: train. Epoch: 35. Loss: 0.07481999695301056\n",
      "35 train 15 192\n",
      "Phase: train. Epoch: 35. Loss: 0.10417664051055908\n",
      "35 train 16 204\n",
      "Phase: train. Epoch: 35. Loss: 0.1009141355752945\n",
      "35 train 17 216\n",
      "Phase: train. Epoch: 35. Loss: 0.07397725433111191\n",
      "35 train 18 228\n",
      "Phase: train. Epoch: 35. Loss: 0.09958874434232712\n",
      "35 train 19 240\n",
      "Phase: train. Epoch: 35. Loss: 0.08322961628437042\n",
      "35 train 20 252\n",
      "Phase: train. Epoch: 35. Loss: 0.07394769042730331\n",
      "35 train 21 264\n",
      "Phase: train. Epoch: 35. Loss: 0.07270385324954987\n",
      "35 train 22 276\n",
      "Phase: train. Epoch: 35. Loss: 0.064496248960495\n",
      "35 train 23 288\n",
      "Phase: train. Epoch: 35. Loss: 0.0829835832118988\n",
      "35 train 24 300\n",
      "Phase: train. Epoch: 35. Loss: 0.08341924846172333\n",
      "35 train 25 312\n",
      "Phase: train. Epoch: 35. Loss: 0.0742243081331253\n",
      "35 train 26 324\n",
      "Phase: train. Epoch: 35. Loss: 0.10290869325399399\n",
      "35 train 27 336\n",
      "Phase: train. Epoch: 35. Loss: 0.09391956031322479\n",
      "35 train 28 348\n",
      "Phase: train. Epoch: 35. Loss: 0.10603949427604675\n",
      "35 train 29 360\n",
      "Phase: train. Epoch: 35. Loss: 0.07936820387840271\n",
      "35 train 30 372\n",
      "Phase: train. Epoch: 35. Loss: 0.07669004797935486\n",
      "35 train 31 384\n",
      "Phase: train. Epoch: 35. Loss: 0.10776448249816895\n",
      "35 train 32 396\n",
      "Phase: train. Epoch: 35. Loss: 0.10775792598724365\n",
      "35 train 33 408\n",
      "Phase: train. Epoch: 35. Loss: 0.10068224370479584\n",
      "35 train 34 420\n",
      "Phase: train. Epoch: 35. Loss: 0.06529049575328827\n",
      "35 train 35 432\n",
      "Phase: train. Epoch: 35. Loss: 0.09198999404907227\n",
      "35 train 36 444\n",
      "Phase: train. Epoch: 35. Loss: 0.08894136548042297\n",
      "35 train 37 456\n",
      "Phase: train. Epoch: 35. Loss: 0.06480526924133301\n",
      "35 train 38 468\n",
      "Phase: train. Epoch: 35. Loss: 0.09358444064855576\n",
      "35 train 39 480\n",
      "Phase: train. Epoch: 35. Loss: 0.08577722311019897\n",
      "35 train 40 492\n",
      "Phase: train. Epoch: 35. Loss: 0.10177996009588242\n",
      "35 train 41 504\n",
      "Phase: train. Epoch: 35. Loss: 0.0939735472202301\n",
      "35 train 42 516\n",
      "Phase: train. Epoch: 35. Loss: 0.10670630633831024\n",
      "35 train 43 528\n",
      "Phase: train. Epoch: 35. Loss: 0.08303885906934738\n",
      "35 train 44 540\n",
      "Phase: train. Epoch: 35. Loss: 0.08572682738304138\n",
      "35 train 45 552\n",
      "Phase: train. Epoch: 35. Loss: 0.12036685645580292\n",
      "35 train 46 564\n",
      "Phase: train. Epoch: 35. Loss: 0.07888850569725037\n",
      "35 train 47 576\n",
      "Phase: train. Epoch: 35. Loss: 0.09361070394515991\n",
      "35 train 48 588\n",
      "Phase: train. Epoch: 35. Loss: 0.0950135588645935\n",
      "35 train 49 600\n",
      "Phase: train. Epoch: 35. Loss: 0.09994509071111679\n",
      "35 train 50 612\n",
      "Phase: train. Epoch: 35. Loss: 0.08411100506782532\n",
      "35 train 51 624\n",
      "Phase: train. Epoch: 35. Loss: 0.08500140905380249\n",
      "35 train 52 636\n",
      "Phase: train. Epoch: 35. Loss: 0.09710638225078583\n",
      "35 train 53 648\n",
      "Phase: train. Epoch: 35. Loss: 0.10223513841629028\n",
      "35 train 54 660\n",
      "Phase: train. Epoch: 35. Loss: 0.08166054636240005\n",
      "35 train 55 672\n",
      "Phase: train. Epoch: 35. Loss: 0.07847767323255539\n",
      "35 train 56 684\n",
      "Phase: train. Epoch: 35. Loss: 0.09969829767942429\n",
      "35 train 57 696\n",
      "Phase: train. Epoch: 35. Loss: 0.06872978806495667\n",
      "35 train 58 708\n",
      "Phase: train. Epoch: 35. Loss: 0.08364363759756088\n",
      "35 train 59 720\n",
      "Phase: train. Epoch: 35. Loss: 0.07339870929718018\n",
      "35 train 60 732\n",
      "Phase: train. Epoch: 35. Loss: 0.09391537308692932\n",
      "35 train 61 744\n",
      "Phase: train. Epoch: 35. Loss: 0.11300963163375854\n",
      "35 train 62 751\n",
      "Phase: train. Epoch: 35. Loss: 0.08768681436777115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 val 0 763\n",
      "Phase: val. Epoch: 35. Loss: 0.06837447732686996\n",
      "35 val 1 775\n",
      "Phase: val. Epoch: 35. Loss: 0.07053770124912262\n",
      "35 val 2 787\n",
      "Phase: val. Epoch: 35. Loss: 0.0948830246925354\n",
      "35 val 3 799\n",
      "Phase: val. Epoch: 35. Loss: 0.07806181907653809\n",
      "35 val 4 811\n",
      "Phase: val. Epoch: 35. Loss: 0.07863466441631317\n",
      "35 val 5 823\n",
      "Phase: val. Epoch: 35. Loss: 0.0725911408662796\n",
      "35 val 6 835\n",
      "Phase: val. Epoch: 35. Loss: 0.08055858314037323\n",
      "35 val 7 847\n",
      "Phase: val. Epoch: 35. Loss: 0.09396135807037354\n",
      "35 val 8 859\n",
      "Phase: val. Epoch: 35. Loss: 0.10011907666921616\n",
      "35 val 9 871\n",
      "Phase: val. Epoch: 35. Loss: 0.08662247657775879\n",
      "35 val 10 883\n",
      "Phase: val. Epoch: 35. Loss: 0.09223033487796783\n",
      "35 val 11 884\n",
      "Phase: val. Epoch: 35. Loss: 0.10954442620277405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 train 0 12\n",
      "Phase: train. Epoch: 36. Loss: 0.09234419465065002\n",
      "36 train 1 24\n",
      "Phase: train. Epoch: 36. Loss: 0.07741301506757736\n",
      "36 train 2 36\n",
      "Phase: train. Epoch: 36. Loss: 0.07018719613552094\n",
      "36 train 3 48\n",
      "Phase: train. Epoch: 36. Loss: 0.0791788175702095\n",
      "36 train 4 60\n",
      "Phase: train. Epoch: 36. Loss: 0.13312070071697235\n",
      "36 train 5 72\n",
      "Phase: train. Epoch: 36. Loss: 0.07038478553295135\n",
      "36 train 6 84\n",
      "Phase: train. Epoch: 36. Loss: 0.0849292129278183\n",
      "36 train 7 96\n",
      "Phase: train. Epoch: 36. Loss: 0.07412843406200409\n",
      "36 train 8 108\n",
      "Phase: train. Epoch: 36. Loss: 0.08278846740722656\n",
      "36 train 9 120\n",
      "Phase: train. Epoch: 36. Loss: 0.09723847359418869\n",
      "36 train 10 132\n",
      "Phase: train. Epoch: 36. Loss: 0.13283222913742065\n",
      "36 train 11 144\n",
      "Phase: train. Epoch: 36. Loss: 0.10870995372533798\n",
      "36 train 12 156\n",
      "Phase: train. Epoch: 36. Loss: 0.09340155124664307\n",
      "36 train 13 168\n",
      "Phase: train. Epoch: 36. Loss: 0.09667622298002243\n",
      "36 train 14 180\n",
      "Phase: train. Epoch: 36. Loss: 0.1075873076915741\n",
      "36 train 15 192\n",
      "Phase: train. Epoch: 36. Loss: 0.09348221868276596\n",
      "36 train 16 204\n",
      "Phase: train. Epoch: 36. Loss: 0.11044934391975403\n",
      "36 train 17 216\n",
      "Phase: train. Epoch: 36. Loss: 0.0729885995388031\n",
      "36 train 18 228\n",
      "Phase: train. Epoch: 36. Loss: 0.10351823270320892\n",
      "36 train 19 240\n",
      "Phase: train. Epoch: 36. Loss: 0.09002421796321869\n",
      "36 train 20 252\n",
      "Phase: train. Epoch: 36. Loss: 0.13152843713760376\n",
      "36 train 21 264\n",
      "Phase: train. Epoch: 36. Loss: 0.07795973122119904\n",
      "36 train 22 276\n",
      "Phase: train. Epoch: 36. Loss: 0.08221392333507538\n",
      "36 train 23 288\n",
      "Phase: train. Epoch: 36. Loss: 0.08130519837141037\n",
      "36 train 24 300\n",
      "Phase: train. Epoch: 36. Loss: 0.08354001492261887\n",
      "36 train 25 312\n",
      "Phase: train. Epoch: 36. Loss: 0.07757732272148132\n",
      "36 train 26 324\n",
      "Phase: train. Epoch: 36. Loss: 0.08344831317663193\n",
      "36 train 27 336\n",
      "Phase: train. Epoch: 36. Loss: 0.08494281768798828\n",
      "36 train 28 348\n",
      "Phase: train. Epoch: 36. Loss: 0.093573197722435\n",
      "36 train 29 360\n",
      "Phase: train. Epoch: 36. Loss: 0.09376786649227142\n",
      "36 train 30 372\n",
      "Phase: train. Epoch: 36. Loss: 0.09708210080862045\n",
      "36 train 31 384\n",
      "Phase: train. Epoch: 36. Loss: 0.06297546625137329\n",
      "36 train 32 396\n",
      "Phase: train. Epoch: 36. Loss: 0.09193520992994308\n",
      "36 train 33 408\n",
      "Phase: train. Epoch: 36. Loss: 0.08426398038864136\n",
      "36 train 34 420\n",
      "Phase: train. Epoch: 36. Loss: 0.07794671505689621\n",
      "36 train 35 432\n",
      "Phase: train. Epoch: 36. Loss: 0.09267857670783997\n",
      "36 train 36 444\n",
      "Phase: train. Epoch: 36. Loss: 0.1229216456413269\n",
      "36 train 37 456\n",
      "Phase: train. Epoch: 36. Loss: 0.10834094136953354\n",
      "36 train 38 468\n",
      "Phase: train. Epoch: 36. Loss: 0.08556191623210907\n",
      "36 train 39 480\n",
      "Phase: train. Epoch: 36. Loss: 0.12492714822292328\n",
      "36 train 40 492\n",
      "Phase: train. Epoch: 36. Loss: 0.1033959686756134\n",
      "36 train 41 504\n",
      "Phase: train. Epoch: 36. Loss: 0.07562645524740219\n",
      "36 train 42 516\n",
      "Phase: train. Epoch: 36. Loss: 0.06768994778394699\n",
      "36 train 43 528\n",
      "Phase: train. Epoch: 36. Loss: 0.067367322742939\n",
      "36 train 44 540\n",
      "Phase: train. Epoch: 36. Loss: 0.07397866249084473\n",
      "36 train 45 552\n",
      "Phase: train. Epoch: 36. Loss: 0.07965417206287384\n",
      "36 train 46 564\n",
      "Phase: train. Epoch: 36. Loss: 0.07091648876667023\n",
      "36 train 47 576\n",
      "Phase: train. Epoch: 36. Loss: 0.11201529204845428\n",
      "36 train 48 588\n",
      "Phase: train. Epoch: 36. Loss: 0.07227610051631927\n",
      "36 train 49 600\n",
      "Phase: train. Epoch: 36. Loss: 0.07909491658210754\n",
      "36 train 50 612\n",
      "Phase: train. Epoch: 36. Loss: 0.07902594655752182\n",
      "36 train 51 624\n",
      "Phase: train. Epoch: 36. Loss: 0.08535341918468475\n",
      "36 train 52 636\n",
      "Phase: train. Epoch: 36. Loss: 0.11263980716466904\n",
      "36 train 53 648\n",
      "Phase: train. Epoch: 36. Loss: 0.08191239088773727\n",
      "36 train 54 660\n",
      "Phase: train. Epoch: 36. Loss: 0.08249139040708542\n",
      "36 train 55 672\n",
      "Phase: train. Epoch: 36. Loss: 0.06904608011245728\n",
      "36 train 56 684\n",
      "Phase: train. Epoch: 36. Loss: 0.07368728518486023\n",
      "36 train 57 696\n",
      "Phase: train. Epoch: 36. Loss: 0.07338649034500122\n",
      "36 train 58 708\n",
      "Phase: train. Epoch: 36. Loss: 0.0738837942481041\n",
      "36 train 59 720\n",
      "Phase: train. Epoch: 36. Loss: 0.09375321865081787\n",
      "36 train 60 732\n",
      "Phase: train. Epoch: 36. Loss: 0.07104679197072983\n",
      "36 train 61 744\n",
      "Phase: train. Epoch: 36. Loss: 0.10367117822170258\n",
      "36 train 62 751\n",
      "Phase: train. Epoch: 36. Loss: 0.07304711639881134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 val 0 763\n",
      "Phase: val. Epoch: 36. Loss: 0.07334103435277939\n",
      "36 val 1 775\n",
      "Phase: val. Epoch: 36. Loss: 0.09675078839063644\n",
      "36 val 2 787\n",
      "Phase: val. Epoch: 36. Loss: 0.09424885362386703\n",
      "36 val 3 799\n",
      "Phase: val. Epoch: 36. Loss: 0.10605046153068542\n",
      "36 val 4 811\n",
      "Phase: val. Epoch: 36. Loss: 0.08219623565673828\n",
      "36 val 5 823\n",
      "Phase: val. Epoch: 36. Loss: 0.06540995091199875\n",
      "36 val 6 835\n",
      "Phase: val. Epoch: 36. Loss: 0.10757438838481903\n",
      "36 val 7 847\n",
      "Phase: val. Epoch: 36. Loss: 0.08351312577724457\n",
      "36 val 8 859\n",
      "Phase: val. Epoch: 36. Loss: 0.07612871378660202\n",
      "36 val 9 871\n",
      "Phase: val. Epoch: 36. Loss: 0.06946028769016266\n",
      "36 val 10 883\n",
      "Phase: val. Epoch: 36. Loss: 0.08747877180576324\n",
      "36 val 11 884\n",
      "Phase: val. Epoch: 36. Loss: 0.05480358377099037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 train 0 12\n",
      "Phase: train. Epoch: 37. Loss: 0.08762644976377487\n",
      "37 train 1 24\n",
      "Phase: train. Epoch: 37. Loss: 0.06940793246030807\n",
      "37 train 2 36\n",
      "Phase: train. Epoch: 37. Loss: 0.1059059128165245\n",
      "37 train 3 48\n",
      "Phase: train. Epoch: 37. Loss: 0.09312039613723755\n",
      "37 train 4 60\n",
      "Phase: train. Epoch: 37. Loss: 0.09691547602415085\n",
      "37 train 5 72\n",
      "Phase: train. Epoch: 37. Loss: 0.07804659008979797\n",
      "37 train 6 84\n",
      "Phase: train. Epoch: 37. Loss: 0.10507798194885254\n",
      "37 train 7 96\n",
      "Phase: train. Epoch: 37. Loss: 0.07242544740438461\n",
      "37 train 8 108\n",
      "Phase: train. Epoch: 37. Loss: 0.08971041440963745\n",
      "37 train 9 120\n",
      "Phase: train. Epoch: 37. Loss: 0.08850976079702377\n",
      "37 train 10 132\n",
      "Phase: train. Epoch: 37. Loss: 0.07246781885623932\n",
      "37 train 11 144\n",
      "Phase: train. Epoch: 37. Loss: 0.10681422799825668\n",
      "37 train 12 156\n",
      "Phase: train. Epoch: 37. Loss: 0.08724389970302582\n",
      "37 train 13 168\n",
      "Phase: train. Epoch: 37. Loss: 0.10640645027160645\n",
      "37 train 14 180\n",
      "Phase: train. Epoch: 37. Loss: 0.08431264013051987\n",
      "37 train 15 192\n",
      "Phase: train. Epoch: 37. Loss: 0.07373524457216263\n",
      "37 train 16 204\n",
      "Phase: train. Epoch: 37. Loss: 0.07328665256500244\n",
      "37 train 17 216\n",
      "Phase: train. Epoch: 37. Loss: 0.10496100038290024\n",
      "37 train 18 228\n",
      "Phase: train. Epoch: 37. Loss: 0.06946419924497604\n",
      "37 train 19 240\n",
      "Phase: train. Epoch: 37. Loss: 0.0930548906326294\n",
      "37 train 20 252\n",
      "Phase: train. Epoch: 37. Loss: 0.07419511675834656\n",
      "37 train 21 264\n",
      "Phase: train. Epoch: 37. Loss: 0.07145480811595917\n",
      "37 train 22 276\n",
      "Phase: train. Epoch: 37. Loss: 0.09561890363693237\n",
      "37 train 23 288\n",
      "Phase: train. Epoch: 37. Loss: 0.10227416455745697\n",
      "37 train 24 300\n",
      "Phase: train. Epoch: 37. Loss: 0.08764328807592392\n",
      "37 train 25 312\n",
      "Phase: train. Epoch: 37. Loss: 0.08289352059364319\n",
      "37 train 26 324\n",
      "Phase: train. Epoch: 37. Loss: 0.07852139323949814\n",
      "37 train 27 336\n",
      "Phase: train. Epoch: 37. Loss: 0.07672291249036789\n",
      "37 train 28 348\n",
      "Phase: train. Epoch: 37. Loss: 0.09349119663238525\n",
      "37 train 29 360\n",
      "Phase: train. Epoch: 37. Loss: 0.08555106818675995\n",
      "37 train 30 372\n",
      "Phase: train. Epoch: 37. Loss: 0.08730366080999374\n",
      "37 train 31 384\n",
      "Phase: train. Epoch: 37. Loss: 0.0963841900229454\n",
      "37 train 32 396\n",
      "Phase: train. Epoch: 37. Loss: 0.07204784452915192\n",
      "37 train 33 408\n",
      "Phase: train. Epoch: 37. Loss: 0.07506625354290009\n",
      "37 train 34 420\n",
      "Phase: train. Epoch: 37. Loss: 0.08446081727743149\n",
      "37 train 35 432\n",
      "Phase: train. Epoch: 37. Loss: 0.08017630875110626\n",
      "37 train 36 444\n",
      "Phase: train. Epoch: 37. Loss: 0.06626438349485397\n",
      "37 train 37 456\n",
      "Phase: train. Epoch: 37. Loss: 0.12053098529577255\n",
      "37 train 38 468\n",
      "Phase: train. Epoch: 37. Loss: 0.094174824655056\n",
      "37 train 39 480\n",
      "Phase: train. Epoch: 37. Loss: 0.08345713466405869\n",
      "37 train 40 492\n",
      "Phase: train. Epoch: 37. Loss: 0.11978042125701904\n",
      "37 train 41 504\n",
      "Phase: train. Epoch: 37. Loss: 0.11207255721092224\n",
      "37 train 42 516\n",
      "Phase: train. Epoch: 37. Loss: 0.08211605250835419\n",
      "37 train 43 528\n",
      "Phase: train. Epoch: 37. Loss: 0.0896168202161789\n",
      "37 train 44 540\n",
      "Phase: train. Epoch: 37. Loss: 0.07522749900817871\n",
      "37 train 45 552\n",
      "Phase: train. Epoch: 37. Loss: 0.08600011467933655\n",
      "37 train 46 564\n",
      "Phase: train. Epoch: 37. Loss: 0.07024769484996796\n",
      "37 train 47 576\n",
      "Phase: train. Epoch: 37. Loss: 0.08745627105236053\n",
      "37 train 48 588\n",
      "Phase: train. Epoch: 37. Loss: 0.07019118964672089\n",
      "37 train 49 600\n",
      "Phase: train. Epoch: 37. Loss: 0.12813761830329895\n",
      "37 train 50 612\n",
      "Phase: train. Epoch: 37. Loss: 0.09630920737981796\n",
      "37 train 51 624\n",
      "Phase: train. Epoch: 37. Loss: 0.11046625673770905\n",
      "37 train 52 636\n",
      "Phase: train. Epoch: 37. Loss: 0.08797533810138702\n",
      "37 train 53 648\n",
      "Phase: train. Epoch: 37. Loss: 0.09687076508998871\n",
      "37 train 54 660\n",
      "Phase: train. Epoch: 37. Loss: 0.08112163096666336\n",
      "37 train 55 672\n",
      "Phase: train. Epoch: 37. Loss: 0.10610418766736984\n",
      "37 train 56 684\n",
      "Phase: train. Epoch: 37. Loss: 0.10009400546550751\n",
      "37 train 57 696\n",
      "Phase: train. Epoch: 37. Loss: 0.08068081736564636\n",
      "37 train 58 708\n",
      "Phase: train. Epoch: 37. Loss: 0.07407712936401367\n",
      "37 train 59 720\n",
      "Phase: train. Epoch: 37. Loss: 0.10733743757009506\n",
      "37 train 60 732\n",
      "Phase: train. Epoch: 37. Loss: 0.08974067866802216\n",
      "37 train 61 744\n",
      "Phase: train. Epoch: 37. Loss: 0.06611182540655136\n",
      "37 train 62 751\n",
      "Phase: train. Epoch: 37. Loss: 0.12252024561166763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 val 0 763\n",
      "Phase: val. Epoch: 37. Loss: 0.08949076384305954\n",
      "37 val 1 775\n",
      "Phase: val. Epoch: 37. Loss: 0.10032033920288086\n",
      "37 val 2 787\n",
      "Phase: val. Epoch: 37. Loss: 0.08164803683757782\n",
      "37 val 3 799\n",
      "Phase: val. Epoch: 37. Loss: 0.08716349303722382\n",
      "37 val 4 811\n",
      "Phase: val. Epoch: 37. Loss: 0.08501225709915161\n",
      "37 val 5 823\n",
      "Phase: val. Epoch: 37. Loss: 0.05451510474085808\n",
      "37 val 6 835\n",
      "Phase: val. Epoch: 37. Loss: 0.08974066376686096\n",
      "37 val 7 847\n",
      "Phase: val. Epoch: 37. Loss: 0.09133091568946838\n",
      "37 val 8 859\n",
      "Phase: val. Epoch: 37. Loss: 0.0813831239938736\n",
      "37 val 9 871\n",
      "Phase: val. Epoch: 37. Loss: 0.0918005108833313\n",
      "37 val 10 883\n",
      "Phase: val. Epoch: 37. Loss: 0.07966478168964386\n",
      "37 val 11 884\n",
      "Phase: val. Epoch: 37. Loss: 0.08556165546178818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 train 0 12\n",
      "Phase: train. Epoch: 38. Loss: 0.07286529988050461\n",
      "38 train 1 24\n",
      "Phase: train. Epoch: 38. Loss: 0.08333446830511093\n",
      "38 train 2 36\n",
      "Phase: train. Epoch: 38. Loss: 0.06980974972248077\n",
      "38 train 3 48\n",
      "Phase: train. Epoch: 38. Loss: 0.09573139250278473\n",
      "38 train 4 60\n",
      "Phase: train. Epoch: 38. Loss: 0.07250461727380753\n",
      "38 train 5 72\n",
      "Phase: train. Epoch: 38. Loss: 0.09352026879787445\n",
      "38 train 6 84\n",
      "Phase: train. Epoch: 38. Loss: 0.09188979864120483\n",
      "38 train 7 96\n",
      "Phase: train. Epoch: 38. Loss: 0.08474837243556976\n",
      "38 train 8 108\n",
      "Phase: train. Epoch: 38. Loss: 0.08306971937417984\n",
      "38 train 9 120\n",
      "Phase: train. Epoch: 38. Loss: 0.08831974864006042\n",
      "38 train 10 132\n",
      "Phase: train. Epoch: 38. Loss: 0.09114988893270493\n",
      "38 train 11 144\n",
      "Phase: train. Epoch: 38. Loss: 0.09634429216384888\n",
      "38 train 12 156\n",
      "Phase: train. Epoch: 38. Loss: 0.08815249055624008\n",
      "38 train 13 168\n",
      "Phase: train. Epoch: 38. Loss: 0.08289994299411774\n",
      "38 train 14 180\n",
      "Phase: train. Epoch: 38. Loss: 0.07237251102924347\n",
      "38 train 15 192\n",
      "Phase: train. Epoch: 38. Loss: 0.08498084545135498\n",
      "38 train 16 204\n",
      "Phase: train. Epoch: 38. Loss: 0.10321813821792603\n",
      "38 train 17 216\n",
      "Phase: train. Epoch: 38. Loss: 0.08737941831350327\n",
      "38 train 18 228\n",
      "Phase: train. Epoch: 38. Loss: 0.06310681998729706\n",
      "38 train 19 240\n",
      "Phase: train. Epoch: 38. Loss: 0.08046764135360718\n",
      "38 train 20 252\n",
      "Phase: train. Epoch: 38. Loss: 0.08416130393743515\n",
      "38 train 21 264\n",
      "Phase: train. Epoch: 38. Loss: 0.12322278320789337\n",
      "38 train 22 276\n",
      "Phase: train. Epoch: 38. Loss: 0.11573782563209534\n",
      "38 train 23 288\n",
      "Phase: train. Epoch: 38. Loss: 0.07717825472354889\n",
      "38 train 24 300\n",
      "Phase: train. Epoch: 38. Loss: 0.08570315688848495\n",
      "38 train 25 312\n",
      "Phase: train. Epoch: 38. Loss: 0.081868015229702\n",
      "38 train 26 324\n",
      "Phase: train. Epoch: 38. Loss: 0.07280154526233673\n",
      "38 train 27 336\n",
      "Phase: train. Epoch: 38. Loss: 0.0833781287074089\n",
      "38 train 28 348\n",
      "Phase: train. Epoch: 38. Loss: 0.10677235573530197\n",
      "38 train 29 360\n",
      "Phase: train. Epoch: 38. Loss: 0.07114878296852112\n",
      "38 train 30 372\n",
      "Phase: train. Epoch: 38. Loss: 0.0929238423705101\n",
      "38 train 31 384\n",
      "Phase: train. Epoch: 38. Loss: 0.09036066383123398\n",
      "38 train 32 396\n",
      "Phase: train. Epoch: 38. Loss: 0.08354952186346054\n",
      "38 train 33 408\n",
      "Phase: train. Epoch: 38. Loss: 0.08867740631103516\n",
      "38 train 34 420\n",
      "Phase: train. Epoch: 38. Loss: 0.08285534381866455\n",
      "38 train 35 432\n",
      "Phase: train. Epoch: 38. Loss: 0.07742486894130707\n",
      "38 train 36 444\n",
      "Phase: train. Epoch: 38. Loss: 0.06613199412822723\n",
      "38 train 37 456\n",
      "Phase: train. Epoch: 38. Loss: 0.06993208825588226\n",
      "38 train 38 468\n",
      "Phase: train. Epoch: 38. Loss: 0.0980687290430069\n",
      "38 train 39 480\n",
      "Phase: train. Epoch: 38. Loss: 0.09122737497091293\n",
      "38 train 40 492\n",
      "Phase: train. Epoch: 38. Loss: 0.07539749145507812\n",
      "38 train 41 504\n",
      "Phase: train. Epoch: 38. Loss: 0.0764538049697876\n",
      "38 train 42 516\n",
      "Phase: train. Epoch: 38. Loss: 0.11165015399456024\n",
      "38 train 43 528\n",
      "Phase: train. Epoch: 38. Loss: 0.1053796261548996\n",
      "38 train 44 540\n",
      "Phase: train. Epoch: 38. Loss: 0.10588452219963074\n",
      "38 train 45 552\n",
      "Phase: train. Epoch: 38. Loss: 0.10705768316984177\n",
      "38 train 46 564\n",
      "Phase: train. Epoch: 38. Loss: 0.10095101594924927\n",
      "38 train 47 576\n",
      "Phase: train. Epoch: 38. Loss: 0.1071648895740509\n",
      "38 train 48 588\n",
      "Phase: train. Epoch: 38. Loss: 0.06734297424554825\n",
      "38 train 49 600\n",
      "Phase: train. Epoch: 38. Loss: 0.0947074145078659\n",
      "38 train 50 612\n",
      "Phase: train. Epoch: 38. Loss: 0.09150880575180054\n",
      "38 train 51 624\n",
      "Phase: train. Epoch: 38. Loss: 0.10805949568748474\n",
      "38 train 52 636\n",
      "Phase: train. Epoch: 38. Loss: 0.10829368233680725\n",
      "38 train 53 648\n",
      "Phase: train. Epoch: 38. Loss: 0.08946409076452255\n",
      "38 train 54 660\n",
      "Phase: train. Epoch: 38. Loss: 0.07464015483856201\n",
      "38 train 55 672\n",
      "Phase: train. Epoch: 38. Loss: 0.0805310383439064\n",
      "38 train 56 684\n",
      "Phase: train. Epoch: 38. Loss: 0.06897769868373871\n",
      "38 train 57 696\n",
      "Phase: train. Epoch: 38. Loss: 0.074564129114151\n",
      "38 train 58 708\n",
      "Phase: train. Epoch: 38. Loss: 0.1434575766324997\n",
      "38 train 59 720\n",
      "Phase: train. Epoch: 38. Loss: 0.1233503520488739\n",
      "38 train 60 732\n",
      "Phase: train. Epoch: 38. Loss: 0.09008024632930756\n",
      "38 train 61 744\n",
      "Phase: train. Epoch: 38. Loss: 0.09081649035215378\n",
      "38 train 62 751\n",
      "Phase: train. Epoch: 38. Loss: 0.0771450325846672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 val 0 763\n",
      "Phase: val. Epoch: 38. Loss: 0.08960005640983582\n",
      "38 val 1 775\n",
      "Phase: val. Epoch: 38. Loss: 0.09398394078016281\n",
      "38 val 2 787\n",
      "Phase: val. Epoch: 38. Loss: 0.06741397082805634\n",
      "38 val 3 799\n",
      "Phase: val. Epoch: 38. Loss: 0.07217701524496078\n",
      "38 val 4 811\n",
      "Phase: val. Epoch: 38. Loss: 0.07127105444669724\n",
      "38 val 5 823\n",
      "Phase: val. Epoch: 38. Loss: 0.07453171163797379\n",
      "38 val 6 835\n",
      "Phase: val. Epoch: 38. Loss: 0.08797544240951538\n",
      "38 val 7 847\n",
      "Phase: val. Epoch: 38. Loss: 0.10252852737903595\n",
      "38 val 8 859\n",
      "Phase: val. Epoch: 38. Loss: 0.09866897761821747\n",
      "38 val 9 871\n",
      "Phase: val. Epoch: 38. Loss: 0.06941857933998108\n",
      "38 val 10 883\n",
      "Phase: val. Epoch: 38. Loss: 0.07663506269454956\n",
      "38 val 11 884\n",
      "Phase: val. Epoch: 38. Loss: 0.03012867458164692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 train 0 12\n",
      "Phase: train. Epoch: 39. Loss: 0.07567016780376434\n",
      "39 train 1 24\n",
      "Phase: train. Epoch: 39. Loss: 0.0887359231710434\n",
      "39 train 2 36\n",
      "Phase: train. Epoch: 39. Loss: 0.09373539686203003\n",
      "39 train 3 48\n",
      "Phase: train. Epoch: 39. Loss: 0.09835746139287949\n",
      "39 train 4 60\n",
      "Phase: train. Epoch: 39. Loss: 0.08352796733379364\n",
      "39 train 5 72\n",
      "Phase: train. Epoch: 39. Loss: 0.08220910280942917\n",
      "39 train 6 84\n",
      "Phase: train. Epoch: 39. Loss: 0.09070213884115219\n",
      "39 train 7 96\n",
      "Phase: train. Epoch: 39. Loss: 0.08967065811157227\n",
      "39 train 8 108\n",
      "Phase: train. Epoch: 39. Loss: 0.08447623252868652\n",
      "39 train 9 120\n",
      "Phase: train. Epoch: 39. Loss: 0.09168438613414764\n",
      "39 train 10 132\n",
      "Phase: train. Epoch: 39. Loss: 0.09148576855659485\n",
      "39 train 11 144\n",
      "Phase: train. Epoch: 39. Loss: 0.10316579043865204\n",
      "39 train 12 156\n",
      "Phase: train. Epoch: 39. Loss: 0.059819936752319336\n",
      "39 train 13 168\n",
      "Phase: train. Epoch: 39. Loss: 0.10035429894924164\n",
      "39 train 14 180\n",
      "Phase: train. Epoch: 39. Loss: 0.08559766411781311\n",
      "39 train 15 192\n",
      "Phase: train. Epoch: 39. Loss: 0.0659015104174614\n",
      "39 train 16 204\n",
      "Phase: train. Epoch: 39. Loss: 0.09636717289686203\n",
      "39 train 17 216\n",
      "Phase: train. Epoch: 39. Loss: 0.06751427054405212\n",
      "39 train 18 228\n",
      "Phase: train. Epoch: 39. Loss: 0.0787704735994339\n",
      "39 train 19 240\n",
      "Phase: train. Epoch: 39. Loss: 0.09204541146755219\n",
      "39 train 20 252\n",
      "Phase: train. Epoch: 39. Loss: 0.10016985982656479\n",
      "39 train 21 264\n",
      "Phase: train. Epoch: 39. Loss: 0.07265328615903854\n",
      "39 train 22 276\n",
      "Phase: train. Epoch: 39. Loss: 0.0902581512928009\n",
      "39 train 23 288\n",
      "Phase: train. Epoch: 39. Loss: 0.09339678287506104\n",
      "39 train 24 300\n",
      "Phase: train. Epoch: 39. Loss: 0.0977795347571373\n",
      "39 train 25 312\n",
      "Phase: train. Epoch: 39. Loss: 0.08448947966098785\n",
      "39 train 26 324\n",
      "Phase: train. Epoch: 39. Loss: 0.09823445975780487\n",
      "39 train 27 336\n",
      "Phase: train. Epoch: 39. Loss: 0.07757456600666046\n",
      "39 train 28 348\n",
      "Phase: train. Epoch: 39. Loss: 0.08211502432823181\n",
      "39 train 29 360\n",
      "Phase: train. Epoch: 39. Loss: 0.07805558294057846\n",
      "39 train 30 372\n",
      "Phase: train. Epoch: 39. Loss: 0.10177747905254364\n",
      "39 train 31 384\n",
      "Phase: train. Epoch: 39. Loss: 0.08892768621444702\n",
      "39 train 32 396\n",
      "Phase: train. Epoch: 39. Loss: 0.0624377503991127\n",
      "39 train 33 408\n",
      "Phase: train. Epoch: 39. Loss: 0.07555535435676575\n",
      "39 train 34 420\n",
      "Phase: train. Epoch: 39. Loss: 0.10307703912258148\n",
      "39 train 35 432\n",
      "Phase: train. Epoch: 39. Loss: 0.0912541002035141\n",
      "39 train 36 444\n",
      "Phase: train. Epoch: 39. Loss: 0.08416520059108734\n",
      "39 train 37 456\n",
      "Phase: train. Epoch: 39. Loss: 0.09188725799322128\n",
      "39 train 38 468\n",
      "Phase: train. Epoch: 39. Loss: 0.0765453577041626\n",
      "39 train 39 480\n",
      "Phase: train. Epoch: 39. Loss: 0.07085472345352173\n",
      "39 train 40 492\n",
      "Phase: train. Epoch: 39. Loss: 0.08559352159500122\n",
      "39 train 41 504\n",
      "Phase: train. Epoch: 39. Loss: 0.09458675235509872\n",
      "39 train 42 516\n",
      "Phase: train. Epoch: 39. Loss: 0.10480435937643051\n",
      "39 train 43 528\n",
      "Phase: train. Epoch: 39. Loss: 0.07769417762756348\n",
      "39 train 44 540\n",
      "Phase: train. Epoch: 39. Loss: 0.10011857748031616\n",
      "39 train 45 552\n",
      "Phase: train. Epoch: 39. Loss: 0.1049952507019043\n",
      "39 train 46 564\n",
      "Phase: train. Epoch: 39. Loss: 0.08164152503013611\n",
      "39 train 47 576\n",
      "Phase: train. Epoch: 39. Loss: 0.07771281898021698\n",
      "39 train 48 588\n",
      "Phase: train. Epoch: 39. Loss: 0.06862559169530869\n",
      "39 train 49 600\n",
      "Phase: train. Epoch: 39. Loss: 0.07879562675952911\n",
      "39 train 50 612\n",
      "Phase: train. Epoch: 39. Loss: 0.08330453187227249\n",
      "39 train 51 624\n",
      "Phase: train. Epoch: 39. Loss: 0.1274133175611496\n",
      "39 train 52 636\n",
      "Phase: train. Epoch: 39. Loss: 0.09213823080062866\n",
      "39 train 53 648\n",
      "Phase: train. Epoch: 39. Loss: 0.07301383465528488\n",
      "39 train 54 660\n",
      "Phase: train. Epoch: 39. Loss: 0.07821527123451233\n",
      "39 train 55 672\n",
      "Phase: train. Epoch: 39. Loss: 0.10010562837123871\n",
      "39 train 56 684\n",
      "Phase: train. Epoch: 39. Loss: 0.08070650696754456\n",
      "39 train 57 696\n",
      "Phase: train. Epoch: 39. Loss: 0.1301405131816864\n",
      "39 train 58 708\n",
      "Phase: train. Epoch: 39. Loss: 0.0861421450972557\n",
      "39 train 59 720\n",
      "Phase: train. Epoch: 39. Loss: 0.09289169311523438\n",
      "39 train 60 732\n",
      "Phase: train. Epoch: 39. Loss: 0.1325133740901947\n",
      "39 train 61 744\n",
      "Phase: train. Epoch: 39. Loss: 0.07666946202516556\n",
      "39 train 62 751\n",
      "Phase: train. Epoch: 39. Loss: 0.07871730625629425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 val 0 763\n",
      "Phase: val. Epoch: 39. Loss: 0.059144701808691025\n",
      "39 val 1 775\n",
      "Phase: val. Epoch: 39. Loss: 0.09939193725585938\n",
      "39 val 2 787\n",
      "Phase: val. Epoch: 39. Loss: 0.08820930123329163\n",
      "39 val 3 799\n",
      "Phase: val. Epoch: 39. Loss: 0.0752505362033844\n",
      "39 val 4 811\n",
      "Phase: val. Epoch: 39. Loss: 0.09851343929767609\n",
      "39 val 5 823\n",
      "Phase: val. Epoch: 39. Loss: 0.09876394271850586\n",
      "39 val 6 835\n",
      "Phase: val. Epoch: 39. Loss: 0.10981661081314087\n",
      "39 val 7 847\n",
      "Phase: val. Epoch: 39. Loss: 0.09608899801969528\n",
      "39 val 8 859\n",
      "Phase: val. Epoch: 39. Loss: 0.11241179704666138\n",
      "39 val 9 871\n",
      "Phase: val. Epoch: 39. Loss: 0.10872560739517212\n",
      "39 val 10 883\n",
      "Phase: val. Epoch: 39. Loss: 0.06721881031990051\n",
      "39 val 11 884\n",
      "Phase: val. Epoch: 39. Loss: 0.02922159805893898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 train 0 12\n",
      "Phase: train. Epoch: 40. Loss: 0.06534042954444885\n",
      "40 train 1 24\n",
      "Phase: train. Epoch: 40. Loss: 0.07643869519233704\n",
      "40 train 2 36\n",
      "Phase: train. Epoch: 40. Loss: 0.10674672573804855\n",
      "40 train 3 48\n",
      "Phase: train. Epoch: 40. Loss: 0.08958041667938232\n",
      "40 train 4 60\n",
      "Phase: train. Epoch: 40. Loss: 0.08361925929784775\n",
      "40 train 5 72\n",
      "Phase: train. Epoch: 40. Loss: 0.07496992498636246\n",
      "40 train 6 84\n",
      "Phase: train. Epoch: 40. Loss: 0.07416652143001556\n",
      "40 train 7 96\n",
      "Phase: train. Epoch: 40. Loss: 0.08405175060033798\n",
      "40 train 8 108\n",
      "Phase: train. Epoch: 40. Loss: 0.0815381109714508\n",
      "40 train 9 120\n",
      "Phase: train. Epoch: 40. Loss: 0.09304773807525635\n",
      "40 train 10 132\n",
      "Phase: train. Epoch: 40. Loss: 0.0931040495634079\n",
      "40 train 11 144\n",
      "Phase: train. Epoch: 40. Loss: 0.09073440730571747\n",
      "40 train 12 156\n",
      "Phase: train. Epoch: 40. Loss: 0.10197528451681137\n",
      "40 train 13 168\n",
      "Phase: train. Epoch: 40. Loss: 0.07708636671304703\n",
      "40 train 14 180\n",
      "Phase: train. Epoch: 40. Loss: 0.08295335620641708\n",
      "40 train 15 192\n",
      "Phase: train. Epoch: 40. Loss: 0.07986749708652496\n",
      "40 train 16 204\n",
      "Phase: train. Epoch: 40. Loss: 0.08320268988609314\n",
      "40 train 17 216\n",
      "Phase: train. Epoch: 40. Loss: 0.07795190811157227\n",
      "40 train 18 228\n",
      "Phase: train. Epoch: 40. Loss: 0.06766178458929062\n",
      "40 train 19 240\n",
      "Phase: train. Epoch: 40. Loss: 0.06302633881568909\n",
      "40 train 20 252\n",
      "Phase: train. Epoch: 40. Loss: 0.10477219521999359\n",
      "40 train 21 264\n",
      "Phase: train. Epoch: 40. Loss: 0.08102920651435852\n",
      "40 train 22 276\n",
      "Phase: train. Epoch: 40. Loss: 0.0846884697675705\n",
      "40 train 23 288\n",
      "Phase: train. Epoch: 40. Loss: 0.09509363770484924\n",
      "40 train 24 300\n",
      "Phase: train. Epoch: 40. Loss: 0.07273693382740021\n",
      "40 train 25 312\n",
      "Phase: train. Epoch: 40. Loss: 0.08535438776016235\n",
      "40 train 26 324\n",
      "Phase: train. Epoch: 40. Loss: 0.08257636427879333\n",
      "40 train 27 336\n",
      "Phase: train. Epoch: 40. Loss: 0.08055349439382553\n",
      "40 train 28 348\n",
      "Phase: train. Epoch: 40. Loss: 0.08600562810897827\n",
      "40 train 29 360\n",
      "Phase: train. Epoch: 40. Loss: 0.08607469499111176\n",
      "40 train 30 372\n",
      "Phase: train. Epoch: 40. Loss: 0.10432535409927368\n",
      "40 train 31 384\n",
      "Phase: train. Epoch: 40. Loss: 0.12570005655288696\n",
      "40 train 32 396\n",
      "Phase: train. Epoch: 40. Loss: 0.06601133942604065\n",
      "40 train 33 408\n",
      "Phase: train. Epoch: 40. Loss: 0.07589272409677505\n",
      "40 train 34 420\n",
      "Phase: train. Epoch: 40. Loss: 0.06710405647754669\n",
      "40 train 35 432\n",
      "Phase: train. Epoch: 40. Loss: 0.0873815044760704\n",
      "40 train 36 444\n",
      "Phase: train. Epoch: 40. Loss: 0.10524892807006836\n",
      "40 train 37 456\n",
      "Phase: train. Epoch: 40. Loss: 0.1096590906381607\n",
      "40 train 38 468\n",
      "Phase: train. Epoch: 40. Loss: 0.09675146639347076\n",
      "40 train 39 480\n",
      "Phase: train. Epoch: 40. Loss: 0.08012883365154266\n",
      "40 train 40 492\n",
      "Phase: train. Epoch: 40. Loss: 0.08175373822450638\n",
      "40 train 41 504\n",
      "Phase: train. Epoch: 40. Loss: 0.108170285820961\n",
      "40 train 42 516\n",
      "Phase: train. Epoch: 40. Loss: 0.08308474719524384\n",
      "40 train 43 528\n",
      "Phase: train. Epoch: 40. Loss: 0.08002278208732605\n",
      "40 train 44 540\n",
      "Phase: train. Epoch: 40. Loss: 0.1045343279838562\n",
      "40 train 45 552\n",
      "Phase: train. Epoch: 40. Loss: 0.08037599921226501\n",
      "40 train 46 564\n",
      "Phase: train. Epoch: 40. Loss: 0.09916790574789047\n",
      "40 train 47 576\n",
      "Phase: train. Epoch: 40. Loss: 0.08015221357345581\n",
      "40 train 48 588\n",
      "Phase: train. Epoch: 40. Loss: 0.09727909415960312\n",
      "40 train 49 600\n",
      "Phase: train. Epoch: 40. Loss: 0.07030899077653885\n",
      "40 train 50 612\n",
      "Phase: train. Epoch: 40. Loss: 0.09055693447589874\n",
      "40 train 51 624\n",
      "Phase: train. Epoch: 40. Loss: 0.10276544094085693\n",
      "40 train 52 636\n",
      "Phase: train. Epoch: 40. Loss: 0.08459232747554779\n",
      "40 train 53 648\n",
      "Phase: train. Epoch: 40. Loss: 0.09743880480527878\n",
      "40 train 54 660\n",
      "Phase: train. Epoch: 40. Loss: 0.08891614526510239\n",
      "40 train 55 672\n",
      "Phase: train. Epoch: 40. Loss: 0.09111928194761276\n",
      "40 train 56 684\n",
      "Phase: train. Epoch: 40. Loss: 0.0814482569694519\n",
      "40 train 57 696\n",
      "Phase: train. Epoch: 40. Loss: 0.08002378046512604\n",
      "40 train 58 708\n",
      "Phase: train. Epoch: 40. Loss: 0.09108579158782959\n",
      "40 train 59 720\n",
      "Phase: train. Epoch: 40. Loss: 0.09496641159057617\n",
      "40 train 60 732\n",
      "Phase: train. Epoch: 40. Loss: 0.1277334988117218\n",
      "40 train 61 744\n",
      "Phase: train. Epoch: 40. Loss: 0.13003501296043396\n",
      "40 train 62 751\n",
      "Phase: train. Epoch: 40. Loss: 0.0666666179895401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 val 0 763\n",
      "Phase: val. Epoch: 40. Loss: 0.0973503440618515\n",
      "40 val 1 775\n",
      "Phase: val. Epoch: 40. Loss: 0.08669142425060272\n",
      "40 val 2 787\n",
      "Phase: val. Epoch: 40. Loss: 0.10021066665649414\n",
      "40 val 3 799\n",
      "Phase: val. Epoch: 40. Loss: 0.11730249971151352\n",
      "40 val 4 811\n",
      "Phase: val. Epoch: 40. Loss: 0.06368421018123627\n",
      "40 val 5 823\n",
      "Phase: val. Epoch: 40. Loss: 0.057204585522413254\n",
      "40 val 6 835\n",
      "Phase: val. Epoch: 40. Loss: 0.05721273273229599\n",
      "40 val 7 847\n",
      "Phase: val. Epoch: 40. Loss: 0.08493654429912567\n",
      "40 val 8 859\n",
      "Phase: val. Epoch: 40. Loss: 0.0970039814710617\n",
      "40 val 9 871\n",
      "Phase: val. Epoch: 40. Loss: 0.07587384432554245\n",
      "40 val 10 883\n",
      "Phase: val. Epoch: 40. Loss: 0.07167384028434753\n",
      "40 val 11 884\n",
      "Phase: val. Epoch: 40. Loss: 0.18377354741096497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 train 0 12\n",
      "Phase: train. Epoch: 41. Loss: 0.09836926311254501\n",
      "41 train 1 24\n",
      "Phase: train. Epoch: 41. Loss: 0.09083389490842819\n",
      "41 train 2 36\n",
      "Phase: train. Epoch: 41. Loss: 0.07746577262878418\n",
      "41 train 3 48\n",
      "Phase: train. Epoch: 41. Loss: 0.10794264078140259\n",
      "41 train 4 60\n",
      "Phase: train. Epoch: 41. Loss: 0.09558682888746262\n",
      "41 train 5 72\n",
      "Phase: train. Epoch: 41. Loss: 0.11258047819137573\n",
      "41 train 6 84\n",
      "Phase: train. Epoch: 41. Loss: 0.0828050747513771\n",
      "41 train 7 96\n",
      "Phase: train. Epoch: 41. Loss: 0.09365479648113251\n",
      "41 train 8 108\n",
      "Phase: train. Epoch: 41. Loss: 0.08615082502365112\n",
      "41 train 9 120\n",
      "Phase: train. Epoch: 41. Loss: 0.08744629472494125\n",
      "41 train 10 132\n",
      "Phase: train. Epoch: 41. Loss: 0.0781332477927208\n",
      "41 train 11 144\n",
      "Phase: train. Epoch: 41. Loss: 0.07336254417896271\n",
      "41 train 12 156\n",
      "Phase: train. Epoch: 41. Loss: 0.08615051209926605\n",
      "41 train 13 168\n",
      "Phase: train. Epoch: 41. Loss: 0.07795563340187073\n",
      "41 train 14 180\n",
      "Phase: train. Epoch: 41. Loss: 0.08606714010238647\n",
      "41 train 15 192\n",
      "Phase: train. Epoch: 41. Loss: 0.10913752764463425\n",
      "41 train 16 204\n",
      "Phase: train. Epoch: 41. Loss: 0.11103802919387817\n",
      "41 train 17 216\n",
      "Phase: train. Epoch: 41. Loss: 0.10339376330375671\n",
      "41 train 18 228\n",
      "Phase: train. Epoch: 41. Loss: 0.076504185795784\n",
      "41 train 19 240\n",
      "Phase: train. Epoch: 41. Loss: 0.0994475707411766\n",
      "41 train 20 252\n",
      "Phase: train. Epoch: 41. Loss: 0.09826318919658661\n",
      "41 train 21 264\n",
      "Phase: train. Epoch: 41. Loss: 0.09026368707418442\n",
      "41 train 22 276\n",
      "Phase: train. Epoch: 41. Loss: 0.09584122896194458\n",
      "41 train 23 288\n",
      "Phase: train. Epoch: 41. Loss: 0.07797597348690033\n",
      "41 train 24 300\n",
      "Phase: train. Epoch: 41. Loss: 0.07305602729320526\n",
      "41 train 25 312\n",
      "Phase: train. Epoch: 41. Loss: 0.09206132590770721\n",
      "41 train 26 324\n",
      "Phase: train. Epoch: 41. Loss: 0.07088407129049301\n",
      "41 train 27 336\n",
      "Phase: train. Epoch: 41. Loss: 0.08672376722097397\n",
      "41 train 28 348\n",
      "Phase: train. Epoch: 41. Loss: 0.09827946126461029\n",
      "41 train 29 360\n",
      "Phase: train. Epoch: 41. Loss: 0.10113658756017685\n",
      "41 train 30 372\n",
      "Phase: train. Epoch: 41. Loss: 0.0823836475610733\n",
      "41 train 31 384\n",
      "Phase: train. Epoch: 41. Loss: 0.09199045598506927\n",
      "41 train 32 396\n",
      "Phase: train. Epoch: 41. Loss: 0.07946055382490158\n",
      "41 train 33 408\n",
      "Phase: train. Epoch: 41. Loss: 0.08109855651855469\n",
      "41 train 34 420\n",
      "Phase: train. Epoch: 41. Loss: 0.0713411271572113\n",
      "41 train 35 432\n",
      "Phase: train. Epoch: 41. Loss: 0.067680224776268\n",
      "41 train 36 444\n",
      "Phase: train. Epoch: 41. Loss: 0.08527972549200058\n",
      "41 train 37 456\n",
      "Phase: train. Epoch: 41. Loss: 0.07155158370733261\n",
      "41 train 38 468\n",
      "Phase: train. Epoch: 41. Loss: 0.09468719363212585\n",
      "41 train 39 480\n",
      "Phase: train. Epoch: 41. Loss: 0.08667893707752228\n",
      "41 train 40 492\n",
      "Phase: train. Epoch: 41. Loss: 0.08100655674934387\n",
      "41 train 41 504\n",
      "Phase: train. Epoch: 41. Loss: 0.08861969411373138\n",
      "41 train 42 516\n",
      "Phase: train. Epoch: 41. Loss: 0.09685580432415009\n",
      "41 train 43 528\n",
      "Phase: train. Epoch: 41. Loss: 0.10035115480422974\n",
      "41 train 44 540\n",
      "Phase: train. Epoch: 41. Loss: 0.07047893851995468\n",
      "41 train 45 552\n",
      "Phase: train. Epoch: 41. Loss: 0.07527852058410645\n",
      "41 train 46 564\n",
      "Phase: train. Epoch: 41. Loss: 0.0746583640575409\n",
      "41 train 47 576\n",
      "Phase: train. Epoch: 41. Loss: 0.10139203071594238\n",
      "41 train 48 588\n",
      "Phase: train. Epoch: 41. Loss: 0.07740002870559692\n",
      "41 train 49 600\n",
      "Phase: train. Epoch: 41. Loss: 0.09089748561382294\n",
      "41 train 50 612\n",
      "Phase: train. Epoch: 41. Loss: 0.08070182800292969\n",
      "41 train 51 624\n",
      "Phase: train. Epoch: 41. Loss: 0.10185303539037704\n",
      "41 train 52 636\n",
      "Phase: train. Epoch: 41. Loss: 0.07458283007144928\n",
      "41 train 53 648\n",
      "Phase: train. Epoch: 41. Loss: 0.08769812434911728\n",
      "41 train 54 660\n",
      "Phase: train. Epoch: 41. Loss: 0.06505045294761658\n",
      "41 train 55 672\n",
      "Phase: train. Epoch: 41. Loss: 0.11224444955587387\n",
      "41 train 56 684\n",
      "Phase: train. Epoch: 41. Loss: 0.0834532380104065\n",
      "41 train 57 696\n",
      "Phase: train. Epoch: 41. Loss: 0.07913339138031006\n",
      "41 train 58 708\n",
      "Phase: train. Epoch: 41. Loss: 0.0826408788561821\n",
      "41 train 59 720\n",
      "Phase: train. Epoch: 41. Loss: 0.0675223246216774\n",
      "41 train 60 732\n",
      "Phase: train. Epoch: 41. Loss: 0.0630953460931778\n",
      "41 train 61 744\n",
      "Phase: train. Epoch: 41. Loss: 0.0832497626543045\n",
      "41 train 62 751\n",
      "Phase: train. Epoch: 41. Loss: 0.11195285618305206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 val 0 763\n",
      "Phase: val. Epoch: 41. Loss: 0.11936789751052856\n",
      "41 val 1 775\n",
      "Phase: val. Epoch: 41. Loss: 0.05702952668070793\n",
      "41 val 2 787\n",
      "Phase: val. Epoch: 41. Loss: 0.07895763963460922\n",
      "41 val 3 799\n",
      "Phase: val. Epoch: 41. Loss: 0.05082614719867706\n",
      "41 val 4 811\n",
      "Phase: val. Epoch: 41. Loss: 0.1309947371482849\n",
      "41 val 5 823\n",
      "Phase: val. Epoch: 41. Loss: 0.09713403880596161\n",
      "41 val 6 835\n",
      "Phase: val. Epoch: 41. Loss: 0.08346142619848251\n",
      "41 val 7 847\n",
      "Phase: val. Epoch: 41. Loss: 0.1444157361984253\n",
      "41 val 8 859\n",
      "Phase: val. Epoch: 41. Loss: 0.0991477221250534\n",
      "41 val 9 871\n",
      "Phase: val. Epoch: 41. Loss: 0.0838264673948288\n",
      "41 val 10 883\n",
      "Phase: val. Epoch: 41. Loss: 0.058205973356962204\n",
      "41 val 11 884\n",
      "Phase: val. Epoch: 41. Loss: 0.0887390524148941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 train 0 12\n",
      "Phase: train. Epoch: 42. Loss: 0.07470816373825073\n",
      "42 train 1 24\n",
      "Phase: train. Epoch: 42. Loss: 0.10796795785427094\n",
      "42 train 2 36\n",
      "Phase: train. Epoch: 42. Loss: 0.08368508517742157\n",
      "42 train 3 48\n",
      "Phase: train. Epoch: 42. Loss: 0.0743718296289444\n",
      "42 train 4 60\n",
      "Phase: train. Epoch: 42. Loss: 0.11019477248191833\n",
      "42 train 5 72\n",
      "Phase: train. Epoch: 42. Loss: 0.0991690456867218\n",
      "42 train 6 84\n",
      "Phase: train. Epoch: 42. Loss: 0.08796036243438721\n",
      "42 train 7 96\n",
      "Phase: train. Epoch: 42. Loss: 0.07006049156188965\n",
      "42 train 8 108\n",
      "Phase: train. Epoch: 42. Loss: 0.09100332856178284\n",
      "42 train 9 120\n",
      "Phase: train. Epoch: 42. Loss: 0.0727953165769577\n",
      "42 train 10 132\n",
      "Phase: train. Epoch: 42. Loss: 0.07314863801002502\n",
      "42 train 11 144\n",
      "Phase: train. Epoch: 42. Loss: 0.1021827906370163\n",
      "42 train 12 156\n",
      "Phase: train. Epoch: 42. Loss: 0.12080543488264084\n",
      "42 train 13 168\n",
      "Phase: train. Epoch: 42. Loss: 0.07426512986421585\n",
      "42 train 14 180\n",
      "Phase: train. Epoch: 42. Loss: 0.101149782538414\n",
      "42 train 15 192\n",
      "Phase: train. Epoch: 42. Loss: 0.08565690368413925\n",
      "42 train 16 204\n",
      "Phase: train. Epoch: 42. Loss: 0.0793207585811615\n",
      "42 train 17 216\n",
      "Phase: train. Epoch: 42. Loss: 0.09552036225795746\n",
      "42 train 18 228\n",
      "Phase: train. Epoch: 42. Loss: 0.0877828449010849\n",
      "42 train 19 240\n",
      "Phase: train. Epoch: 42. Loss: 0.08534687757492065\n",
      "42 train 20 252\n",
      "Phase: train. Epoch: 42. Loss: 0.10211917012929916\n",
      "42 train 21 264\n",
      "Phase: train. Epoch: 42. Loss: 0.07644359767436981\n",
      "42 train 22 276\n",
      "Phase: train. Epoch: 42. Loss: 0.09892013669013977\n",
      "42 train 23 288\n",
      "Phase: train. Epoch: 42. Loss: 0.07523466646671295\n",
      "42 train 24 300\n",
      "Phase: train. Epoch: 42. Loss: 0.09465979039669037\n",
      "42 train 25 312\n",
      "Phase: train. Epoch: 42. Loss: 0.106029212474823\n",
      "42 train 26 324\n",
      "Phase: train. Epoch: 42. Loss: 0.09549363702535629\n",
      "42 train 27 336\n",
      "Phase: train. Epoch: 42. Loss: 0.07647353410720825\n",
      "42 train 28 348\n",
      "Phase: train. Epoch: 42. Loss: 0.09100791811943054\n",
      "42 train 29 360\n",
      "Phase: train. Epoch: 42. Loss: 0.09349563717842102\n",
      "42 train 30 372\n",
      "Phase: train. Epoch: 42. Loss: 0.12009225785732269\n",
      "42 train 31 384\n",
      "Phase: train. Epoch: 42. Loss: 0.08173634111881256\n",
      "42 train 32 396\n",
      "Phase: train. Epoch: 42. Loss: 0.07141533493995667\n",
      "42 train 33 408\n",
      "Phase: train. Epoch: 42. Loss: 0.0970844030380249\n",
      "42 train 34 420\n",
      "Phase: train. Epoch: 42. Loss: 0.06480725109577179\n",
      "42 train 35 432\n",
      "Phase: train. Epoch: 42. Loss: 0.08379220217466354\n",
      "42 train 36 444\n",
      "Phase: train. Epoch: 42. Loss: 0.09114496409893036\n",
      "42 train 37 456\n",
      "Phase: train. Epoch: 42. Loss: 0.09591361880302429\n",
      "42 train 38 468\n",
      "Phase: train. Epoch: 42. Loss: 0.08626260608434677\n",
      "42 train 39 480\n",
      "Phase: train. Epoch: 42. Loss: 0.12755617499351501\n",
      "42 train 40 492\n",
      "Phase: train. Epoch: 42. Loss: 0.0672510415315628\n",
      "42 train 41 504\n",
      "Phase: train. Epoch: 42. Loss: 0.08100929111242294\n",
      "42 train 42 516\n",
      "Phase: train. Epoch: 42. Loss: 0.06585383415222168\n",
      "42 train 43 528\n",
      "Phase: train. Epoch: 42. Loss: 0.0747177004814148\n",
      "42 train 44 540\n",
      "Phase: train. Epoch: 42. Loss: 0.08182087540626526\n",
      "42 train 45 552\n",
      "Phase: train. Epoch: 42. Loss: 0.1075294241309166\n",
      "42 train 46 564\n",
      "Phase: train. Epoch: 42. Loss: 0.06729164719581604\n",
      "42 train 47 576\n",
      "Phase: train. Epoch: 42. Loss: 0.07050851732492447\n",
      "42 train 48 588\n",
      "Phase: train. Epoch: 42. Loss: 0.07716809958219528\n",
      "42 train 49 600\n",
      "Phase: train. Epoch: 42. Loss: 0.08628799766302109\n",
      "42 train 50 612\n",
      "Phase: train. Epoch: 42. Loss: 0.09599046409130096\n",
      "42 train 51 624\n",
      "Phase: train. Epoch: 42. Loss: 0.0994947999715805\n",
      "42 train 52 636\n",
      "Phase: train. Epoch: 42. Loss: 0.07746350020170212\n",
      "42 train 53 648\n",
      "Phase: train. Epoch: 42. Loss: 0.08463315665721893\n",
      "42 train 54 660\n",
      "Phase: train. Epoch: 42. Loss: 0.11168279498815536\n",
      "42 train 55 672\n",
      "Phase: train. Epoch: 42. Loss: 0.08370071649551392\n",
      "42 train 56 684\n",
      "Phase: train. Epoch: 42. Loss: 0.09464091062545776\n",
      "42 train 57 696\n",
      "Phase: train. Epoch: 42. Loss: 0.0669008120894432\n",
      "42 train 58 708\n",
      "Phase: train. Epoch: 42. Loss: 0.09148089587688446\n",
      "42 train 59 720\n",
      "Phase: train. Epoch: 42. Loss: 0.07920517027378082\n",
      "42 train 60 732\n",
      "Phase: train. Epoch: 42. Loss: 0.07925549149513245\n",
      "42 train 61 744\n",
      "Phase: train. Epoch: 42. Loss: 0.09298093616962433\n",
      "42 train 62 751\n",
      "Phase: train. Epoch: 42. Loss: 0.08633354306221008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 val 0 763\n",
      "Phase: val. Epoch: 42. Loss: 0.08516966551542282\n",
      "42 val 1 775\n",
      "Phase: val. Epoch: 42. Loss: 0.07103385031223297\n",
      "42 val 2 787\n",
      "Phase: val. Epoch: 42. Loss: 0.08828701823949814\n",
      "42 val 3 799\n",
      "Phase: val. Epoch: 42. Loss: 0.11517839878797531\n",
      "42 val 4 811\n",
      "Phase: val. Epoch: 42. Loss: 0.06235940381884575\n",
      "42 val 5 823\n",
      "Phase: val. Epoch: 42. Loss: 0.08050437271595001\n",
      "42 val 6 835\n",
      "Phase: val. Epoch: 42. Loss: 0.06534358114004135\n",
      "42 val 7 847\n",
      "Phase: val. Epoch: 42. Loss: 0.07752415537834167\n",
      "42 val 8 859\n",
      "Phase: val. Epoch: 42. Loss: 0.0926513522863388\n",
      "42 val 9 871\n",
      "Phase: val. Epoch: 42. Loss: 0.10909973084926605\n",
      "42 val 10 883\n",
      "Phase: val. Epoch: 42. Loss: 0.09683671593666077\n",
      "42 val 11 884\n",
      "Phase: val. Epoch: 42. Loss: 0.07767036557197571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 train 0 12\n",
      "Phase: train. Epoch: 43. Loss: 0.09779119491577148\n",
      "43 train 1 24\n",
      "Phase: train. Epoch: 43. Loss: 0.07735688984394073\n",
      "43 train 2 36\n",
      "Phase: train. Epoch: 43. Loss: 0.06932489573955536\n",
      "43 train 3 48\n",
      "Phase: train. Epoch: 43. Loss: 0.0898004025220871\n",
      "43 train 4 60\n",
      "Phase: train. Epoch: 43. Loss: 0.08187294006347656\n",
      "43 train 5 72\n",
      "Phase: train. Epoch: 43. Loss: 0.09672026336193085\n",
      "43 train 6 84\n",
      "Phase: train. Epoch: 43. Loss: 0.0947079211473465\n",
      "43 train 7 96\n",
      "Phase: train. Epoch: 43. Loss: 0.07426171749830246\n",
      "43 train 8 108\n",
      "Phase: train. Epoch: 43. Loss: 0.0788627415895462\n",
      "43 train 9 120\n",
      "Phase: train. Epoch: 43. Loss: 0.07360878586769104\n",
      "43 train 10 132\n",
      "Phase: train. Epoch: 43. Loss: 0.08606190979480743\n",
      "43 train 11 144\n",
      "Phase: train. Epoch: 43. Loss: 0.08180698752403259\n",
      "43 train 12 156\n",
      "Phase: train. Epoch: 43. Loss: 0.06634320318698883\n",
      "43 train 13 168\n",
      "Phase: train. Epoch: 43. Loss: 0.08210153877735138\n",
      "43 train 14 180\n",
      "Phase: train. Epoch: 43. Loss: 0.07172667980194092\n",
      "43 train 15 192\n",
      "Phase: train. Epoch: 43. Loss: 0.09499945491552353\n",
      "43 train 16 204\n",
      "Phase: train. Epoch: 43. Loss: 0.10946016013622284\n",
      "43 train 17 216\n",
      "Phase: train. Epoch: 43. Loss: 0.10790765285491943\n",
      "43 train 18 228\n",
      "Phase: train. Epoch: 43. Loss: 0.08952762186527252\n",
      "43 train 19 240\n",
      "Phase: train. Epoch: 43. Loss: 0.08552884310483932\n",
      "43 train 20 252\n",
      "Phase: train. Epoch: 43. Loss: 0.0955921933054924\n",
      "43 train 21 264\n",
      "Phase: train. Epoch: 43. Loss: 0.08650141954421997\n",
      "43 train 22 276\n",
      "Phase: train. Epoch: 43. Loss: 0.07963059842586517\n",
      "43 train 23 288\n",
      "Phase: train. Epoch: 43. Loss: 0.0830862745642662\n",
      "43 train 24 300\n",
      "Phase: train. Epoch: 43. Loss: 0.09284518659114838\n",
      "43 train 25 312\n",
      "Phase: train. Epoch: 43. Loss: 0.08702382445335388\n",
      "43 train 26 324\n",
      "Phase: train. Epoch: 43. Loss: 0.08567838370800018\n",
      "43 train 27 336\n",
      "Phase: train. Epoch: 43. Loss: 0.07900448888540268\n",
      "43 train 28 348\n",
      "Phase: train. Epoch: 43. Loss: 0.08581334352493286\n",
      "43 train 29 360\n",
      "Phase: train. Epoch: 43. Loss: 0.11799438297748566\n",
      "43 train 30 372\n",
      "Phase: train. Epoch: 43. Loss: 0.10785245150327682\n",
      "43 train 31 384\n",
      "Phase: train. Epoch: 43. Loss: 0.05944336950778961\n",
      "43 train 32 396\n",
      "Phase: train. Epoch: 43. Loss: 0.10427176207304001\n",
      "43 train 33 408\n",
      "Phase: train. Epoch: 43. Loss: 0.093597412109375\n",
      "43 train 34 420\n",
      "Phase: train. Epoch: 43. Loss: 0.09435039758682251\n",
      "43 train 35 432\n",
      "Phase: train. Epoch: 43. Loss: 0.07206808030605316\n",
      "43 train 36 444\n",
      "Phase: train. Epoch: 43. Loss: 0.09573255479335785\n",
      "43 train 37 456\n",
      "Phase: train. Epoch: 43. Loss: 0.08890291303396225\n",
      "43 train 38 468\n",
      "Phase: train. Epoch: 43. Loss: 0.07707484066486359\n",
      "43 train 39 480\n",
      "Phase: train. Epoch: 43. Loss: 0.09567146003246307\n",
      "43 train 40 492\n",
      "Phase: train. Epoch: 43. Loss: 0.0773230642080307\n",
      "43 train 41 504\n",
      "Phase: train. Epoch: 43. Loss: 0.0946958065032959\n",
      "43 train 42 516\n",
      "Phase: train. Epoch: 43. Loss: 0.08593465387821198\n",
      "43 train 43 528\n",
      "Phase: train. Epoch: 43. Loss: 0.10665719956159592\n",
      "43 train 44 540\n",
      "Phase: train. Epoch: 43. Loss: 0.11054515838623047\n",
      "43 train 45 552\n",
      "Phase: train. Epoch: 43. Loss: 0.10364540666341782\n",
      "43 train 46 564\n",
      "Phase: train. Epoch: 43. Loss: 0.08654989302158356\n",
      "43 train 47 576\n",
      "Phase: train. Epoch: 43. Loss: 0.09971833229064941\n",
      "43 train 48 588\n",
      "Phase: train. Epoch: 43. Loss: 0.07240062206983566\n",
      "43 train 49 600\n",
      "Phase: train. Epoch: 43. Loss: 0.10670103877782822\n",
      "43 train 50 612\n",
      "Phase: train. Epoch: 43. Loss: 0.05814987048506737\n",
      "43 train 51 624\n",
      "Phase: train. Epoch: 43. Loss: 0.10265334695577621\n",
      "43 train 52 636\n",
      "Phase: train. Epoch: 43. Loss: 0.08120907098054886\n",
      "43 train 53 648\n",
      "Phase: train. Epoch: 43. Loss: 0.07793685048818588\n",
      "43 train 54 660\n",
      "Phase: train. Epoch: 43. Loss: 0.09098666906356812\n",
      "43 train 55 672\n",
      "Phase: train. Epoch: 43. Loss: 0.0848308652639389\n",
      "43 train 56 684\n",
      "Phase: train. Epoch: 43. Loss: 0.09491699934005737\n",
      "43 train 57 696\n",
      "Phase: train. Epoch: 43. Loss: 0.08036495745182037\n",
      "43 train 58 708\n",
      "Phase: train. Epoch: 43. Loss: 0.0877377986907959\n",
      "43 train 59 720\n",
      "Phase: train. Epoch: 43. Loss: 0.07717899233102798\n",
      "43 train 60 732\n",
      "Phase: train. Epoch: 43. Loss: 0.10270440578460693\n",
      "43 train 61 744\n",
      "Phase: train. Epoch: 43. Loss: 0.10333691537380219\n",
      "43 train 62 751\n",
      "Phase: train. Epoch: 43. Loss: 0.08886483311653137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 val 0 763\n",
      "Phase: val. Epoch: 43. Loss: 0.10096796602010727\n",
      "43 val 1 775\n",
      "Phase: val. Epoch: 43. Loss: 0.07006537914276123\n",
      "43 val 2 787\n",
      "Phase: val. Epoch: 43. Loss: 0.10690882056951523\n",
      "43 val 3 799\n",
      "Phase: val. Epoch: 43. Loss: 0.06915342062711716\n",
      "43 val 4 811\n",
      "Phase: val. Epoch: 43. Loss: 0.10173525661230087\n",
      "43 val 5 823\n",
      "Phase: val. Epoch: 43. Loss: 0.10381060838699341\n",
      "43 val 6 835\n",
      "Phase: val. Epoch: 43. Loss: 0.07978736609220505\n",
      "43 val 7 847\n",
      "Phase: val. Epoch: 43. Loss: 0.054858624935150146\n",
      "43 val 8 859\n",
      "Phase: val. Epoch: 43. Loss: 0.0742766410112381\n",
      "43 val 9 871\n",
      "Phase: val. Epoch: 43. Loss: 0.06546852737665176\n",
      "43 val 10 883\n",
      "Phase: val. Epoch: 43. Loss: 0.09515859186649323\n",
      "43 val 11 884\n",
      "Phase: val. Epoch: 43. Loss: 0.06552950292825699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 train 0 12\n",
      "Phase: train. Epoch: 44. Loss: 0.06888832151889801\n",
      "44 train 1 24\n",
      "Phase: train. Epoch: 44. Loss: 0.07904311269521713\n",
      "44 train 2 36\n",
      "Phase: train. Epoch: 44. Loss: 0.08192102611064911\n",
      "44 train 3 48\n",
      "Phase: train. Epoch: 44. Loss: 0.07313373684883118\n",
      "44 train 4 60\n",
      "Phase: train. Epoch: 44. Loss: 0.11070118844509125\n",
      "44 train 5 72\n",
      "Phase: train. Epoch: 44. Loss: 0.0934210866689682\n",
      "44 train 6 84\n",
      "Phase: train. Epoch: 44. Loss: 0.0902690514922142\n",
      "44 train 7 96\n",
      "Phase: train. Epoch: 44. Loss: 0.07759681344032288\n",
      "44 train 8 108\n",
      "Phase: train. Epoch: 44. Loss: 0.09618397802114487\n",
      "44 train 9 120\n",
      "Phase: train. Epoch: 44. Loss: 0.07269614189863205\n",
      "44 train 10 132\n",
      "Phase: train. Epoch: 44. Loss: 0.08304222673177719\n",
      "44 train 11 144\n",
      "Phase: train. Epoch: 44. Loss: 0.09559403359889984\n",
      "44 train 12 156\n",
      "Phase: train. Epoch: 44. Loss: 0.08627661317586899\n",
      "44 train 13 168\n",
      "Phase: train. Epoch: 44. Loss: 0.07940852642059326\n",
      "44 train 14 180\n",
      "Phase: train. Epoch: 44. Loss: 0.061611395329236984\n",
      "44 train 15 192\n",
      "Phase: train. Epoch: 44. Loss: 0.09089942276477814\n",
      "44 train 16 204\n",
      "Phase: train. Epoch: 44. Loss: 0.07628554105758667\n",
      "44 train 17 216\n",
      "Phase: train. Epoch: 44. Loss: 0.10461126267910004\n",
      "44 train 18 228\n",
      "Phase: train. Epoch: 44. Loss: 0.07244653254747391\n",
      "44 train 19 240\n",
      "Phase: train. Epoch: 44. Loss: 0.08285284042358398\n",
      "44 train 20 252\n",
      "Phase: train. Epoch: 44. Loss: 0.08641986548900604\n",
      "44 train 21 264\n",
      "Phase: train. Epoch: 44. Loss: 0.09129422903060913\n",
      "44 train 22 276\n",
      "Phase: train. Epoch: 44. Loss: 0.0868234783411026\n",
      "44 train 23 288\n",
      "Phase: train. Epoch: 44. Loss: 0.0681125596165657\n",
      "44 train 24 300\n",
      "Phase: train. Epoch: 44. Loss: 0.09870791435241699\n",
      "44 train 25 312\n",
      "Phase: train. Epoch: 44. Loss: 0.07818470895290375\n",
      "44 train 26 324\n",
      "Phase: train. Epoch: 44. Loss: 0.09693144261837006\n",
      "44 train 27 336\n",
      "Phase: train. Epoch: 44. Loss: 0.10453121364116669\n",
      "44 train 28 348\n",
      "Phase: train. Epoch: 44. Loss: 0.09382504224777222\n",
      "44 train 29 360\n",
      "Phase: train. Epoch: 44. Loss: 0.07157738506793976\n",
      "44 train 30 372\n",
      "Phase: train. Epoch: 44. Loss: 0.07571139186620712\n",
      "44 train 31 384\n",
      "Phase: train. Epoch: 44. Loss: 0.08442337810993195\n",
      "44 train 32 396\n",
      "Phase: train. Epoch: 44. Loss: 0.09969907999038696\n",
      "44 train 33 408\n",
      "Phase: train. Epoch: 44. Loss: 0.1081356406211853\n",
      "44 train 34 420\n",
      "Phase: train. Epoch: 44. Loss: 0.07981995493173599\n",
      "44 train 35 432\n",
      "Phase: train. Epoch: 44. Loss: 0.07968848943710327\n",
      "44 train 36 444\n",
      "Phase: train. Epoch: 44. Loss: 0.10552769154310226\n",
      "44 train 37 456\n",
      "Phase: train. Epoch: 44. Loss: 0.08363612741231918\n",
      "44 train 38 468\n",
      "Phase: train. Epoch: 44. Loss: 0.1430043876171112\n",
      "44 train 39 480\n",
      "Phase: train. Epoch: 44. Loss: 0.07830371707677841\n",
      "44 train 40 492\n",
      "Phase: train. Epoch: 44. Loss: 0.07126054167747498\n",
      "44 train 41 504\n",
      "Phase: train. Epoch: 44. Loss: 0.08003883808851242\n",
      "44 train 42 516\n",
      "Phase: train. Epoch: 44. Loss: 0.08601997792720795\n",
      "44 train 43 528\n",
      "Phase: train. Epoch: 44. Loss: 0.08791999518871307\n",
      "44 train 44 540\n",
      "Phase: train. Epoch: 44. Loss: 0.09039091318845749\n",
      "44 train 45 552\n",
      "Phase: train. Epoch: 44. Loss: 0.06489284336566925\n",
      "44 train 46 564\n",
      "Phase: train. Epoch: 44. Loss: 0.10130856931209564\n",
      "44 train 47 576\n",
      "Phase: train. Epoch: 44. Loss: 0.08796658366918564\n",
      "44 train 48 588\n",
      "Phase: train. Epoch: 44. Loss: 0.08906634896993637\n",
      "44 train 49 600\n",
      "Phase: train. Epoch: 44. Loss: 0.09495212882757187\n",
      "44 train 50 612\n",
      "Phase: train. Epoch: 44. Loss: 0.10665376484394073\n",
      "44 train 51 624\n",
      "Phase: train. Epoch: 44. Loss: 0.0766969621181488\n",
      "44 train 52 636\n",
      "Phase: train. Epoch: 44. Loss: 0.09029632806777954\n",
      "44 train 53 648\n",
      "Phase: train. Epoch: 44. Loss: 0.11286531388759613\n",
      "44 train 54 660\n",
      "Phase: train. Epoch: 44. Loss: 0.07635654509067535\n",
      "44 train 55 672\n",
      "Phase: train. Epoch: 44. Loss: 0.08251279592514038\n",
      "44 train 56 684\n",
      "Phase: train. Epoch: 44. Loss: 0.07649168372154236\n",
      "44 train 57 696\n",
      "Phase: train. Epoch: 44. Loss: 0.08094672858715057\n",
      "44 train 58 708\n",
      "Phase: train. Epoch: 44. Loss: 0.09698021411895752\n",
      "44 train 59 720\n",
      "Phase: train. Epoch: 44. Loss: 0.06502248346805573\n",
      "44 train 60 732\n",
      "Phase: train. Epoch: 44. Loss: 0.08817259967327118\n",
      "44 train 61 744\n",
      "Phase: train. Epoch: 44. Loss: 0.10572421550750732\n",
      "44 train 62 751\n",
      "Phase: train. Epoch: 44. Loss: 0.08967999368906021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 val 0 763\n",
      "Phase: val. Epoch: 44. Loss: 0.061070822179317474\n",
      "44 val 1 775\n",
      "Phase: val. Epoch: 44. Loss: 0.09128572791814804\n",
      "44 val 2 787\n",
      "Phase: val. Epoch: 44. Loss: 0.07205642759799957\n",
      "44 val 3 799\n",
      "Phase: val. Epoch: 44. Loss: 0.08283090591430664\n",
      "44 val 4 811\n",
      "Phase: val. Epoch: 44. Loss: 0.09639948606491089\n",
      "44 val 5 823\n",
      "Phase: val. Epoch: 44. Loss: 0.06170503795146942\n",
      "44 val 6 835\n",
      "Phase: val. Epoch: 44. Loss: 0.07391682267189026\n",
      "44 val 7 847\n",
      "Phase: val. Epoch: 44. Loss: 0.13196015357971191\n",
      "44 val 8 859\n",
      "Phase: val. Epoch: 44. Loss: 0.09333071112632751\n",
      "44 val 9 871\n",
      "Phase: val. Epoch: 44. Loss: 0.0684535950422287\n",
      "44 val 10 883\n",
      "Phase: val. Epoch: 44. Loss: 0.0668376162648201\n",
      "44 val 11 884\n",
      "Phase: val. Epoch: 44. Loss: 0.08087009191513062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 train 0 12\n",
      "Phase: train. Epoch: 45. Loss: 0.09609320759773254\n",
      "45 train 1 24\n",
      "Phase: train. Epoch: 45. Loss: 0.0692998394370079\n",
      "45 train 2 36\n",
      "Phase: train. Epoch: 45. Loss: 0.06498896330595016\n",
      "45 train 3 48\n",
      "Phase: train. Epoch: 45. Loss: 0.08423976600170135\n",
      "45 train 4 60\n",
      "Phase: train. Epoch: 45. Loss: 0.08626570552587509\n",
      "45 train 5 72\n",
      "Phase: train. Epoch: 45. Loss: 0.08405308425426483\n",
      "45 train 6 84\n",
      "Phase: train. Epoch: 45. Loss: 0.12745995819568634\n",
      "45 train 7 96\n",
      "Phase: train. Epoch: 45. Loss: 0.09204115718603134\n",
      "45 train 8 108\n",
      "Phase: train. Epoch: 45. Loss: 0.0800161212682724\n",
      "45 train 9 120\n",
      "Phase: train. Epoch: 45. Loss: 0.09304414689540863\n",
      "45 train 10 132\n",
      "Phase: train. Epoch: 45. Loss: 0.07746937870979309\n",
      "45 train 11 144\n",
      "Phase: train. Epoch: 45. Loss: 0.08413076400756836\n",
      "45 train 12 156\n",
      "Phase: train. Epoch: 45. Loss: 0.07688645273447037\n",
      "45 train 13 168\n",
      "Phase: train. Epoch: 45. Loss: 0.09421750158071518\n",
      "45 train 14 180\n",
      "Phase: train. Epoch: 45. Loss: 0.07786618173122406\n",
      "45 train 15 192\n",
      "Phase: train. Epoch: 45. Loss: 0.07037490606307983\n",
      "45 train 16 204\n",
      "Phase: train. Epoch: 45. Loss: 0.07944537699222565\n",
      "45 train 17 216\n",
      "Phase: train. Epoch: 45. Loss: 0.07367473095655441\n",
      "45 train 18 228\n",
      "Phase: train. Epoch: 45. Loss: 0.09158849716186523\n",
      "45 train 19 240\n",
      "Phase: train. Epoch: 45. Loss: 0.08935299515724182\n",
      "45 train 20 252\n",
      "Phase: train. Epoch: 45. Loss: 0.08744895458221436\n",
      "45 train 21 264\n",
      "Phase: train. Epoch: 45. Loss: 0.1082763671875\n",
      "45 train 22 276\n",
      "Phase: train. Epoch: 45. Loss: 0.0701824426651001\n",
      "45 train 23 288\n",
      "Phase: train. Epoch: 45. Loss: 0.13636350631713867\n",
      "45 train 24 300\n",
      "Phase: train. Epoch: 45. Loss: 0.09905817359685898\n",
      "45 train 25 312\n",
      "Phase: train. Epoch: 45. Loss: 0.07316747307777405\n",
      "45 train 26 324\n",
      "Phase: train. Epoch: 45. Loss: 0.09266237914562225\n",
      "45 train 27 336\n",
      "Phase: train. Epoch: 45. Loss: 0.07372568547725677\n",
      "45 train 28 348\n",
      "Phase: train. Epoch: 45. Loss: 0.07775509357452393\n",
      "45 train 29 360\n",
      "Phase: train. Epoch: 45. Loss: 0.08550326526165009\n",
      "45 train 30 372\n",
      "Phase: train. Epoch: 45. Loss: 0.10317077487707138\n",
      "45 train 31 384\n",
      "Phase: train. Epoch: 45. Loss: 0.10198773443698883\n",
      "45 train 32 396\n",
      "Phase: train. Epoch: 45. Loss: 0.07792308926582336\n",
      "45 train 33 408\n",
      "Phase: train. Epoch: 45. Loss: 0.08240532130002975\n",
      "45 train 34 420\n",
      "Phase: train. Epoch: 45. Loss: 0.07622954249382019\n",
      "45 train 35 432\n",
      "Phase: train. Epoch: 45. Loss: 0.09973417222499847\n",
      "45 train 36 444\n",
      "Phase: train. Epoch: 45. Loss: 0.08369326591491699\n",
      "45 train 37 456\n",
      "Phase: train. Epoch: 45. Loss: 0.08922767639160156\n",
      "45 train 38 468\n",
      "Phase: train. Epoch: 45. Loss: 0.08270597457885742\n",
      "45 train 39 480\n",
      "Phase: train. Epoch: 45. Loss: 0.06869657337665558\n",
      "45 train 40 492\n",
      "Phase: train. Epoch: 45. Loss: 0.08140832185745239\n",
      "45 train 41 504\n",
      "Phase: train. Epoch: 45. Loss: 0.07383314520120621\n",
      "45 train 42 516\n",
      "Phase: train. Epoch: 45. Loss: 0.08261121809482574\n",
      "45 train 43 528\n",
      "Phase: train. Epoch: 45. Loss: 0.10886965692043304\n",
      "45 train 44 540\n",
      "Phase: train. Epoch: 45. Loss: 0.10718858242034912\n",
      "45 train 45 552\n",
      "Phase: train. Epoch: 45. Loss: 0.08507480472326279\n",
      "45 train 46 564\n",
      "Phase: train. Epoch: 45. Loss: 0.11432535946369171\n",
      "45 train 47 576\n",
      "Phase: train. Epoch: 45. Loss: 0.09101678431034088\n",
      "45 train 48 588\n",
      "Phase: train. Epoch: 45. Loss: 0.10460172593593597\n",
      "45 train 49 600\n",
      "Phase: train. Epoch: 45. Loss: 0.10363027453422546\n",
      "45 train 50 612\n",
      "Phase: train. Epoch: 45. Loss: 0.09551563113927841\n",
      "45 train 51 624\n",
      "Phase: train. Epoch: 45. Loss: 0.09920396655797958\n",
      "45 train 52 636\n",
      "Phase: train. Epoch: 45. Loss: 0.10062316060066223\n",
      "45 train 53 648\n",
      "Phase: train. Epoch: 45. Loss: 0.09185422956943512\n",
      "45 train 54 660\n",
      "Phase: train. Epoch: 45. Loss: 0.06741465628147125\n",
      "45 train 55 672\n",
      "Phase: train. Epoch: 45. Loss: 0.0905841588973999\n",
      "45 train 56 684\n",
      "Phase: train. Epoch: 45. Loss: 0.09793613851070404\n",
      "45 train 57 696\n",
      "Phase: train. Epoch: 45. Loss: 0.08193238079547882\n",
      "45 train 58 708\n",
      "Phase: train. Epoch: 45. Loss: 0.09166815131902695\n",
      "45 train 59 720\n",
      "Phase: train. Epoch: 45. Loss: 0.06570881605148315\n",
      "45 train 60 732\n",
      "Phase: train. Epoch: 45. Loss: 0.07998054474592209\n",
      "45 train 61 744\n",
      "Phase: train. Epoch: 45. Loss: 0.06896097213029861\n",
      "45 train 62 751\n",
      "Phase: train. Epoch: 45. Loss: 0.0794164165854454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 val 0 763\n",
      "Phase: val. Epoch: 45. Loss: 0.0906798467040062\n",
      "45 val 1 775\n",
      "Phase: val. Epoch: 45. Loss: 0.10861511528491974\n",
      "45 val 2 787\n",
      "Phase: val. Epoch: 45. Loss: 0.0561351478099823\n",
      "45 val 3 799\n",
      "Phase: val. Epoch: 45. Loss: 0.09307768195867538\n",
      "45 val 4 811\n",
      "Phase: val. Epoch: 45. Loss: 0.06359778344631195\n",
      "45 val 5 823\n",
      "Phase: val. Epoch: 45. Loss: 0.08067508786916733\n",
      "45 val 6 835\n",
      "Phase: val. Epoch: 45. Loss: 0.07763530313968658\n",
      "45 val 7 847\n",
      "Phase: val. Epoch: 45. Loss: 0.11627089977264404\n",
      "45 val 8 859\n",
      "Phase: val. Epoch: 45. Loss: 0.10519379377365112\n",
      "45 val 9 871\n",
      "Phase: val. Epoch: 45. Loss: 0.05724891275167465\n",
      "45 val 10 883\n",
      "Phase: val. Epoch: 45. Loss: 0.08168987929821014\n",
      "45 val 11 884\n",
      "Phase: val. Epoch: 45. Loss: 0.08992618322372437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 train 0 12\n",
      "Phase: train. Epoch: 46. Loss: 0.08251352608203888\n",
      "46 train 1 24\n",
      "Phase: train. Epoch: 46. Loss: 0.07404103130102158\n",
      "46 train 2 36\n",
      "Phase: train. Epoch: 46. Loss: 0.07423623651266098\n",
      "46 train 3 48\n",
      "Phase: train. Epoch: 46. Loss: 0.09135156869888306\n",
      "46 train 4 60\n",
      "Phase: train. Epoch: 46. Loss: 0.08652272075414658\n",
      "46 train 5 72\n",
      "Phase: train. Epoch: 46. Loss: 0.09014517068862915\n",
      "46 train 6 84\n",
      "Phase: train. Epoch: 46. Loss: 0.09916819632053375\n",
      "46 train 7 96\n",
      "Phase: train. Epoch: 46. Loss: 0.10260574519634247\n",
      "46 train 8 108\n",
      "Phase: train. Epoch: 46. Loss: 0.059975337237119675\n",
      "46 train 9 120\n",
      "Phase: train. Epoch: 46. Loss: 0.0725441500544548\n",
      "46 train 10 132\n",
      "Phase: train. Epoch: 46. Loss: 0.10396146029233932\n",
      "46 train 11 144\n",
      "Phase: train. Epoch: 46. Loss: 0.06657975912094116\n",
      "46 train 12 156\n",
      "Phase: train. Epoch: 46. Loss: 0.08837368339300156\n",
      "46 train 13 168\n",
      "Phase: train. Epoch: 46. Loss: 0.1145220622420311\n",
      "46 train 14 180\n",
      "Phase: train. Epoch: 46. Loss: 0.0811500996351242\n",
      "46 train 15 192\n",
      "Phase: train. Epoch: 46. Loss: 0.07487724721431732\n",
      "46 train 16 204\n",
      "Phase: train. Epoch: 46. Loss: 0.11958365887403488\n",
      "46 train 17 216\n",
      "Phase: train. Epoch: 46. Loss: 0.10233835875988007\n",
      "46 train 18 228\n",
      "Phase: train. Epoch: 46. Loss: 0.1078648567199707\n",
      "46 train 19 240\n",
      "Phase: train. Epoch: 46. Loss: 0.07984622567892075\n",
      "46 train 20 252\n",
      "Phase: train. Epoch: 46. Loss: 0.12300034612417221\n",
      "46 train 21 264\n",
      "Phase: train. Epoch: 46. Loss: 0.08477907627820969\n",
      "46 train 22 276\n",
      "Phase: train. Epoch: 46. Loss: 0.08338472247123718\n",
      "46 train 23 288\n",
      "Phase: train. Epoch: 46. Loss: 0.07533686608076096\n",
      "46 train 24 300\n",
      "Phase: train. Epoch: 46. Loss: 0.07533793151378632\n",
      "46 train 25 312\n",
      "Phase: train. Epoch: 46. Loss: 0.08084508031606674\n",
      "46 train 26 324\n",
      "Phase: train. Epoch: 46. Loss: 0.08765578269958496\n",
      "46 train 27 336\n",
      "Phase: train. Epoch: 46. Loss: 0.10081543773412704\n",
      "46 train 28 348\n",
      "Phase: train. Epoch: 46. Loss: 0.07123031467199326\n",
      "46 train 29 360\n",
      "Phase: train. Epoch: 46. Loss: 0.08428332209587097\n",
      "46 train 30 372\n",
      "Phase: train. Epoch: 46. Loss: 0.08058921992778778\n",
      "46 train 31 384\n",
      "Phase: train. Epoch: 46. Loss: 0.08758804202079773\n",
      "46 train 32 396\n",
      "Phase: train. Epoch: 46. Loss: 0.07631485909223557\n",
      "46 train 33 408\n",
      "Phase: train. Epoch: 46. Loss: 0.08227695524692535\n",
      "46 train 34 420\n",
      "Phase: train. Epoch: 46. Loss: 0.0798492357134819\n",
      "46 train 35 432\n",
      "Phase: train. Epoch: 46. Loss: 0.09240800142288208\n",
      "46 train 36 444\n",
      "Phase: train. Epoch: 46. Loss: 0.11244591325521469\n",
      "46 train 37 456\n",
      "Phase: train. Epoch: 46. Loss: 0.0599459707736969\n",
      "46 train 38 468\n",
      "Phase: train. Epoch: 46. Loss: 0.09784932434558868\n",
      "46 train 39 480\n",
      "Phase: train. Epoch: 46. Loss: 0.08772584795951843\n",
      "46 train 40 492\n",
      "Phase: train. Epoch: 46. Loss: 0.0929267629981041\n",
      "46 train 41 504\n",
      "Phase: train. Epoch: 46. Loss: 0.11084543168544769\n",
      "46 train 42 516\n",
      "Phase: train. Epoch: 46. Loss: 0.0839996486902237\n",
      "46 train 43 528\n",
      "Phase: train. Epoch: 46. Loss: 0.0664706751704216\n",
      "46 train 44 540\n",
      "Phase: train. Epoch: 46. Loss: 0.10904565453529358\n",
      "46 train 45 552\n",
      "Phase: train. Epoch: 46. Loss: 0.07695534080266953\n",
      "46 train 46 564\n",
      "Phase: train. Epoch: 46. Loss: 0.10242721438407898\n",
      "46 train 47 576\n",
      "Phase: train. Epoch: 46. Loss: 0.08990495651960373\n",
      "46 train 48 588\n",
      "Phase: train. Epoch: 46. Loss: 0.09944875538349152\n",
      "46 train 49 600\n",
      "Phase: train. Epoch: 46. Loss: 0.09729697555303574\n",
      "46 train 50 612\n",
      "Phase: train. Epoch: 46. Loss: 0.06637366861104965\n",
      "46 train 51 624\n",
      "Phase: train. Epoch: 46. Loss: 0.10126667469739914\n",
      "46 train 52 636\n",
      "Phase: train. Epoch: 46. Loss: 0.06438278406858444\n",
      "46 train 53 648\n",
      "Phase: train. Epoch: 46. Loss: 0.06869722902774811\n",
      "46 train 54 660\n",
      "Phase: train. Epoch: 46. Loss: 0.08611207455396652\n",
      "46 train 55 672\n",
      "Phase: train. Epoch: 46. Loss: 0.08322739601135254\n",
      "46 train 56 684\n",
      "Phase: train. Epoch: 46. Loss: 0.07090771198272705\n",
      "46 train 57 696\n",
      "Phase: train. Epoch: 46. Loss: 0.10385042428970337\n",
      "46 train 58 708\n",
      "Phase: train. Epoch: 46. Loss: 0.10108812153339386\n",
      "46 train 59 720\n",
      "Phase: train. Epoch: 46. Loss: 0.10020595788955688\n",
      "46 train 60 732\n",
      "Phase: train. Epoch: 46. Loss: 0.09119739383459091\n",
      "46 train 61 744\n",
      "Phase: train. Epoch: 46. Loss: 0.06588023155927658\n",
      "46 train 62 751\n",
      "Phase: train. Epoch: 46. Loss: 0.07548484206199646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 val 0 763\n",
      "Phase: val. Epoch: 46. Loss: 0.07297110557556152\n",
      "46 val 1 775\n",
      "Phase: val. Epoch: 46. Loss: 0.08416639268398285\n",
      "46 val 2 787\n",
      "Phase: val. Epoch: 46. Loss: 0.13133373856544495\n",
      "46 val 3 799\n",
      "Phase: val. Epoch: 46. Loss: 0.07608262449502945\n",
      "46 val 4 811\n",
      "Phase: val. Epoch: 46. Loss: 0.07307261228561401\n",
      "46 val 5 823\n",
      "Phase: val. Epoch: 46. Loss: 0.06404738873243332\n",
      "46 val 6 835\n",
      "Phase: val. Epoch: 46. Loss: 0.08016564697027206\n",
      "46 val 7 847\n",
      "Phase: val. Epoch: 46. Loss: 0.04623367637395859\n",
      "46 val 8 859\n",
      "Phase: val. Epoch: 46. Loss: 0.08045420050621033\n",
      "46 val 9 871\n",
      "Phase: val. Epoch: 46. Loss: 0.10660557448863983\n",
      "46 val 10 883\n",
      "Phase: val. Epoch: 46. Loss: 0.09417041391134262\n",
      "46 val 11 884\n",
      "Phase: val. Epoch: 46. Loss: 0.029533905908465385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 train 0 12\n",
      "Phase: train. Epoch: 47. Loss: 0.10332994163036346\n",
      "47 train 1 24\n",
      "Phase: train. Epoch: 47. Loss: 0.06682953238487244\n",
      "47 train 2 36\n",
      "Phase: train. Epoch: 47. Loss: 0.0771925151348114\n",
      "47 train 3 48\n",
      "Phase: train. Epoch: 47. Loss: 0.08421114087104797\n",
      "47 train 4 60\n",
      "Phase: train. Epoch: 47. Loss: 0.10419167578220367\n",
      "47 train 5 72\n",
      "Phase: train. Epoch: 47. Loss: 0.13571606576442719\n",
      "47 train 6 84\n",
      "Phase: train. Epoch: 47. Loss: 0.07182750850915909\n",
      "47 train 7 96\n",
      "Phase: train. Epoch: 47. Loss: 0.08383794128894806\n",
      "47 train 8 108\n",
      "Phase: train. Epoch: 47. Loss: 0.0768488198518753\n",
      "47 train 9 120\n",
      "Phase: train. Epoch: 47. Loss: 0.06989292055368423\n",
      "47 train 10 132\n",
      "Phase: train. Epoch: 47. Loss: 0.10369431972503662\n",
      "47 train 11 144\n",
      "Phase: train. Epoch: 47. Loss: 0.08979067206382751\n",
      "47 train 12 156\n",
      "Phase: train. Epoch: 47. Loss: 0.08095555752515793\n",
      "47 train 13 168\n",
      "Phase: train. Epoch: 47. Loss: 0.0957014411687851\n",
      "47 train 14 180\n",
      "Phase: train. Epoch: 47. Loss: 0.10363736003637314\n",
      "47 train 15 192\n",
      "Phase: train. Epoch: 47. Loss: 0.11738580465316772\n",
      "47 train 16 204\n",
      "Phase: train. Epoch: 47. Loss: 0.0942862331867218\n",
      "47 train 17 216\n",
      "Phase: train. Epoch: 47. Loss: 0.09731575846672058\n",
      "47 train 18 228\n",
      "Phase: train. Epoch: 47. Loss: 0.1114339828491211\n",
      "47 train 19 240\n",
      "Phase: train. Epoch: 47. Loss: 0.08291023224592209\n",
      "47 train 20 252\n",
      "Phase: train. Epoch: 47. Loss: 0.08078262954950333\n",
      "47 train 21 264\n",
      "Phase: train. Epoch: 47. Loss: 0.0790686309337616\n",
      "47 train 22 276\n",
      "Phase: train. Epoch: 47. Loss: 0.07799562811851501\n",
      "47 train 23 288\n",
      "Phase: train. Epoch: 47. Loss: 0.10458390414714813\n",
      "47 train 24 300\n",
      "Phase: train. Epoch: 47. Loss: 0.08555888384580612\n",
      "47 train 25 312\n",
      "Phase: train. Epoch: 47. Loss: 0.07376512885093689\n",
      "47 train 26 324\n",
      "Phase: train. Epoch: 47. Loss: 0.0844908282160759\n",
      "47 train 27 336\n",
      "Phase: train. Epoch: 47. Loss: 0.09691997617483139\n",
      "47 train 28 348\n",
      "Phase: train. Epoch: 47. Loss: 0.08820372819900513\n",
      "47 train 29 360\n",
      "Phase: train. Epoch: 47. Loss: 0.09119372069835663\n",
      "47 train 30 372\n",
      "Phase: train. Epoch: 47. Loss: 0.07505857199430466\n",
      "47 train 31 384\n",
      "Phase: train. Epoch: 47. Loss: 0.10333477705717087\n",
      "47 train 32 396\n",
      "Phase: train. Epoch: 47. Loss: 0.09573520720005035\n",
      "47 train 33 408\n",
      "Phase: train. Epoch: 47. Loss: 0.08626585453748703\n",
      "47 train 34 420\n",
      "Phase: train. Epoch: 47. Loss: 0.08585216850042343\n",
      "47 train 35 432\n",
      "Phase: train. Epoch: 47. Loss: 0.08269645273685455\n",
      "47 train 36 444\n",
      "Phase: train. Epoch: 47. Loss: 0.06891050934791565\n",
      "47 train 37 456\n",
      "Phase: train. Epoch: 47. Loss: 0.07301565259695053\n",
      "47 train 38 468\n",
      "Phase: train. Epoch: 47. Loss: 0.08358582109212875\n",
      "47 train 39 480\n",
      "Phase: train. Epoch: 47. Loss: 0.07769909501075745\n",
      "47 train 40 492\n",
      "Phase: train. Epoch: 47. Loss: 0.08918461203575134\n",
      "47 train 41 504\n",
      "Phase: train. Epoch: 47. Loss: 0.081777423620224\n",
      "47 train 42 516\n",
      "Phase: train. Epoch: 47. Loss: 0.07975436002016068\n",
      "47 train 43 528\n",
      "Phase: train. Epoch: 47. Loss: 0.12047281116247177\n",
      "47 train 44 540\n",
      "Phase: train. Epoch: 47. Loss: 0.07990184426307678\n",
      "47 train 45 552\n",
      "Phase: train. Epoch: 47. Loss: 0.08241875469684601\n",
      "47 train 46 564\n",
      "Phase: train. Epoch: 47. Loss: 0.08509969711303711\n",
      "47 train 47 576\n",
      "Phase: train. Epoch: 47. Loss: 0.0654786080121994\n",
      "47 train 48 588\n",
      "Phase: train. Epoch: 47. Loss: 0.09790964424610138\n",
      "47 train 49 600\n",
      "Phase: train. Epoch: 47. Loss: 0.07940524816513062\n",
      "47 train 50 612\n",
      "Phase: train. Epoch: 47. Loss: 0.07252387702465057\n",
      "47 train 51 624\n",
      "Phase: train. Epoch: 47. Loss: 0.09120063483715057\n",
      "47 train 52 636\n",
      "Phase: train. Epoch: 47. Loss: 0.10288752615451813\n",
      "47 train 53 648\n",
      "Phase: train. Epoch: 47. Loss: 0.09133455157279968\n",
      "47 train 54 660\n",
      "Phase: train. Epoch: 47. Loss: 0.0688355341553688\n",
      "47 train 55 672\n",
      "Phase: train. Epoch: 47. Loss: 0.060934267938137054\n",
      "47 train 56 684\n",
      "Phase: train. Epoch: 47. Loss: 0.07061046361923218\n",
      "47 train 57 696\n",
      "Phase: train. Epoch: 47. Loss: 0.09021900594234467\n",
      "47 train 58 708\n",
      "Phase: train. Epoch: 47. Loss: 0.0693824514746666\n",
      "47 train 59 720\n",
      "Phase: train. Epoch: 47. Loss: 0.10162678360939026\n",
      "47 train 60 732\n",
      "Phase: train. Epoch: 47. Loss: 0.08179701864719391\n",
      "47 train 61 744\n",
      "Phase: train. Epoch: 47. Loss: 0.0782083198428154\n",
      "47 train 62 751\n",
      "Phase: train. Epoch: 47. Loss: 0.09086297452449799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 val 0 763\n",
      "Phase: val. Epoch: 47. Loss: 0.1047496646642685\n",
      "47 val 1 775\n",
      "Phase: val. Epoch: 47. Loss: 0.08165735006332397\n",
      "47 val 2 787\n",
      "Phase: val. Epoch: 47. Loss: 0.09070747345685959\n",
      "47 val 3 799\n",
      "Phase: val. Epoch: 47. Loss: 0.07532699406147003\n",
      "47 val 4 811\n",
      "Phase: val. Epoch: 47. Loss: 0.09068145602941513\n",
      "47 val 5 823\n",
      "Phase: val. Epoch: 47. Loss: 0.09153901040554047\n",
      "47 val 6 835\n",
      "Phase: val. Epoch: 47. Loss: 0.09836022555828094\n",
      "47 val 7 847\n",
      "Phase: val. Epoch: 47. Loss: 0.08873959630727768\n",
      "47 val 8 859\n",
      "Phase: val. Epoch: 47. Loss: 0.07719516009092331\n",
      "47 val 9 871\n",
      "Phase: val. Epoch: 47. Loss: 0.05895253270864487\n",
      "47 val 10 883\n",
      "Phase: val. Epoch: 47. Loss: 0.06182829290628433\n",
      "47 val 11 884\n",
      "Phase: val. Epoch: 47. Loss: 0.11566126346588135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 train 0 12\n",
      "Phase: train. Epoch: 48. Loss: 0.08474212884902954\n",
      "48 train 1 24\n",
      "Phase: train. Epoch: 48. Loss: 0.08547121286392212\n",
      "48 train 2 36\n",
      "Phase: train. Epoch: 48. Loss: 0.08850379288196564\n",
      "48 train 3 48\n",
      "Phase: train. Epoch: 48. Loss: 0.09330248832702637\n",
      "48 train 4 60\n",
      "Phase: train. Epoch: 48. Loss: 0.10189308971166611\n",
      "48 train 5 72\n",
      "Phase: train. Epoch: 48. Loss: 0.08584394305944443\n",
      "48 train 6 84\n",
      "Phase: train. Epoch: 48. Loss: 0.08650120347738266\n",
      "48 train 7 96\n",
      "Phase: train. Epoch: 48. Loss: 0.08916117995977402\n",
      "48 train 8 108\n",
      "Phase: train. Epoch: 48. Loss: 0.10098438709974289\n",
      "48 train 9 120\n",
      "Phase: train. Epoch: 48. Loss: 0.06770441681146622\n",
      "48 train 10 132\n",
      "Phase: train. Epoch: 48. Loss: 0.0929165631532669\n",
      "48 train 11 144\n",
      "Phase: train. Epoch: 48. Loss: 0.09107693284749985\n",
      "48 train 12 156\n",
      "Phase: train. Epoch: 48. Loss: 0.0672069862484932\n",
      "48 train 13 168\n",
      "Phase: train. Epoch: 48. Loss: 0.08270411938428879\n",
      "48 train 14 180\n",
      "Phase: train. Epoch: 48. Loss: 0.06593465059995651\n",
      "48 train 15 192\n",
      "Phase: train. Epoch: 48. Loss: 0.07829051464796066\n",
      "48 train 16 204\n",
      "Phase: train. Epoch: 48. Loss: 0.08226318657398224\n",
      "48 train 17 216\n",
      "Phase: train. Epoch: 48. Loss: 0.09786711633205414\n",
      "48 train 18 228\n",
      "Phase: train. Epoch: 48. Loss: 0.11457961797714233\n",
      "48 train 19 240\n",
      "Phase: train. Epoch: 48. Loss: 0.11389671266078949\n",
      "48 train 20 252\n",
      "Phase: train. Epoch: 48. Loss: 0.06417606770992279\n",
      "48 train 21 264\n",
      "Phase: train. Epoch: 48. Loss: 0.07979778200387955\n",
      "48 train 22 276\n",
      "Phase: train. Epoch: 48. Loss: 0.08755387365818024\n",
      "48 train 23 288\n",
      "Phase: train. Epoch: 48. Loss: 0.09827445447444916\n",
      "48 train 24 300\n",
      "Phase: train. Epoch: 48. Loss: 0.08232618123292923\n",
      "48 train 25 312\n",
      "Phase: train. Epoch: 48. Loss: 0.08186286687850952\n",
      "48 train 26 324\n",
      "Phase: train. Epoch: 48. Loss: 0.08459930866956711\n",
      "48 train 27 336\n",
      "Phase: train. Epoch: 48. Loss: 0.09366530179977417\n",
      "48 train 28 348\n",
      "Phase: train. Epoch: 48. Loss: 0.07952450960874557\n",
      "48 train 29 360\n",
      "Phase: train. Epoch: 48. Loss: 0.09047423303127289\n",
      "48 train 30 372\n",
      "Phase: train. Epoch: 48. Loss: 0.08129595220088959\n",
      "48 train 31 384\n",
      "Phase: train. Epoch: 48. Loss: 0.06603211164474487\n",
      "48 train 32 396\n",
      "Phase: train. Epoch: 48. Loss: 0.08138459175825119\n",
      "48 train 33 408\n",
      "Phase: train. Epoch: 48. Loss: 0.07742258906364441\n",
      "48 train 34 420\n",
      "Phase: train. Epoch: 48. Loss: 0.11193268746137619\n",
      "48 train 35 432\n",
      "Phase: train. Epoch: 48. Loss: 0.0723055750131607\n",
      "48 train 36 444\n",
      "Phase: train. Epoch: 48. Loss: 0.11103369295597076\n",
      "48 train 37 456\n",
      "Phase: train. Epoch: 48. Loss: 0.08700309693813324\n",
      "48 train 38 468\n",
      "Phase: train. Epoch: 48. Loss: 0.07710644602775574\n",
      "48 train 39 480\n",
      "Phase: train. Epoch: 48. Loss: 0.07511015236377716\n",
      "48 train 40 492\n",
      "Phase: train. Epoch: 48. Loss: 0.09221764653921127\n",
      "48 train 41 504\n",
      "Phase: train. Epoch: 48. Loss: 0.0862368494272232\n",
      "48 train 42 516\n",
      "Phase: train. Epoch: 48. Loss: 0.08713369816541672\n",
      "48 train 43 528\n",
      "Phase: train. Epoch: 48. Loss: 0.07478334754705429\n",
      "48 train 44 540\n",
      "Phase: train. Epoch: 48. Loss: 0.10145633667707443\n",
      "48 train 45 552\n",
      "Phase: train. Epoch: 48. Loss: 0.08644609898328781\n",
      "48 train 46 564\n",
      "Phase: train. Epoch: 48. Loss: 0.08303894102573395\n",
      "48 train 47 576\n",
      "Phase: train. Epoch: 48. Loss: 0.0748668685555458\n",
      "48 train 48 588\n",
      "Phase: train. Epoch: 48. Loss: 0.08411705493927002\n",
      "48 train 49 600\n",
      "Phase: train. Epoch: 48. Loss: 0.09147307276725769\n",
      "48 train 50 612\n",
      "Phase: train. Epoch: 48. Loss: 0.09359823167324066\n",
      "48 train 51 624\n",
      "Phase: train. Epoch: 48. Loss: 0.09660213440656662\n",
      "48 train 52 636\n",
      "Phase: train. Epoch: 48. Loss: 0.10252915322780609\n",
      "48 train 53 648\n",
      "Phase: train. Epoch: 48. Loss: 0.07764557003974915\n",
      "48 train 54 660\n",
      "Phase: train. Epoch: 48. Loss: 0.0993928611278534\n",
      "48 train 55 672\n",
      "Phase: train. Epoch: 48. Loss: 0.08711296319961548\n",
      "48 train 56 684\n",
      "Phase: train. Epoch: 48. Loss: 0.09199750423431396\n",
      "48 train 57 696\n",
      "Phase: train. Epoch: 48. Loss: 0.07044661790132523\n",
      "48 train 58 708\n",
      "Phase: train. Epoch: 48. Loss: 0.09804670512676239\n",
      "48 train 59 720\n",
      "Phase: train. Epoch: 48. Loss: 0.07160846143960953\n",
      "48 train 60 732\n",
      "Phase: train. Epoch: 48. Loss: 0.05715809389948845\n",
      "48 train 61 744\n",
      "Phase: train. Epoch: 48. Loss: 0.06752264499664307\n",
      "48 train 62 751\n",
      "Phase: train. Epoch: 48. Loss: 0.10970373451709747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 val 0 763\n",
      "Phase: val. Epoch: 48. Loss: 0.05529005825519562\n",
      "48 val 1 775\n",
      "Phase: val. Epoch: 48. Loss: 0.07148929685354233\n",
      "48 val 2 787\n",
      "Phase: val. Epoch: 48. Loss: 0.058694373816251755\n",
      "48 val 3 799\n",
      "Phase: val. Epoch: 48. Loss: 0.12304125726222992\n",
      "48 val 4 811\n",
      "Phase: val. Epoch: 48. Loss: 0.10527169704437256\n",
      "48 val 5 823\n",
      "Phase: val. Epoch: 48. Loss: 0.08277523517608643\n",
      "48 val 6 835\n",
      "Phase: val. Epoch: 48. Loss: 0.07116618007421494\n",
      "48 val 7 847\n",
      "Phase: val. Epoch: 48. Loss: 0.08033899962902069\n",
      "48 val 8 859\n",
      "Phase: val. Epoch: 48. Loss: 0.09711306542158127\n",
      "48 val 9 871\n",
      "Phase: val. Epoch: 48. Loss: 0.09167718887329102\n",
      "48 val 10 883\n",
      "Phase: val. Epoch: 48. Loss: 0.10055136680603027\n",
      "48 val 11 884\n",
      "Phase: val. Epoch: 48. Loss: 0.11872965842485428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 train 0 12\n",
      "Phase: train. Epoch: 49. Loss: 0.09380887448787689\n",
      "49 train 1 24\n",
      "Phase: train. Epoch: 49. Loss: 0.12149715423583984\n",
      "49 train 2 36\n",
      "Phase: train. Epoch: 49. Loss: 0.07054410874843597\n",
      "49 train 3 48\n",
      "Phase: train. Epoch: 49. Loss: 0.08608927577733994\n",
      "49 train 4 60\n",
      "Phase: train. Epoch: 49. Loss: 0.06628665328025818\n",
      "49 train 5 72\n",
      "Phase: train. Epoch: 49. Loss: 0.09355317056179047\n",
      "49 train 6 84\n",
      "Phase: train. Epoch: 49. Loss: 0.10491041839122772\n",
      "49 train 7 96\n",
      "Phase: train. Epoch: 49. Loss: 0.0869302749633789\n",
      "49 train 8 108\n",
      "Phase: train. Epoch: 49. Loss: 0.09369808435440063\n",
      "49 train 9 120\n",
      "Phase: train. Epoch: 49. Loss: 0.06857363879680634\n",
      "49 train 10 132\n",
      "Phase: train. Epoch: 49. Loss: 0.09075404703617096\n",
      "49 train 11 144\n",
      "Phase: train. Epoch: 49. Loss: 0.10411572456359863\n",
      "49 train 12 156\n",
      "Phase: train. Epoch: 49. Loss: 0.08533674478530884\n",
      "49 train 13 168\n",
      "Phase: train. Epoch: 49. Loss: 0.0767432451248169\n",
      "49 train 14 180\n",
      "Phase: train. Epoch: 49. Loss: 0.09961044788360596\n",
      "49 train 15 192\n",
      "Phase: train. Epoch: 49. Loss: 0.08829545974731445\n",
      "49 train 16 204\n",
      "Phase: train. Epoch: 49. Loss: 0.11293667554855347\n",
      "49 train 17 216\n",
      "Phase: train. Epoch: 49. Loss: 0.09690050780773163\n",
      "49 train 18 228\n",
      "Phase: train. Epoch: 49. Loss: 0.07417398691177368\n",
      "49 train 19 240\n",
      "Phase: train. Epoch: 49. Loss: 0.10820631682872772\n",
      "49 train 20 252\n",
      "Phase: train. Epoch: 49. Loss: 0.08049893379211426\n",
      "49 train 21 264\n",
      "Phase: train. Epoch: 49. Loss: 0.06773959845304489\n",
      "49 train 22 276\n",
      "Phase: train. Epoch: 49. Loss: 0.08680880069732666\n",
      "49 train 23 288\n",
      "Phase: train. Epoch: 49. Loss: 0.0972134917974472\n",
      "49 train 24 300\n",
      "Phase: train. Epoch: 49. Loss: 0.08184119313955307\n",
      "49 train 25 312\n",
      "Phase: train. Epoch: 49. Loss: 0.079108327627182\n",
      "49 train 26 324\n",
      "Phase: train. Epoch: 49. Loss: 0.07318982481956482\n",
      "49 train 27 336\n",
      "Phase: train. Epoch: 49. Loss: 0.0819997489452362\n",
      "49 train 28 348\n",
      "Phase: train. Epoch: 49. Loss: 0.07579030096530914\n",
      "49 train 29 360\n",
      "Phase: train. Epoch: 49. Loss: 0.06886672973632812\n",
      "49 train 30 372\n",
      "Phase: train. Epoch: 49. Loss: 0.06889201700687408\n",
      "49 train 31 384\n",
      "Phase: train. Epoch: 49. Loss: 0.09149469435214996\n",
      "49 train 32 396\n",
      "Phase: train. Epoch: 49. Loss: 0.08091855049133301\n",
      "49 train 33 408\n",
      "Phase: train. Epoch: 49. Loss: 0.08404852449893951\n",
      "49 train 34 420\n",
      "Phase: train. Epoch: 49. Loss: 0.077689990401268\n",
      "49 train 35 432\n",
      "Phase: train. Epoch: 49. Loss: 0.10832294076681137\n",
      "49 train 36 444\n",
      "Phase: train. Epoch: 49. Loss: 0.07950663566589355\n",
      "49 train 37 456\n",
      "Phase: train. Epoch: 49. Loss: 0.084650419652462\n",
      "49 train 38 468\n",
      "Phase: train. Epoch: 49. Loss: 0.08047040551900864\n",
      "49 train 39 480\n",
      "Phase: train. Epoch: 49. Loss: 0.07115861773490906\n",
      "49 train 40 492\n",
      "Phase: train. Epoch: 49. Loss: 0.08455133438110352\n",
      "49 train 41 504\n",
      "Phase: train. Epoch: 49. Loss: 0.09410710632801056\n",
      "49 train 42 516\n",
      "Phase: train. Epoch: 49. Loss: 0.12024511396884918\n",
      "49 train 43 528\n",
      "Phase: train. Epoch: 49. Loss: 0.09384086728096008\n",
      "49 train 44 540\n",
      "Phase: train. Epoch: 49. Loss: 0.08366291224956512\n",
      "49 train 45 552\n",
      "Phase: train. Epoch: 49. Loss: 0.08057955652475357\n",
      "49 train 46 564\n",
      "Phase: train. Epoch: 49. Loss: 0.09927025437355042\n",
      "49 train 47 576\n",
      "Phase: train. Epoch: 49. Loss: 0.09114798158407211\n",
      "49 train 48 588\n",
      "Phase: train. Epoch: 49. Loss: 0.10616739839315414\n",
      "49 train 49 600\n",
      "Phase: train. Epoch: 49. Loss: 0.0905347689986229\n",
      "49 train 50 612\n",
      "Phase: train. Epoch: 49. Loss: 0.1013713926076889\n",
      "49 train 51 624\n",
      "Phase: train. Epoch: 49. Loss: 0.09848850220441818\n",
      "49 train 52 636\n",
      "Phase: train. Epoch: 49. Loss: 0.08909444510936737\n",
      "49 train 53 648\n",
      "Phase: train. Epoch: 49. Loss: 0.07275919616222382\n",
      "49 train 54 660\n",
      "Phase: train. Epoch: 49. Loss: 0.07354330271482468\n",
      "49 train 55 672\n",
      "Phase: train. Epoch: 49. Loss: 0.0733334869146347\n",
      "49 train 56 684\n",
      "Phase: train. Epoch: 49. Loss: 0.06482162326574326\n",
      "49 train 57 696\n",
      "Phase: train. Epoch: 49. Loss: 0.11694890260696411\n",
      "49 train 58 708\n",
      "Phase: train. Epoch: 49. Loss: 0.06887184083461761\n",
      "49 train 59 720\n",
      "Phase: train. Epoch: 49. Loss: 0.08127706497907639\n",
      "49 train 60 732\n",
      "Phase: train. Epoch: 49. Loss: 0.11656498908996582\n",
      "49 train 61 744\n",
      "Phase: train. Epoch: 49. Loss: 0.10713694989681244\n",
      "49 train 62 751\n",
      "Phase: train. Epoch: 49. Loss: 0.08139470964670181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 val 0 763\n",
      "Phase: val. Epoch: 49. Loss: 0.08665710687637329\n",
      "49 val 1 775\n",
      "Phase: val. Epoch: 49. Loss: 0.08942493051290512\n",
      "49 val 2 787\n",
      "Phase: val. Epoch: 49. Loss: 0.06649325788021088\n",
      "49 val 3 799\n",
      "Phase: val. Epoch: 49. Loss: 0.06377515196800232\n",
      "49 val 4 811\n",
      "Phase: val. Epoch: 49. Loss: 0.07320749759674072\n",
      "49 val 5 823\n",
      "Phase: val. Epoch: 49. Loss: 0.09008269011974335\n",
      "49 val 6 835\n",
      "Phase: val. Epoch: 49. Loss: 0.1112215518951416\n",
      "49 val 7 847\n",
      "Phase: val. Epoch: 49. Loss: 0.05337090045213699\n",
      "49 val 8 859\n",
      "Phase: val. Epoch: 49. Loss: 0.08881738781929016\n",
      "49 val 9 871\n",
      "Phase: val. Epoch: 49. Loss: 0.07836443185806274\n",
      "49 val 10 883\n",
      "Phase: val. Epoch: 49. Loss: 0.11605589091777802\n",
      "49 val 11 884\n",
      "Phase: val. Epoch: 49. Loss: 0.02744794450700283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 train 0 12\n",
      "Phase: train. Epoch: 50. Loss: 0.11436592042446136\n",
      "50 train 1 24\n",
      "Phase: train. Epoch: 50. Loss: 0.10840485990047455\n",
      "50 train 2 36\n",
      "Phase: train. Epoch: 50. Loss: 0.05916298180818558\n",
      "50 train 3 48\n",
      "Phase: train. Epoch: 50. Loss: 0.06946758925914764\n",
      "50 train 4 60\n",
      "Phase: train. Epoch: 50. Loss: 0.06794866919517517\n",
      "50 train 5 72\n",
      "Phase: train. Epoch: 50. Loss: 0.07445159554481506\n",
      "50 train 6 84\n",
      "Phase: train. Epoch: 50. Loss: 0.06818453222513199\n",
      "50 train 7 96\n",
      "Phase: train. Epoch: 50. Loss: 0.0724819004535675\n",
      "50 train 8 108\n",
      "Phase: train. Epoch: 50. Loss: 0.1002807468175888\n",
      "50 train 9 120\n",
      "Phase: train. Epoch: 50. Loss: 0.06907365471124649\n",
      "50 train 10 132\n",
      "Phase: train. Epoch: 50. Loss: 0.10266932100057602\n",
      "50 train 11 144\n",
      "Phase: train. Epoch: 50. Loss: 0.07211228460073471\n",
      "50 train 12 156\n",
      "Phase: train. Epoch: 50. Loss: 0.060869961977005005\n",
      "50 train 13 168\n",
      "Phase: train. Epoch: 50. Loss: 0.08654531091451645\n",
      "50 train 14 180\n",
      "Phase: train. Epoch: 50. Loss: 0.07349801808595657\n",
      "50 train 15 192\n",
      "Phase: train. Epoch: 50. Loss: 0.061894215643405914\n",
      "50 train 16 204\n",
      "Phase: train. Epoch: 50. Loss: 0.08092746883630753\n",
      "50 train 17 216\n",
      "Phase: train. Epoch: 50. Loss: 0.08658697456121445\n",
      "50 train 18 228\n",
      "Phase: train. Epoch: 50. Loss: 0.08872666954994202\n",
      "50 train 19 240\n",
      "Phase: train. Epoch: 50. Loss: 0.0754421129822731\n",
      "50 train 20 252\n",
      "Phase: train. Epoch: 50. Loss: 0.0806606262922287\n",
      "50 train 21 264\n",
      "Phase: train. Epoch: 50. Loss: 0.07260417938232422\n",
      "50 train 22 276\n",
      "Phase: train. Epoch: 50. Loss: 0.1353539526462555\n",
      "50 train 23 288\n",
      "Phase: train. Epoch: 50. Loss: 0.06750299036502838\n",
      "50 train 24 300\n",
      "Phase: train. Epoch: 50. Loss: 0.0826030820608139\n",
      "50 train 25 312\n",
      "Phase: train. Epoch: 50. Loss: 0.07494646310806274\n",
      "50 train 26 324\n",
      "Phase: train. Epoch: 50. Loss: 0.06613235175609589\n",
      "50 train 27 336\n",
      "Phase: train. Epoch: 50. Loss: 0.08614262193441391\n",
      "50 train 28 348\n",
      "Phase: train. Epoch: 50. Loss: 0.07671789824962616\n",
      "50 train 29 360\n",
      "Phase: train. Epoch: 50. Loss: 0.12136179208755493\n",
      "50 train 30 372\n",
      "Phase: train. Epoch: 50. Loss: 0.08543796837329865\n",
      "50 train 31 384\n",
      "Phase: train. Epoch: 50. Loss: 0.08090616762638092\n",
      "50 train 32 396\n",
      "Phase: train. Epoch: 50. Loss: 0.10093285143375397\n",
      "50 train 33 408\n",
      "Phase: train. Epoch: 50. Loss: 0.11601553857326508\n",
      "50 train 34 420\n",
      "Phase: train. Epoch: 50. Loss: 0.1056833565235138\n",
      "50 train 35 432\n",
      "Phase: train. Epoch: 50. Loss: 0.0981854796409607\n",
      "50 train 36 444\n",
      "Phase: train. Epoch: 50. Loss: 0.09314175695180893\n",
      "50 train 37 456\n",
      "Phase: train. Epoch: 50. Loss: 0.07531687617301941\n",
      "50 train 38 468\n",
      "Phase: train. Epoch: 50. Loss: 0.0947064608335495\n",
      "50 train 39 480\n",
      "Phase: train. Epoch: 50. Loss: 0.08682432025671005\n",
      "50 train 40 492\n",
      "Phase: train. Epoch: 50. Loss: 0.10111695528030396\n",
      "50 train 41 504\n",
      "Phase: train. Epoch: 50. Loss: 0.078907810151577\n",
      "50 train 42 516\n",
      "Phase: train. Epoch: 50. Loss: 0.08546918630599976\n",
      "50 train 43 528\n",
      "Phase: train. Epoch: 50. Loss: 0.08785694092512131\n",
      "50 train 44 540\n",
      "Phase: train. Epoch: 50. Loss: 0.0802069902420044\n",
      "50 train 45 552\n",
      "Phase: train. Epoch: 50. Loss: 0.11076923459768295\n",
      "50 train 46 564\n",
      "Phase: train. Epoch: 50. Loss: 0.08738549798727036\n",
      "50 train 47 576\n",
      "Phase: train. Epoch: 50. Loss: 0.10338757932186127\n",
      "50 train 48 588\n",
      "Phase: train. Epoch: 50. Loss: 0.0660204142332077\n",
      "50 train 49 600\n",
      "Phase: train. Epoch: 50. Loss: 0.09600089490413666\n",
      "50 train 50 612\n",
      "Phase: train. Epoch: 50. Loss: 0.085199736058712\n",
      "50 train 51 624\n",
      "Phase: train. Epoch: 50. Loss: 0.06311455368995667\n",
      "50 train 52 636\n",
      "Phase: train. Epoch: 50. Loss: 0.1407773345708847\n",
      "50 train 53 648\n",
      "Phase: train. Epoch: 50. Loss: 0.12781202793121338\n",
      "50 train 54 660\n",
      "Phase: train. Epoch: 50. Loss: 0.08114547282457352\n",
      "50 train 55 672\n",
      "Phase: train. Epoch: 50. Loss: 0.11719456315040588\n",
      "50 train 56 684\n",
      "Phase: train. Epoch: 50. Loss: 0.08676420152187347\n",
      "50 train 57 696\n",
      "Phase: train. Epoch: 50. Loss: 0.09303712099790573\n",
      "50 train 58 708\n",
      "Phase: train. Epoch: 50. Loss: 0.09324613213539124\n",
      "50 train 59 720\n",
      "Phase: train. Epoch: 50. Loss: 0.08580624312162399\n",
      "50 train 60 732\n",
      "Phase: train. Epoch: 50. Loss: 0.09631761163473129\n",
      "50 train 61 744\n",
      "Phase: train. Epoch: 50. Loss: 0.08731099218130112\n",
      "50 train 62 751\n",
      "Phase: train. Epoch: 50. Loss: 0.11606284230947495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 val 0 763\n",
      "Phase: val. Epoch: 50. Loss: 0.04568188637495041\n",
      "50 val 1 775\n",
      "Phase: val. Epoch: 50. Loss: 0.08436455577611923\n",
      "50 val 2 787\n",
      "Phase: val. Epoch: 50. Loss: 0.14857959747314453\n",
      "50 val 3 799\n",
      "Phase: val. Epoch: 50. Loss: 0.1165771335363388\n",
      "50 val 4 811\n",
      "Phase: val. Epoch: 50. Loss: 0.08083592355251312\n",
      "50 val 5 823\n",
      "Phase: val. Epoch: 50. Loss: 0.1143195852637291\n",
      "50 val 6 835\n",
      "Phase: val. Epoch: 50. Loss: 0.06866464018821716\n",
      "50 val 7 847\n",
      "Phase: val. Epoch: 50. Loss: 0.09159144014120102\n",
      "50 val 8 859\n",
      "Phase: val. Epoch: 50. Loss: 0.11558528989553452\n",
      "50 val 9 871\n",
      "Phase: val. Epoch: 50. Loss: 0.07502563297748566\n",
      "50 val 10 883\n",
      "Phase: val. Epoch: 50. Loss: 0.07731110602617264\n",
      "50 val 11 884\n",
      "Phase: val. Epoch: 50. Loss: 0.027351317927241325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 train 0 12\n",
      "Phase: train. Epoch: 51. Loss: 0.07294666767120361\n",
      "51 train 1 24\n",
      "Phase: train. Epoch: 51. Loss: 0.08773009479045868\n",
      "51 train 2 36\n",
      "Phase: train. Epoch: 51. Loss: 0.07324995845556259\n",
      "51 train 3 48\n",
      "Phase: train. Epoch: 51. Loss: 0.0882309302687645\n",
      "51 train 4 60\n",
      "Phase: train. Epoch: 51. Loss: 0.07442730665206909\n",
      "51 train 5 72\n",
      "Phase: train. Epoch: 51. Loss: 0.08424891531467438\n",
      "51 train 6 84\n",
      "Phase: train. Epoch: 51. Loss: 0.07294722646474838\n",
      "51 train 7 96\n",
      "Phase: train. Epoch: 51. Loss: 0.08175244927406311\n",
      "51 train 8 108\n",
      "Phase: train. Epoch: 51. Loss: 0.07007656246423721\n",
      "51 train 9 120\n",
      "Phase: train. Epoch: 51. Loss: 0.0878385528922081\n",
      "51 train 10 132\n",
      "Phase: train. Epoch: 51. Loss: 0.0647772029042244\n",
      "51 train 11 144\n",
      "Phase: train. Epoch: 51. Loss: 0.0800303965806961\n",
      "51 train 12 156\n",
      "Phase: train. Epoch: 51. Loss: 0.11673995107412338\n",
      "51 train 13 168\n",
      "Phase: train. Epoch: 51. Loss: 0.07289581745862961\n",
      "51 train 14 180\n",
      "Phase: train. Epoch: 51. Loss: 0.07283152639865875\n",
      "51 train 15 192\n",
      "Phase: train. Epoch: 51. Loss: 0.08156829327344894\n",
      "51 train 16 204\n",
      "Phase: train. Epoch: 51. Loss: 0.09641753137111664\n",
      "51 train 17 216\n",
      "Phase: train. Epoch: 51. Loss: 0.08546625822782516\n",
      "51 train 18 228\n",
      "Phase: train. Epoch: 51. Loss: 0.06589850783348083\n",
      "51 train 19 240\n",
      "Phase: train. Epoch: 51. Loss: 0.11523320525884628\n",
      "51 train 20 252\n",
      "Phase: train. Epoch: 51. Loss: 0.09124916046857834\n",
      "51 train 21 264\n",
      "Phase: train. Epoch: 51. Loss: 0.08411948382854462\n",
      "51 train 22 276\n",
      "Phase: train. Epoch: 51. Loss: 0.08055607229471207\n",
      "51 train 23 288\n",
      "Phase: train. Epoch: 51. Loss: 0.09059866517782211\n",
      "51 train 24 300\n",
      "Phase: train. Epoch: 51. Loss: 0.09951125830411911\n",
      "51 train 25 312\n",
      "Phase: train. Epoch: 51. Loss: 0.09293927252292633\n",
      "51 train 26 324\n",
      "Phase: train. Epoch: 51. Loss: 0.09160695970058441\n",
      "51 train 27 336\n",
      "Phase: train. Epoch: 51. Loss: 0.09603123366832733\n",
      "51 train 28 348\n",
      "Phase: train. Epoch: 51. Loss: 0.08145233243703842\n",
      "51 train 29 360\n",
      "Phase: train. Epoch: 51. Loss: 0.09956690669059753\n",
      "51 train 30 372\n",
      "Phase: train. Epoch: 51. Loss: 0.08743631094694138\n",
      "51 train 31 384\n",
      "Phase: train. Epoch: 51. Loss: 0.08214645087718964\n",
      "51 train 32 396\n",
      "Phase: train. Epoch: 51. Loss: 0.09179161489009857\n",
      "51 train 33 408\n",
      "Phase: train. Epoch: 51. Loss: 0.09712503850460052\n",
      "51 train 34 420\n",
      "Phase: train. Epoch: 51. Loss: 0.08387073129415512\n",
      "51 train 35 432\n",
      "Phase: train. Epoch: 51. Loss: 0.07183726131916046\n",
      "51 train 36 444\n",
      "Phase: train. Epoch: 51. Loss: 0.08463543653488159\n",
      "51 train 37 456\n",
      "Phase: train. Epoch: 51. Loss: 0.08794640004634857\n",
      "51 train 38 468\n",
      "Phase: train. Epoch: 51. Loss: 0.07979507744312286\n",
      "51 train 39 480\n",
      "Phase: train. Epoch: 51. Loss: 0.07613226771354675\n",
      "51 train 40 492\n",
      "Phase: train. Epoch: 51. Loss: 0.08921387791633606\n",
      "51 train 41 504\n",
      "Phase: train. Epoch: 51. Loss: 0.09121493995189667\n",
      "51 train 42 516\n",
      "Phase: train. Epoch: 51. Loss: 0.08608198165893555\n",
      "51 train 43 528\n",
      "Phase: train. Epoch: 51. Loss: 0.07716720551252365\n",
      "51 train 44 540\n",
      "Phase: train. Epoch: 51. Loss: 0.09743642807006836\n",
      "51 train 45 552\n",
      "Phase: train. Epoch: 51. Loss: 0.09542196989059448\n",
      "51 train 46 564\n",
      "Phase: train. Epoch: 51. Loss: 0.0772673487663269\n",
      "51 train 47 576\n",
      "Phase: train. Epoch: 51. Loss: 0.07333783805370331\n",
      "51 train 48 588\n",
      "Phase: train. Epoch: 51. Loss: 0.0904228687286377\n",
      "51 train 49 600\n",
      "Phase: train. Epoch: 51. Loss: 0.07686706632375717\n",
      "51 train 50 612\n",
      "Phase: train. Epoch: 51. Loss: 0.08027040213346481\n",
      "51 train 51 624\n",
      "Phase: train. Epoch: 51. Loss: 0.09949257224798203\n",
      "51 train 52 636\n",
      "Phase: train. Epoch: 51. Loss: 0.08076873421669006\n",
      "51 train 53 648\n",
      "Phase: train. Epoch: 51. Loss: 0.11274179071187973\n",
      "51 train 54 660\n",
      "Phase: train. Epoch: 51. Loss: 0.09150801599025726\n",
      "51 train 55 672\n",
      "Phase: train. Epoch: 51. Loss: 0.10609818994998932\n",
      "51 train 56 684\n",
      "Phase: train. Epoch: 51. Loss: 0.09959160536527634\n",
      "51 train 57 696\n",
      "Phase: train. Epoch: 51. Loss: 0.07668837159872055\n",
      "51 train 58 708\n",
      "Phase: train. Epoch: 51. Loss: 0.06635065376758575\n",
      "51 train 59 720\n",
      "Phase: train. Epoch: 51. Loss: 0.07599715888500214\n",
      "51 train 60 732\n",
      "Phase: train. Epoch: 51. Loss: 0.07698681205511093\n",
      "51 train 61 744\n",
      "Phase: train. Epoch: 51. Loss: 0.07902165502309799\n",
      "51 train 62 751\n",
      "Phase: train. Epoch: 51. Loss: 0.11314183473587036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 val 0 763\n",
      "Phase: val. Epoch: 51. Loss: 0.09223516285419464\n",
      "51 val 1 775\n",
      "Phase: val. Epoch: 51. Loss: 0.08979126065969467\n",
      "51 val 2 787\n",
      "Phase: val. Epoch: 51. Loss: 0.11440485715866089\n",
      "51 val 3 799\n",
      "Phase: val. Epoch: 51. Loss: 0.11112749576568604\n",
      "51 val 4 811\n",
      "Phase: val. Epoch: 51. Loss: 0.05957300588488579\n",
      "51 val 5 823\n",
      "Phase: val. Epoch: 51. Loss: 0.07046462595462799\n",
      "51 val 6 835\n",
      "Phase: val. Epoch: 51. Loss: 0.09409503638744354\n",
      "51 val 7 847\n",
      "Phase: val. Epoch: 51. Loss: 0.07322154939174652\n",
      "51 val 8 859\n",
      "Phase: val. Epoch: 51. Loss: 0.10169658064842224\n",
      "51 val 9 871\n",
      "Phase: val. Epoch: 51. Loss: 0.06364493817090988\n",
      "51 val 10 883\n",
      "Phase: val. Epoch: 51. Loss: 0.09312465786933899\n",
      "51 val 11 884\n",
      "Phase: val. Epoch: 51. Loss: 0.03574629873037338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 train 0 12\n",
      "Phase: train. Epoch: 52. Loss: 0.07788709551095963\n",
      "52 train 1 24\n",
      "Phase: train. Epoch: 52. Loss: 0.06998097151517868\n",
      "52 train 2 36\n",
      "Phase: train. Epoch: 52. Loss: 0.1073264479637146\n",
      "52 train 3 48\n",
      "Phase: train. Epoch: 52. Loss: 0.08581777662038803\n",
      "52 train 4 60\n",
      "Phase: train. Epoch: 52. Loss: 0.0815097764134407\n",
      "52 train 5 72\n",
      "Phase: train. Epoch: 52. Loss: 0.07731498777866364\n",
      "52 train 6 84\n",
      "Phase: train. Epoch: 52. Loss: 0.08812912553548813\n",
      "52 train 7 96\n",
      "Phase: train. Epoch: 52. Loss: 0.06639479845762253\n",
      "52 train 8 108\n",
      "Phase: train. Epoch: 52. Loss: 0.07186239212751389\n",
      "52 train 9 120\n",
      "Phase: train. Epoch: 52. Loss: 0.08839671313762665\n",
      "52 train 10 132\n",
      "Phase: train. Epoch: 52. Loss: 0.08261501789093018\n",
      "52 train 11 144\n",
      "Phase: train. Epoch: 52. Loss: 0.07587215304374695\n",
      "52 train 12 156\n",
      "Phase: train. Epoch: 52. Loss: 0.10925009101629257\n",
      "52 train 13 168\n",
      "Phase: train. Epoch: 52. Loss: 0.07051349431276321\n",
      "52 train 14 180\n",
      "Phase: train. Epoch: 52. Loss: 0.11843013763427734\n",
      "52 train 15 192\n",
      "Phase: train. Epoch: 52. Loss: 0.0604882538318634\n",
      "52 train 16 204\n",
      "Phase: train. Epoch: 52. Loss: 0.06649909168481827\n",
      "52 train 17 216\n",
      "Phase: train. Epoch: 52. Loss: 0.10091455280780792\n",
      "52 train 18 228\n",
      "Phase: train. Epoch: 52. Loss: 0.10384047031402588\n",
      "52 train 19 240\n",
      "Phase: train. Epoch: 52. Loss: 0.09346483647823334\n",
      "52 train 20 252\n",
      "Phase: train. Epoch: 52. Loss: 0.113420270383358\n",
      "52 train 21 264\n",
      "Phase: train. Epoch: 52. Loss: 0.09770958125591278\n",
      "52 train 22 276\n",
      "Phase: train. Epoch: 52. Loss: 0.08539122343063354\n",
      "52 train 23 288\n",
      "Phase: train. Epoch: 52. Loss: 0.09011034667491913\n",
      "52 train 24 300\n",
      "Phase: train. Epoch: 52. Loss: 0.0883503183722496\n",
      "52 train 25 312\n",
      "Phase: train. Epoch: 52. Loss: 0.09938117116689682\n",
      "52 train 26 324\n",
      "Phase: train. Epoch: 52. Loss: 0.07827837020158768\n",
      "52 train 27 336\n",
      "Phase: train. Epoch: 52. Loss: 0.07279860973358154\n",
      "52 train 28 348\n",
      "Phase: train. Epoch: 52. Loss: 0.07482092827558517\n",
      "52 train 29 360\n",
      "Phase: train. Epoch: 52. Loss: 0.09844982624053955\n",
      "52 train 30 372\n",
      "Phase: train. Epoch: 52. Loss: 0.08164907991886139\n",
      "52 train 31 384\n",
      "Phase: train. Epoch: 52. Loss: 0.07174976170063019\n",
      "52 train 32 396\n",
      "Phase: train. Epoch: 52. Loss: 0.07266408205032349\n",
      "52 train 33 408\n",
      "Phase: train. Epoch: 52. Loss: 0.10581666231155396\n",
      "52 train 34 420\n",
      "Phase: train. Epoch: 52. Loss: 0.09903590381145477\n",
      "52 train 35 432\n",
      "Phase: train. Epoch: 52. Loss: 0.07908178865909576\n",
      "52 train 36 444\n",
      "Phase: train. Epoch: 52. Loss: 0.09185437858104706\n",
      "52 train 37 456\n",
      "Phase: train. Epoch: 52. Loss: 0.06934694945812225\n",
      "52 train 38 468\n",
      "Phase: train. Epoch: 52. Loss: 0.08823107182979584\n",
      "52 train 39 480\n",
      "Phase: train. Epoch: 52. Loss: 0.11763697117567062\n",
      "52 train 40 492\n",
      "Phase: train. Epoch: 52. Loss: 0.08388584107160568\n",
      "52 train 41 504\n",
      "Phase: train. Epoch: 52. Loss: 0.0986185297369957\n",
      "52 train 42 516\n",
      "Phase: train. Epoch: 52. Loss: 0.09499045461416245\n",
      "52 train 43 528\n",
      "Phase: train. Epoch: 52. Loss: 0.07174214720726013\n",
      "52 train 44 540\n",
      "Phase: train. Epoch: 52. Loss: 0.08045904338359833\n",
      "52 train 45 552\n",
      "Phase: train. Epoch: 52. Loss: 0.08192428946495056\n",
      "52 train 46 564\n",
      "Phase: train. Epoch: 52. Loss: 0.06719768047332764\n",
      "52 train 47 576\n",
      "Phase: train. Epoch: 52. Loss: 0.1038394570350647\n",
      "52 train 48 588\n",
      "Phase: train. Epoch: 52. Loss: 0.1046319231390953\n",
      "52 train 49 600\n",
      "Phase: train. Epoch: 52. Loss: 0.07453420758247375\n",
      "52 train 50 612\n",
      "Phase: train. Epoch: 52. Loss: 0.13301990926265717\n",
      "52 train 51 624\n",
      "Phase: train. Epoch: 52. Loss: 0.08997749537229538\n",
      "52 train 52 636\n",
      "Phase: train. Epoch: 52. Loss: 0.09178374707698822\n",
      "52 train 53 648\n",
      "Phase: train. Epoch: 52. Loss: 0.08947201818227768\n",
      "52 train 54 660\n",
      "Phase: train. Epoch: 52. Loss: 0.08141899108886719\n",
      "52 train 55 672\n",
      "Phase: train. Epoch: 52. Loss: 0.11893659830093384\n",
      "52 train 56 684\n",
      "Phase: train. Epoch: 52. Loss: 0.06678690016269684\n",
      "52 train 57 696\n",
      "Phase: train. Epoch: 52. Loss: 0.10525652021169662\n",
      "52 train 58 708\n",
      "Phase: train. Epoch: 52. Loss: 0.08724799007177353\n",
      "52 train 59 720\n",
      "Phase: train. Epoch: 52. Loss: 0.09829922765493393\n",
      "52 train 60 732\n",
      "Phase: train. Epoch: 52. Loss: 0.07998882234096527\n",
      "52 train 61 744\n",
      "Phase: train. Epoch: 52. Loss: 0.08408173173666\n",
      "52 train 62 751\n",
      "Phase: train. Epoch: 52. Loss: 0.06029856950044632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 val 0 763\n",
      "Phase: val. Epoch: 52. Loss: 0.07758054882287979\n",
      "52 val 1 775\n",
      "Phase: val. Epoch: 52. Loss: 0.06252830475568771\n",
      "52 val 2 787\n",
      "Phase: val. Epoch: 52. Loss: 0.10491963475942612\n",
      "52 val 3 799\n",
      "Phase: val. Epoch: 52. Loss: 0.07287336885929108\n",
      "52 val 4 811\n",
      "Phase: val. Epoch: 52. Loss: 0.07080382108688354\n",
      "52 val 5 823\n",
      "Phase: val. Epoch: 52. Loss: 0.07929050922393799\n",
      "52 val 6 835\n",
      "Phase: val. Epoch: 52. Loss: 0.08453598618507385\n",
      "52 val 7 847\n",
      "Phase: val. Epoch: 52. Loss: 0.06232963502407074\n",
      "52 val 8 859\n",
      "Phase: val. Epoch: 52. Loss: 0.07634071260690689\n",
      "52 val 9 871\n",
      "Phase: val. Epoch: 52. Loss: 0.11654280871152878\n",
      "52 val 10 883\n",
      "Phase: val. Epoch: 52. Loss: 0.0964801162481308\n",
      "52 val 11 884\n",
      "Phase: val. Epoch: 52. Loss: 0.04053007811307907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 train 0 12\n",
      "Phase: train. Epoch: 53. Loss: 0.06676168739795685\n",
      "53 train 1 24\n",
      "Phase: train. Epoch: 53. Loss: 0.10153575986623764\n",
      "53 train 2 36\n",
      "Phase: train. Epoch: 53. Loss: 0.10474242269992828\n",
      "53 train 3 48\n",
      "Phase: train. Epoch: 53. Loss: 0.11344049870967865\n",
      "53 train 4 60\n",
      "Phase: train. Epoch: 53. Loss: 0.06750845164060593\n",
      "53 train 5 72\n",
      "Phase: train. Epoch: 53. Loss: 0.09866678714752197\n",
      "53 train 6 84\n",
      "Phase: train. Epoch: 53. Loss: 0.0757703185081482\n",
      "53 train 7 96\n",
      "Phase: train. Epoch: 53. Loss: 0.09316061437129974\n",
      "53 train 8 108\n",
      "Phase: train. Epoch: 53. Loss: 0.08848309516906738\n",
      "53 train 9 120\n",
      "Phase: train. Epoch: 53. Loss: 0.09049131721258163\n",
      "53 train 10 132\n",
      "Phase: train. Epoch: 53. Loss: 0.07290568202733994\n",
      "53 train 11 144\n",
      "Phase: train. Epoch: 53. Loss: 0.09005619585514069\n",
      "53 train 12 156\n",
      "Phase: train. Epoch: 53. Loss: 0.061659108847379684\n",
      "53 train 13 168\n",
      "Phase: train. Epoch: 53. Loss: 0.11329513788223267\n",
      "53 train 14 180\n",
      "Phase: train. Epoch: 53. Loss: 0.0824379175901413\n",
      "53 train 15 192\n",
      "Phase: train. Epoch: 53. Loss: 0.09539811313152313\n",
      "53 train 16 204\n",
      "Phase: train. Epoch: 53. Loss: 0.07775046676397324\n",
      "53 train 17 216\n",
      "Phase: train. Epoch: 53. Loss: 0.0760987177491188\n",
      "53 train 18 228\n",
      "Phase: train. Epoch: 53. Loss: 0.08710216730833054\n",
      "53 train 19 240\n",
      "Phase: train. Epoch: 53. Loss: 0.11325468868017197\n",
      "53 train 20 252\n",
      "Phase: train. Epoch: 53. Loss: 0.062140271067619324\n",
      "53 train 21 264\n",
      "Phase: train. Epoch: 53. Loss: 0.07860587537288666\n",
      "53 train 22 276\n",
      "Phase: train. Epoch: 53. Loss: 0.08834882825613022\n",
      "53 train 23 288\n",
      "Phase: train. Epoch: 53. Loss: 0.07087516784667969\n",
      "53 train 24 300\n",
      "Phase: train. Epoch: 53. Loss: 0.08935080468654633\n",
      "53 train 25 312\n",
      "Phase: train. Epoch: 53. Loss: 0.09279658645391464\n",
      "53 train 26 324\n",
      "Phase: train. Epoch: 53. Loss: 0.07434679567813873\n",
      "53 train 27 336\n",
      "Phase: train. Epoch: 53. Loss: 0.07465555518865585\n",
      "53 train 28 348\n",
      "Phase: train. Epoch: 53. Loss: 0.0950230211019516\n",
      "53 train 29 360\n",
      "Phase: train. Epoch: 53. Loss: 0.07360436022281647\n",
      "53 train 30 372\n",
      "Phase: train. Epoch: 53. Loss: 0.08215031027793884\n",
      "53 train 31 384\n",
      "Phase: train. Epoch: 53. Loss: 0.11815142631530762\n",
      "53 train 32 396\n",
      "Phase: train. Epoch: 53. Loss: 0.07689172774553299\n",
      "53 train 33 408\n",
      "Phase: train. Epoch: 53. Loss: 0.07370801270008087\n",
      "53 train 34 420\n",
      "Phase: train. Epoch: 53. Loss: 0.08587970584630966\n",
      "53 train 35 432\n",
      "Phase: train. Epoch: 53. Loss: 0.06552591174840927\n",
      "53 train 36 444\n",
      "Phase: train. Epoch: 53. Loss: 0.07104894518852234\n",
      "53 train 37 456\n",
      "Phase: train. Epoch: 53. Loss: 0.0891251266002655\n",
      "53 train 38 468\n",
      "Phase: train. Epoch: 53. Loss: 0.0890468955039978\n",
      "53 train 39 480\n",
      "Phase: train. Epoch: 53. Loss: 0.10374578088521957\n",
      "53 train 40 492\n",
      "Phase: train. Epoch: 53. Loss: 0.07304365932941437\n",
      "53 train 41 504\n",
      "Phase: train. Epoch: 53. Loss: 0.11300778388977051\n",
      "53 train 42 516\n",
      "Phase: train. Epoch: 53. Loss: 0.07866695523262024\n",
      "53 train 43 528\n",
      "Phase: train. Epoch: 53. Loss: 0.08171091973781586\n",
      "53 train 44 540\n",
      "Phase: train. Epoch: 53. Loss: 0.062473148107528687\n",
      "53 train 45 552\n",
      "Phase: train. Epoch: 53. Loss: 0.101661317050457\n",
      "53 train 46 564\n",
      "Phase: train. Epoch: 53. Loss: 0.07858620584011078\n",
      "53 train 47 576\n",
      "Phase: train. Epoch: 53. Loss: 0.08711795508861542\n",
      "53 train 48 588\n",
      "Phase: train. Epoch: 53. Loss: 0.08449862897396088\n",
      "53 train 49 600\n",
      "Phase: train. Epoch: 53. Loss: 0.10313569754362106\n",
      "53 train 50 612\n",
      "Phase: train. Epoch: 53. Loss: 0.07509832084178925\n",
      "53 train 51 624\n",
      "Phase: train. Epoch: 53. Loss: 0.10414496064186096\n",
      "53 train 52 636\n",
      "Phase: train. Epoch: 53. Loss: 0.12422746419906616\n",
      "53 train 53 648\n",
      "Phase: train. Epoch: 53. Loss: 0.09771756827831268\n",
      "53 train 54 660\n",
      "Phase: train. Epoch: 53. Loss: 0.07826223224401474\n",
      "53 train 55 672\n",
      "Phase: train. Epoch: 53. Loss: 0.11364626884460449\n",
      "53 train 56 684\n",
      "Phase: train. Epoch: 53. Loss: 0.0822913646697998\n",
      "53 train 57 696\n",
      "Phase: train. Epoch: 53. Loss: 0.10963982343673706\n",
      "53 train 58 708\n",
      "Phase: train. Epoch: 53. Loss: 0.10950304567813873\n",
      "53 train 59 720\n",
      "Phase: train. Epoch: 53. Loss: 0.08926016092300415\n",
      "53 train 60 732\n",
      "Phase: train. Epoch: 53. Loss: 0.06495247781276703\n",
      "53 train 61 744\n",
      "Phase: train. Epoch: 53. Loss: 0.06562839448451996\n",
      "53 train 62 751\n",
      "Phase: train. Epoch: 53. Loss: 0.08646711707115173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 val 0 763\n",
      "Phase: val. Epoch: 53. Loss: 0.09566335380077362\n",
      "53 val 1 775\n",
      "Phase: val. Epoch: 53. Loss: 0.07948017120361328\n",
      "53 val 2 787\n",
      "Phase: val. Epoch: 53. Loss: 0.09783940017223358\n",
      "53 val 3 799\n",
      "Phase: val. Epoch: 53. Loss: 0.09401260316371918\n",
      "53 val 4 811\n",
      "Phase: val. Epoch: 53. Loss: 0.12486472725868225\n",
      "53 val 5 823\n",
      "Phase: val. Epoch: 53. Loss: 0.09391853213310242\n",
      "53 val 6 835\n",
      "Phase: val. Epoch: 53. Loss: 0.06315549463033676\n",
      "53 val 7 847\n",
      "Phase: val. Epoch: 53. Loss: 0.10481268167495728\n",
      "53 val 8 859\n",
      "Phase: val. Epoch: 53. Loss: 0.07109495997428894\n",
      "53 val 9 871\n",
      "Phase: val. Epoch: 53. Loss: 0.06588852405548096\n",
      "53 val 10 883\n",
      "Phase: val. Epoch: 53. Loss: 0.09264068305492401\n",
      "53 val 11 884\n",
      "Phase: val. Epoch: 53. Loss: 0.026889724656939507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 train 0 12\n",
      "Phase: train. Epoch: 54. Loss: 0.09528934955596924\n",
      "54 train 1 24\n",
      "Phase: train. Epoch: 54. Loss: 0.07362209260463715\n",
      "54 train 2 36\n",
      "Phase: train. Epoch: 54. Loss: 0.07954911887645721\n",
      "54 train 3 48\n",
      "Phase: train. Epoch: 54. Loss: 0.09747065603733063\n",
      "54 train 4 60\n",
      "Phase: train. Epoch: 54. Loss: 0.06827107071876526\n",
      "54 train 5 72\n",
      "Phase: train. Epoch: 54. Loss: 0.07303617894649506\n",
      "54 train 6 84\n",
      "Phase: train. Epoch: 54. Loss: 0.07029660791158676\n",
      "54 train 7 96\n",
      "Phase: train. Epoch: 54. Loss: 0.09738116711378098\n",
      "54 train 8 108\n",
      "Phase: train. Epoch: 54. Loss: 0.10636329650878906\n",
      "54 train 9 120\n",
      "Phase: train. Epoch: 54. Loss: 0.09373672306537628\n",
      "54 train 10 132\n",
      "Phase: train. Epoch: 54. Loss: 0.15135136246681213\n",
      "54 train 11 144\n",
      "Phase: train. Epoch: 54. Loss: 0.08581608533859253\n",
      "54 train 12 156\n",
      "Phase: train. Epoch: 54. Loss: 0.09594500064849854\n",
      "54 train 13 168\n",
      "Phase: train. Epoch: 54. Loss: 0.09955841302871704\n",
      "54 train 14 180\n",
      "Phase: train. Epoch: 54. Loss: 0.08404000848531723\n",
      "54 train 15 192\n",
      "Phase: train. Epoch: 54. Loss: 0.07114388793706894\n",
      "54 train 16 204\n",
      "Phase: train. Epoch: 54. Loss: 0.08098005503416061\n",
      "54 train 17 216\n",
      "Phase: train. Epoch: 54. Loss: 0.07339915633201599\n",
      "54 train 18 228\n",
      "Phase: train. Epoch: 54. Loss: 0.10562057793140411\n",
      "54 train 19 240\n",
      "Phase: train. Epoch: 54. Loss: 0.09794805943965912\n",
      "54 train 20 252\n",
      "Phase: train. Epoch: 54. Loss: 0.0663568377494812\n",
      "54 train 21 264\n",
      "Phase: train. Epoch: 54. Loss: 0.09781857579946518\n",
      "54 train 22 276\n",
      "Phase: train. Epoch: 54. Loss: 0.08775999397039413\n",
      "54 train 23 288\n",
      "Phase: train. Epoch: 54. Loss: 0.10856302827596664\n",
      "54 train 24 300\n",
      "Phase: train. Epoch: 54. Loss: 0.09095573425292969\n",
      "54 train 25 312\n",
      "Phase: train. Epoch: 54. Loss: 0.09433930367231369\n",
      "54 train 26 324\n",
      "Phase: train. Epoch: 54. Loss: 0.12379314005374908\n",
      "54 train 27 336\n",
      "Phase: train. Epoch: 54. Loss: 0.06764385104179382\n",
      "54 train 28 348\n",
      "Phase: train. Epoch: 54. Loss: 0.1123141273856163\n",
      "54 train 29 360\n",
      "Phase: train. Epoch: 54. Loss: 0.08243487030267715\n",
      "54 train 30 372\n",
      "Phase: train. Epoch: 54. Loss: 0.06609687209129333\n",
      "54 train 31 384\n",
      "Phase: train. Epoch: 54. Loss: 0.09077801555395126\n",
      "54 train 32 396\n",
      "Phase: train. Epoch: 54. Loss: 0.07847391813993454\n",
      "54 train 33 408\n",
      "Phase: train. Epoch: 54. Loss: 0.08902432769536972\n",
      "54 train 34 420\n",
      "Phase: train. Epoch: 54. Loss: 0.07504405826330185\n",
      "54 train 35 432\n",
      "Phase: train. Epoch: 54. Loss: 0.1089072972536087\n",
      "54 train 36 444\n",
      "Phase: train. Epoch: 54. Loss: 0.07074517011642456\n",
      "54 train 37 456\n",
      "Phase: train. Epoch: 54. Loss: 0.08324841409921646\n",
      "54 train 38 468\n",
      "Phase: train. Epoch: 54. Loss: 0.10851086676120758\n",
      "54 train 39 480\n",
      "Phase: train. Epoch: 54. Loss: 0.07641258835792542\n",
      "54 train 40 492\n",
      "Phase: train. Epoch: 54. Loss: 0.0952424630522728\n",
      "54 train 41 504\n",
      "Phase: train. Epoch: 54. Loss: 0.0768810287117958\n",
      "54 train 42 516\n",
      "Phase: train. Epoch: 54. Loss: 0.07589519023895264\n",
      "54 train 43 528\n",
      "Phase: train. Epoch: 54. Loss: 0.10473450273275375\n",
      "54 train 44 540\n",
      "Phase: train. Epoch: 54. Loss: 0.11353324353694916\n",
      "54 train 45 552\n",
      "Phase: train. Epoch: 54. Loss: 0.0907486081123352\n",
      "54 train 46 564\n",
      "Phase: train. Epoch: 54. Loss: 0.08946283906698227\n",
      "54 train 47 576\n",
      "Phase: train. Epoch: 54. Loss: 0.07931520789861679\n",
      "54 train 48 588\n",
      "Phase: train. Epoch: 54. Loss: 0.09802921861410141\n",
      "54 train 49 600\n",
      "Phase: train. Epoch: 54. Loss: 0.07340100407600403\n",
      "54 train 50 612\n",
      "Phase: train. Epoch: 54. Loss: 0.10158457607030869\n",
      "54 train 51 624\n",
      "Phase: train. Epoch: 54. Loss: 0.07370597124099731\n",
      "54 train 52 636\n",
      "Phase: train. Epoch: 54. Loss: 0.08882560580968857\n",
      "54 train 53 648\n",
      "Phase: train. Epoch: 54. Loss: 0.0654420554637909\n",
      "54 train 54 660\n",
      "Phase: train. Epoch: 54. Loss: 0.06550342589616776\n",
      "54 train 55 672\n",
      "Phase: train. Epoch: 54. Loss: 0.08760099112987518\n",
      "54 train 56 684\n",
      "Phase: train. Epoch: 54. Loss: 0.08990643918514252\n",
      "54 train 57 696\n",
      "Phase: train. Epoch: 54. Loss: 0.08208230137825012\n",
      "54 train 58 708\n",
      "Phase: train. Epoch: 54. Loss: 0.09116730093955994\n",
      "54 train 59 720\n",
      "Phase: train. Epoch: 54. Loss: 0.0666658878326416\n",
      "54 train 60 732\n",
      "Phase: train. Epoch: 54. Loss: 0.06281325966119766\n",
      "54 train 61 744\n",
      "Phase: train. Epoch: 54. Loss: 0.07921361923217773\n",
      "54 train 62 751\n",
      "Phase: train. Epoch: 54. Loss: 0.10312403738498688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 val 0 763\n",
      "Phase: val. Epoch: 54. Loss: 0.09179311245679855\n",
      "54 val 1 775\n",
      "Phase: val. Epoch: 54. Loss: 0.05248021334409714\n",
      "54 val 2 787\n",
      "Phase: val. Epoch: 54. Loss: 0.09111340343952179\n",
      "54 val 3 799\n",
      "Phase: val. Epoch: 54. Loss: 0.08759503811597824\n",
      "54 val 4 811\n",
      "Phase: val. Epoch: 54. Loss: 0.06786678731441498\n",
      "54 val 5 823\n",
      "Phase: val. Epoch: 54. Loss: 0.09843965619802475\n",
      "54 val 6 835\n",
      "Phase: val. Epoch: 54. Loss: 0.10563671588897705\n",
      "54 val 7 847\n",
      "Phase: val. Epoch: 54. Loss: 0.09951527416706085\n",
      "54 val 8 859\n",
      "Phase: val. Epoch: 54. Loss: 0.08028791844844818\n",
      "54 val 9 871\n",
      "Phase: val. Epoch: 54. Loss: 0.07897016406059265\n",
      "54 val 10 883\n",
      "Phase: val. Epoch: 54. Loss: 0.08363428711891174\n",
      "54 val 11 884\n",
      "Phase: val. Epoch: 54. Loss: 0.03642553836107254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 train 0 12\n",
      "Phase: train. Epoch: 55. Loss: 0.0760447159409523\n",
      "55 train 1 24\n",
      "Phase: train. Epoch: 55. Loss: 0.08405941724777222\n",
      "55 train 2 36\n",
      "Phase: train. Epoch: 55. Loss: 0.0626615434885025\n",
      "55 train 3 48\n",
      "Phase: train. Epoch: 55. Loss: 0.08732432126998901\n",
      "55 train 4 60\n",
      "Phase: train. Epoch: 55. Loss: 0.0665055438876152\n",
      "55 train 5 72\n",
      "Phase: train. Epoch: 55. Loss: 0.08277782797813416\n",
      "55 train 6 84\n",
      "Phase: train. Epoch: 55. Loss: 0.10565046966075897\n",
      "55 train 7 96\n",
      "Phase: train. Epoch: 55. Loss: 0.07954926788806915\n",
      "55 train 8 108\n",
      "Phase: train. Epoch: 55. Loss: 0.10275419801473618\n",
      "55 train 9 120\n",
      "Phase: train. Epoch: 55. Loss: 0.10025420039892197\n",
      "55 train 10 132\n",
      "Phase: train. Epoch: 55. Loss: 0.07917989790439606\n",
      "55 train 11 144\n",
      "Phase: train. Epoch: 55. Loss: 0.1041649580001831\n",
      "55 train 12 156\n",
      "Phase: train. Epoch: 55. Loss: 0.07495525479316711\n",
      "55 train 13 168\n",
      "Phase: train. Epoch: 55. Loss: 0.0689721554517746\n",
      "55 train 14 180\n",
      "Phase: train. Epoch: 55. Loss: 0.08061335980892181\n",
      "55 train 15 192\n",
      "Phase: train. Epoch: 55. Loss: 0.09476710855960846\n",
      "55 train 16 204\n",
      "Phase: train. Epoch: 55. Loss: 0.10892364382743835\n",
      "55 train 17 216\n",
      "Phase: train. Epoch: 55. Loss: 0.10370564460754395\n",
      "55 train 18 228\n",
      "Phase: train. Epoch: 55. Loss: 0.12238270044326782\n",
      "55 train 19 240\n",
      "Phase: train. Epoch: 55. Loss: 0.07008952647447586\n",
      "55 train 20 252\n",
      "Phase: train. Epoch: 55. Loss: 0.07166769355535507\n",
      "55 train 21 264\n",
      "Phase: train. Epoch: 55. Loss: 0.07650811970233917\n",
      "55 train 22 276\n",
      "Phase: train. Epoch: 55. Loss: 0.09958285093307495\n",
      "55 train 23 288\n",
      "Phase: train. Epoch: 55. Loss: 0.06716541945934296\n",
      "55 train 24 300\n",
      "Phase: train. Epoch: 55. Loss: 0.08309146016836166\n",
      "55 train 25 312\n",
      "Phase: train. Epoch: 55. Loss: 0.11020617187023163\n",
      "55 train 26 324\n",
      "Phase: train. Epoch: 55. Loss: 0.09946419298648834\n",
      "55 train 27 336\n",
      "Phase: train. Epoch: 55. Loss: 0.07481743395328522\n",
      "55 train 28 348\n",
      "Phase: train. Epoch: 55. Loss: 0.08990143239498138\n",
      "55 train 29 360\n",
      "Phase: train. Epoch: 55. Loss: 0.07509146630764008\n",
      "55 train 30 372\n",
      "Phase: train. Epoch: 55. Loss: 0.09587457776069641\n",
      "55 train 31 384\n",
      "Phase: train. Epoch: 55. Loss: 0.0832616314291954\n",
      "55 train 32 396\n",
      "Phase: train. Epoch: 55. Loss: 0.10574914515018463\n",
      "55 train 33 408\n",
      "Phase: train. Epoch: 55. Loss: 0.10241209715604782\n",
      "55 train 34 420\n",
      "Phase: train. Epoch: 55. Loss: 0.08493100106716156\n",
      "55 train 35 432\n",
      "Phase: train. Epoch: 55. Loss: 0.06712321192026138\n",
      "55 train 36 444\n",
      "Phase: train. Epoch: 55. Loss: 0.0979432612657547\n",
      "55 train 37 456\n",
      "Phase: train. Epoch: 55. Loss: 0.060332637280225754\n",
      "55 train 38 468\n",
      "Phase: train. Epoch: 55. Loss: 0.08727601170539856\n",
      "55 train 39 480\n",
      "Phase: train. Epoch: 55. Loss: 0.07747610658407211\n",
      "55 train 40 492\n",
      "Phase: train. Epoch: 55. Loss: 0.07185760140419006\n",
      "55 train 41 504\n",
      "Phase: train. Epoch: 55. Loss: 0.12566706538200378\n",
      "55 train 42 516\n",
      "Phase: train. Epoch: 55. Loss: 0.12632326781749725\n",
      "55 train 43 528\n",
      "Phase: train. Epoch: 55. Loss: 0.08448778092861176\n",
      "55 train 44 540\n",
      "Phase: train. Epoch: 55. Loss: 0.07365304231643677\n",
      "55 train 45 552\n",
      "Phase: train. Epoch: 55. Loss: 0.06844879686832428\n",
      "55 train 46 564\n",
      "Phase: train. Epoch: 55. Loss: 0.07157554477453232\n",
      "55 train 47 576\n",
      "Phase: train. Epoch: 55. Loss: 0.06969580054283142\n",
      "55 train 48 588\n",
      "Phase: train. Epoch: 55. Loss: 0.06983770430088043\n",
      "55 train 49 600\n",
      "Phase: train. Epoch: 55. Loss: 0.09590785950422287\n",
      "55 train 50 612\n",
      "Phase: train. Epoch: 55. Loss: 0.0916961058974266\n",
      "55 train 51 624\n",
      "Phase: train. Epoch: 55. Loss: 0.09397630393505096\n",
      "55 train 52 636\n",
      "Phase: train. Epoch: 55. Loss: 0.12053646147251129\n",
      "55 train 53 648\n",
      "Phase: train. Epoch: 55. Loss: 0.08541487902402878\n",
      "55 train 54 660\n",
      "Phase: train. Epoch: 55. Loss: 0.0723504051566124\n",
      "55 train 55 672\n",
      "Phase: train. Epoch: 55. Loss: 0.099332295358181\n",
      "55 train 56 684\n",
      "Phase: train. Epoch: 55. Loss: 0.09601996839046478\n",
      "55 train 57 696\n",
      "Phase: train. Epoch: 55. Loss: 0.06715191155672073\n",
      "55 train 58 708\n",
      "Phase: train. Epoch: 55. Loss: 0.07773347198963165\n",
      "55 train 59 720\n",
      "Phase: train. Epoch: 55. Loss: 0.08408409357070923\n",
      "55 train 60 732\n",
      "Phase: train. Epoch: 55. Loss: 0.07992815971374512\n",
      "55 train 61 744\n",
      "Phase: train. Epoch: 55. Loss: 0.07021953910589218\n",
      "55 train 62 751\n",
      "Phase: train. Epoch: 55. Loss: 0.10199923813343048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 val 0 763\n",
      "Phase: val. Epoch: 55. Loss: 0.09628667682409286\n",
      "55 val 1 775\n",
      "Phase: val. Epoch: 55. Loss: 0.08319716155529022\n",
      "55 val 2 787\n",
      "Phase: val. Epoch: 55. Loss: 0.0993034690618515\n",
      "55 val 3 799\n",
      "Phase: val. Epoch: 55. Loss: 0.08675432205200195\n",
      "55 val 4 811\n",
      "Phase: val. Epoch: 55. Loss: 0.09729635715484619\n",
      "55 val 5 823\n",
      "Phase: val. Epoch: 55. Loss: 0.079137422144413\n",
      "55 val 6 835\n",
      "Phase: val. Epoch: 55. Loss: 0.09138739109039307\n",
      "55 val 7 847\n",
      "Phase: val. Epoch: 55. Loss: 0.1194833517074585\n",
      "55 val 8 859\n",
      "Phase: val. Epoch: 55. Loss: 0.08763061463832855\n",
      "55 val 9 871\n",
      "Phase: val. Epoch: 55. Loss: 0.06835925579071045\n",
      "55 val 10 883\n",
      "Phase: val. Epoch: 55. Loss: 0.08212726563215256\n",
      "55 val 11 884\n",
      "Phase: val. Epoch: 55. Loss: 0.027083871886134148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 train 0 12\n",
      "Phase: train. Epoch: 56. Loss: 0.07775548100471497\n",
      "56 train 1 24\n",
      "Phase: train. Epoch: 56. Loss: 0.0966445654630661\n",
      "56 train 2 36\n",
      "Phase: train. Epoch: 56. Loss: 0.07817058265209198\n",
      "56 train 3 48\n",
      "Phase: train. Epoch: 56. Loss: 0.06895370036363602\n",
      "56 train 4 60\n",
      "Phase: train. Epoch: 56. Loss: 0.08484556525945663\n",
      "56 train 5 72\n",
      "Phase: train. Epoch: 56. Loss: 0.07971423119306564\n",
      "56 train 6 84\n",
      "Phase: train. Epoch: 56. Loss: 0.08694156259298325\n",
      "56 train 7 96\n",
      "Phase: train. Epoch: 56. Loss: 0.07844334840774536\n",
      "56 train 8 108\n",
      "Phase: train. Epoch: 56. Loss: 0.07957454025745392\n",
      "56 train 9 120\n",
      "Phase: train. Epoch: 56. Loss: 0.11367549002170563\n",
      "56 train 10 132\n",
      "Phase: train. Epoch: 56. Loss: 0.0833771675825119\n",
      "56 train 11 144\n",
      "Phase: train. Epoch: 56. Loss: 0.0939786434173584\n",
      "56 train 12 156\n",
      "Phase: train. Epoch: 56. Loss: 0.07308989763259888\n",
      "56 train 13 168\n",
      "Phase: train. Epoch: 56. Loss: 0.0950503796339035\n",
      "56 train 14 180\n",
      "Phase: train. Epoch: 56. Loss: 0.09799689054489136\n",
      "56 train 15 192\n",
      "Phase: train. Epoch: 56. Loss: 0.09982423484325409\n",
      "56 train 16 204\n",
      "Phase: train. Epoch: 56. Loss: 0.07714637368917465\n",
      "56 train 17 216\n",
      "Phase: train. Epoch: 56. Loss: 0.09992300719022751\n",
      "56 train 18 228\n",
      "Phase: train. Epoch: 56. Loss: 0.08065813779830933\n",
      "56 train 19 240\n",
      "Phase: train. Epoch: 56. Loss: 0.08087438344955444\n",
      "56 train 20 252\n",
      "Phase: train. Epoch: 56. Loss: 0.08788983523845673\n",
      "56 train 21 264\n",
      "Phase: train. Epoch: 56. Loss: 0.11757232993841171\n",
      "56 train 22 276\n",
      "Phase: train. Epoch: 56. Loss: 0.11248459666967392\n",
      "56 train 23 288\n",
      "Phase: train. Epoch: 56. Loss: 0.08195734024047852\n",
      "56 train 24 300\n",
      "Phase: train. Epoch: 56. Loss: 0.07934268563985825\n",
      "56 train 25 312\n",
      "Phase: train. Epoch: 56. Loss: 0.07004405558109283\n",
      "56 train 26 324\n",
      "Phase: train. Epoch: 56. Loss: 0.09275057166814804\n",
      "56 train 27 336\n",
      "Phase: train. Epoch: 56. Loss: 0.06258964538574219\n",
      "56 train 28 348\n",
      "Phase: train. Epoch: 56. Loss: 0.09243802726268768\n",
      "56 train 29 360\n",
      "Phase: train. Epoch: 56. Loss: 0.07385194301605225\n",
      "56 train 30 372\n",
      "Phase: train. Epoch: 56. Loss: 0.0698952004313469\n",
      "56 train 31 384\n",
      "Phase: train. Epoch: 56. Loss: 0.07469993829727173\n",
      "56 train 32 396\n",
      "Phase: train. Epoch: 56. Loss: 0.0727500468492508\n",
      "56 train 33 408\n",
      "Phase: train. Epoch: 56. Loss: 0.10152585804462433\n",
      "56 train 34 420\n",
      "Phase: train. Epoch: 56. Loss: 0.0781017392873764\n",
      "56 train 35 432\n",
      "Phase: train. Epoch: 56. Loss: 0.08994370698928833\n",
      "56 train 36 444\n",
      "Phase: train. Epoch: 56. Loss: 0.09734990447759628\n",
      "56 train 37 456\n",
      "Phase: train. Epoch: 56. Loss: 0.06570686399936676\n",
      "56 train 38 468\n",
      "Phase: train. Epoch: 56. Loss: 0.09255066514015198\n",
      "56 train 39 480\n",
      "Phase: train. Epoch: 56. Loss: 0.085202157497406\n",
      "56 train 40 492\n",
      "Phase: train. Epoch: 56. Loss: 0.0773390680551529\n",
      "56 train 41 504\n",
      "Phase: train. Epoch: 56. Loss: 0.08702236413955688\n",
      "56 train 42 516\n",
      "Phase: train. Epoch: 56. Loss: 0.06901071965694427\n",
      "56 train 43 528\n",
      "Phase: train. Epoch: 56. Loss: 0.11079498380422592\n",
      "56 train 44 540\n",
      "Phase: train. Epoch: 56. Loss: 0.05926098674535751\n",
      "56 train 45 552\n",
      "Phase: train. Epoch: 56. Loss: 0.08908204734325409\n",
      "56 train 46 564\n",
      "Phase: train. Epoch: 56. Loss: 0.09782195091247559\n",
      "56 train 47 576\n",
      "Phase: train. Epoch: 56. Loss: 0.10087765753269196\n",
      "56 train 48 588\n",
      "Phase: train. Epoch: 56. Loss: 0.1085953339934349\n",
      "56 train 49 600\n",
      "Phase: train. Epoch: 56. Loss: 0.06841512024402618\n",
      "56 train 50 612\n",
      "Phase: train. Epoch: 56. Loss: 0.07797547429800034\n",
      "56 train 51 624\n",
      "Phase: train. Epoch: 56. Loss: 0.08685699105262756\n",
      "56 train 52 636\n",
      "Phase: train. Epoch: 56. Loss: 0.09531443566083908\n",
      "56 train 53 648\n",
      "Phase: train. Epoch: 56. Loss: 0.08024317026138306\n",
      "56 train 54 660\n",
      "Phase: train. Epoch: 56. Loss: 0.0923982784152031\n",
      "56 train 55 672\n",
      "Phase: train. Epoch: 56. Loss: 0.08833765983581543\n",
      "56 train 56 684\n",
      "Phase: train. Epoch: 56. Loss: 0.1076188012957573\n",
      "56 train 57 696\n",
      "Phase: train. Epoch: 56. Loss: 0.10345091670751572\n",
      "56 train 58 708\n",
      "Phase: train. Epoch: 56. Loss: 0.09610447287559509\n",
      "56 train 59 720\n",
      "Phase: train. Epoch: 56. Loss: 0.0960448831319809\n",
      "56 train 60 732\n",
      "Phase: train. Epoch: 56. Loss: 0.06422464549541473\n",
      "56 train 61 744\n",
      "Phase: train. Epoch: 56. Loss: 0.09977523982524872\n",
      "56 train 62 751\n",
      "Phase: train. Epoch: 56. Loss: 0.10060301423072815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 val 0 763\n",
      "Phase: val. Epoch: 56. Loss: 0.06087622046470642\n",
      "56 val 1 775\n",
      "Phase: val. Epoch: 56. Loss: 0.07883088290691376\n",
      "56 val 2 787\n",
      "Phase: val. Epoch: 56. Loss: 0.07700608670711517\n",
      "56 val 3 799\n",
      "Phase: val. Epoch: 56. Loss: 0.06106393039226532\n",
      "56 val 4 811\n",
      "Phase: val. Epoch: 56. Loss: 0.08947475999593735\n",
      "56 val 5 823\n",
      "Phase: val. Epoch: 56. Loss: 0.09765133261680603\n",
      "56 val 6 835\n",
      "Phase: val. Epoch: 56. Loss: 0.061675868928432465\n",
      "56 val 7 847\n",
      "Phase: val. Epoch: 56. Loss: 0.07504774630069733\n",
      "56 val 8 859\n",
      "Phase: val. Epoch: 56. Loss: 0.09221935272216797\n",
      "56 val 9 871\n",
      "Phase: val. Epoch: 56. Loss: 0.07396432757377625\n",
      "56 val 10 883\n",
      "Phase: val. Epoch: 56. Loss: 0.10866721719503403\n",
      "56 val 11 884\n",
      "Phase: val. Epoch: 56. Loss: 0.06249125301837921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 train 0 12\n",
      "Phase: train. Epoch: 57. Loss: 0.06723335385322571\n",
      "57 train 1 24\n",
      "Phase: train. Epoch: 57. Loss: 0.06899833679199219\n",
      "57 train 2 36\n",
      "Phase: train. Epoch: 57. Loss: 0.07981447130441666\n",
      "57 train 3 48\n",
      "Phase: train. Epoch: 57. Loss: 0.07574380934238434\n",
      "57 train 4 60\n",
      "Phase: train. Epoch: 57. Loss: 0.08068811148405075\n",
      "57 train 5 72\n",
      "Phase: train. Epoch: 57. Loss: 0.08300109207630157\n",
      "57 train 6 84\n",
      "Phase: train. Epoch: 57. Loss: 0.08361946046352386\n",
      "57 train 7 96\n",
      "Phase: train. Epoch: 57. Loss: 0.10518316924571991\n",
      "57 train 8 108\n",
      "Phase: train. Epoch: 57. Loss: 0.08352385461330414\n",
      "57 train 9 120\n",
      "Phase: train. Epoch: 57. Loss: 0.07493962347507477\n",
      "57 train 10 132\n",
      "Phase: train. Epoch: 57. Loss: 0.07741180062294006\n",
      "57 train 11 144\n",
      "Phase: train. Epoch: 57. Loss: 0.11408954858779907\n",
      "57 train 12 156\n",
      "Phase: train. Epoch: 57. Loss: 0.08342726528644562\n",
      "57 train 13 168\n",
      "Phase: train. Epoch: 57. Loss: 0.09649626910686493\n",
      "57 train 14 180\n",
      "Phase: train. Epoch: 57. Loss: 0.09594794362783432\n",
      "57 train 15 192\n",
      "Phase: train. Epoch: 57. Loss: 0.11302026361227036\n",
      "57 train 16 204\n",
      "Phase: train. Epoch: 57. Loss: 0.09318964183330536\n",
      "57 train 17 216\n",
      "Phase: train. Epoch: 57. Loss: 0.09260392189025879\n",
      "57 train 18 228\n",
      "Phase: train. Epoch: 57. Loss: 0.08550147712230682\n",
      "57 train 19 240\n",
      "Phase: train. Epoch: 57. Loss: 0.08932387083768845\n",
      "57 train 20 252\n",
      "Phase: train. Epoch: 57. Loss: 0.08325626701116562\n",
      "57 train 21 264\n",
      "Phase: train. Epoch: 57. Loss: 0.10917764157056808\n",
      "57 train 22 276\n",
      "Phase: train. Epoch: 57. Loss: 0.07955028861761093\n",
      "57 train 23 288\n",
      "Phase: train. Epoch: 57. Loss: 0.07171234488487244\n",
      "57 train 24 300\n",
      "Phase: train. Epoch: 57. Loss: 0.08469115197658539\n",
      "57 train 25 312\n",
      "Phase: train. Epoch: 57. Loss: 0.06637260317802429\n",
      "57 train 26 324\n",
      "Phase: train. Epoch: 57. Loss: 0.07390351593494415\n",
      "57 train 27 336\n",
      "Phase: train. Epoch: 57. Loss: 0.08973294496536255\n",
      "57 train 28 348\n",
      "Phase: train. Epoch: 57. Loss: 0.0906233936548233\n",
      "57 train 29 360\n",
      "Phase: train. Epoch: 57. Loss: 0.1108425110578537\n",
      "57 train 30 372\n",
      "Phase: train. Epoch: 57. Loss: 0.07220606505870819\n",
      "57 train 31 384\n",
      "Phase: train. Epoch: 57. Loss: 0.11299324035644531\n",
      "57 train 32 396\n",
      "Phase: train. Epoch: 57. Loss: 0.09609540551900864\n",
      "57 train 33 408\n",
      "Phase: train. Epoch: 57. Loss: 0.07024608552455902\n",
      "57 train 34 420\n",
      "Phase: train. Epoch: 57. Loss: 0.08486005663871765\n",
      "57 train 35 432\n",
      "Phase: train. Epoch: 57. Loss: 0.07309995591640472\n",
      "57 train 36 444\n",
      "Phase: train. Epoch: 57. Loss: 0.10296517610549927\n",
      "57 train 37 456\n",
      "Phase: train. Epoch: 57. Loss: 0.07256202399730682\n",
      "57 train 38 468\n",
      "Phase: train. Epoch: 57. Loss: 0.07486678659915924\n",
      "57 train 39 480\n",
      "Phase: train. Epoch: 57. Loss: 0.0785883367061615\n",
      "57 train 40 492\n",
      "Phase: train. Epoch: 57. Loss: 0.10134048759937286\n",
      "57 train 41 504\n",
      "Phase: train. Epoch: 57. Loss: 0.0657767504453659\n",
      "57 train 42 516\n",
      "Phase: train. Epoch: 57. Loss: 0.08441658318042755\n",
      "57 train 43 528\n",
      "Phase: train. Epoch: 57. Loss: 0.07514305412769318\n",
      "57 train 44 540\n",
      "Phase: train. Epoch: 57. Loss: 0.07916487753391266\n",
      "57 train 45 552\n",
      "Phase: train. Epoch: 57. Loss: 0.07435273379087448\n",
      "57 train 46 564\n",
      "Phase: train. Epoch: 57. Loss: 0.07161478698253632\n",
      "57 train 47 576\n",
      "Phase: train. Epoch: 57. Loss: 0.08252212405204773\n",
      "57 train 48 588\n",
      "Phase: train. Epoch: 57. Loss: 0.08465342223644257\n",
      "57 train 49 600\n",
      "Phase: train. Epoch: 57. Loss: 0.08977077901363373\n",
      "57 train 50 612\n",
      "Phase: train. Epoch: 57. Loss: 0.0844968855381012\n",
      "57 train 51 624\n",
      "Phase: train. Epoch: 57. Loss: 0.10241203010082245\n",
      "57 train 52 636\n",
      "Phase: train. Epoch: 57. Loss: 0.09731703251600266\n",
      "57 train 53 648\n",
      "Phase: train. Epoch: 57. Loss: 0.06955582648515701\n",
      "57 train 54 660\n",
      "Phase: train. Epoch: 57. Loss: 0.10770611464977264\n",
      "57 train 55 672\n",
      "Phase: train. Epoch: 57. Loss: 0.10070066154003143\n",
      "57 train 56 684\n",
      "Phase: train. Epoch: 57. Loss: 0.06272999197244644\n",
      "57 train 57 696\n",
      "Phase: train. Epoch: 57. Loss: 0.10508362948894501\n",
      "57 train 58 708\n",
      "Phase: train. Epoch: 57. Loss: 0.06910982728004456\n",
      "57 train 59 720\n",
      "Phase: train. Epoch: 57. Loss: 0.12056662142276764\n",
      "57 train 60 732\n",
      "Phase: train. Epoch: 57. Loss: 0.11364978551864624\n",
      "57 train 61 744\n",
      "Phase: train. Epoch: 57. Loss: 0.09010081738233566\n",
      "57 train 62 751\n",
      "Phase: train. Epoch: 57. Loss: 0.08779937773942947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 val 0 763\n",
      "Phase: val. Epoch: 57. Loss: 0.09232167899608612\n",
      "57 val 1 775\n",
      "Phase: val. Epoch: 57. Loss: 0.08925831317901611\n",
      "57 val 2 787\n",
      "Phase: val. Epoch: 57. Loss: 0.049798306077718735\n",
      "57 val 3 799\n",
      "Phase: val. Epoch: 57. Loss: 0.08070254325866699\n",
      "57 val 4 811\n",
      "Phase: val. Epoch: 57. Loss: 0.10145336389541626\n",
      "57 val 5 823\n",
      "Phase: val. Epoch: 57. Loss: 0.080764040350914\n",
      "57 val 6 835\n",
      "Phase: val. Epoch: 57. Loss: 0.07901088893413544\n",
      "57 val 7 847\n",
      "Phase: val. Epoch: 57. Loss: 0.0788625031709671\n",
      "57 val 8 859\n",
      "Phase: val. Epoch: 57. Loss: 0.10202368348836899\n",
      "57 val 9 871\n",
      "Phase: val. Epoch: 57. Loss: 0.07316164672374725\n",
      "57 val 10 883\n",
      "Phase: val. Epoch: 57. Loss: 0.07130177319049835\n",
      "57 val 11 884\n",
      "Phase: val. Epoch: 57. Loss: 0.182220458984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 train 0 12\n",
      "Phase: train. Epoch: 58. Loss: 0.1272374391555786\n",
      "58 train 1 24\n",
      "Phase: train. Epoch: 58. Loss: 0.09972362220287323\n",
      "58 train 2 36\n",
      "Phase: train. Epoch: 58. Loss: 0.09039201587438583\n",
      "58 train 3 48\n",
      "Phase: train. Epoch: 58. Loss: 0.09114160388708115\n",
      "58 train 4 60\n",
      "Phase: train. Epoch: 58. Loss: 0.06743913888931274\n",
      "58 train 5 72\n",
      "Phase: train. Epoch: 58. Loss: 0.10293569415807724\n",
      "58 train 6 84\n",
      "Phase: train. Epoch: 58. Loss: 0.08712278306484222\n",
      "58 train 7 96\n",
      "Phase: train. Epoch: 58. Loss: 0.10208000987768173\n",
      "58 train 8 108\n",
      "Phase: train. Epoch: 58. Loss: 0.07058345526456833\n",
      "58 train 9 120\n",
      "Phase: train. Epoch: 58. Loss: 0.0903652235865593\n",
      "58 train 10 132\n",
      "Phase: train. Epoch: 58. Loss: 0.09869503229856491\n",
      "58 train 11 144\n",
      "Phase: train. Epoch: 58. Loss: 0.08669491857290268\n",
      "58 train 12 156\n",
      "Phase: train. Epoch: 58. Loss: 0.08984963595867157\n",
      "58 train 13 168\n",
      "Phase: train. Epoch: 58. Loss: 0.07276826351881027\n",
      "58 train 14 180\n",
      "Phase: train. Epoch: 58. Loss: 0.07064538449048996\n",
      "58 train 15 192\n",
      "Phase: train. Epoch: 58. Loss: 0.07383938878774643\n",
      "58 train 16 204\n",
      "Phase: train. Epoch: 58. Loss: 0.062576524913311\n",
      "58 train 17 216\n",
      "Phase: train. Epoch: 58. Loss: 0.08557452261447906\n",
      "58 train 18 228\n",
      "Phase: train. Epoch: 58. Loss: 0.07950695604085922\n",
      "58 train 19 240\n",
      "Phase: train. Epoch: 58. Loss: 0.08893188089132309\n",
      "58 train 20 252\n",
      "Phase: train. Epoch: 58. Loss: 0.10799847543239594\n",
      "58 train 21 264\n",
      "Phase: train. Epoch: 58. Loss: 0.09257245063781738\n",
      "58 train 22 276\n",
      "Phase: train. Epoch: 58. Loss: 0.06030746549367905\n",
      "58 train 23 288\n",
      "Phase: train. Epoch: 58. Loss: 0.07509791105985641\n",
      "58 train 24 300\n",
      "Phase: train. Epoch: 58. Loss: 0.08543187379837036\n",
      "58 train 25 312\n",
      "Phase: train. Epoch: 58. Loss: 0.08978735655546188\n",
      "58 train 26 324\n",
      "Phase: train. Epoch: 58. Loss: 0.08240237832069397\n",
      "58 train 27 336\n",
      "Phase: train. Epoch: 58. Loss: 0.0939309149980545\n",
      "58 train 28 348\n",
      "Phase: train. Epoch: 58. Loss: 0.0656641274690628\n",
      "58 train 29 360\n",
      "Phase: train. Epoch: 58. Loss: 0.07532530277967453\n",
      "58 train 30 372\n",
      "Phase: train. Epoch: 58. Loss: 0.08251594752073288\n",
      "58 train 31 384\n",
      "Phase: train. Epoch: 58. Loss: 0.06648501008749008\n",
      "58 train 32 396\n",
      "Phase: train. Epoch: 58. Loss: 0.09053283929824829\n",
      "58 train 33 408\n",
      "Phase: train. Epoch: 58. Loss: 0.07884859293699265\n",
      "58 train 34 420\n",
      "Phase: train. Epoch: 58. Loss: 0.07468251883983612\n",
      "58 train 35 432\n",
      "Phase: train. Epoch: 58. Loss: 0.08695924282073975\n",
      "58 train 36 444\n",
      "Phase: train. Epoch: 58. Loss: 0.08221288025379181\n",
      "58 train 37 456\n",
      "Phase: train. Epoch: 58. Loss: 0.07942694425582886\n",
      "58 train 38 468\n",
      "Phase: train. Epoch: 58. Loss: 0.08127260208129883\n",
      "58 train 39 480\n",
      "Phase: train. Epoch: 58. Loss: 0.09757106751203537\n",
      "58 train 40 492\n",
      "Phase: train. Epoch: 58. Loss: 0.11641418933868408\n",
      "58 train 41 504\n",
      "Phase: train. Epoch: 58. Loss: 0.06749631464481354\n",
      "58 train 42 516\n",
      "Phase: train. Epoch: 58. Loss: 0.09875210374593735\n",
      "58 train 43 528\n",
      "Phase: train. Epoch: 58. Loss: 0.07299646735191345\n",
      "58 train 44 540\n",
      "Phase: train. Epoch: 58. Loss: 0.07259891927242279\n",
      "58 train 45 552\n",
      "Phase: train. Epoch: 58. Loss: 0.08884650468826294\n",
      "58 train 46 564\n",
      "Phase: train. Epoch: 58. Loss: 0.07517123222351074\n",
      "58 train 47 576\n",
      "Phase: train. Epoch: 58. Loss: 0.08679527044296265\n",
      "58 train 48 588\n",
      "Phase: train. Epoch: 58. Loss: 0.0937592014670372\n",
      "58 train 49 600\n",
      "Phase: train. Epoch: 58. Loss: 0.090558260679245\n",
      "58 train 50 612\n",
      "Phase: train. Epoch: 58. Loss: 0.10463784635066986\n",
      "58 train 51 624\n",
      "Phase: train. Epoch: 58. Loss: 0.075871542096138\n",
      "58 train 52 636\n",
      "Phase: train. Epoch: 58. Loss: 0.07637666165828705\n",
      "58 train 53 648\n",
      "Phase: train. Epoch: 58. Loss: 0.09240931272506714\n",
      "58 train 54 660\n",
      "Phase: train. Epoch: 58. Loss: 0.11066457629203796\n",
      "58 train 55 672\n",
      "Phase: train. Epoch: 58. Loss: 0.07276330888271332\n",
      "58 train 56 684\n",
      "Phase: train. Epoch: 58. Loss: 0.09543772786855698\n",
      "58 train 57 696\n",
      "Phase: train. Epoch: 58. Loss: 0.07764410227537155\n",
      "58 train 58 708\n",
      "Phase: train. Epoch: 58. Loss: 0.09326472133398056\n",
      "58 train 59 720\n",
      "Phase: train. Epoch: 58. Loss: 0.09913045167922974\n",
      "58 train 60 732\n",
      "Phase: train. Epoch: 58. Loss: 0.09810887277126312\n",
      "58 train 61 744\n",
      "Phase: train. Epoch: 58. Loss: 0.1062198281288147\n",
      "58 train 62 751\n",
      "Phase: train. Epoch: 58. Loss: 0.0815458819270134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 val 0 763\n",
      "Phase: val. Epoch: 58. Loss: 0.06675764918327332\n",
      "58 val 1 775\n",
      "Phase: val. Epoch: 58. Loss: 0.052528880536556244\n",
      "58 val 2 787\n",
      "Phase: val. Epoch: 58. Loss: 0.08896714448928833\n",
      "58 val 3 799\n",
      "Phase: val. Epoch: 58. Loss: 0.09559749066829681\n",
      "58 val 4 811\n",
      "Phase: val. Epoch: 58. Loss: 0.07375334948301315\n",
      "58 val 5 823\n",
      "Phase: val. Epoch: 58. Loss: 0.06332464516162872\n",
      "58 val 6 835\n",
      "Phase: val. Epoch: 58. Loss: 0.08615949004888535\n",
      "58 val 7 847\n",
      "Phase: val. Epoch: 58. Loss: 0.09395730495452881\n",
      "58 val 8 859\n",
      "Phase: val. Epoch: 58. Loss: 0.09423613548278809\n",
      "58 val 9 871\n",
      "Phase: val. Epoch: 58. Loss: 0.09075557440519333\n",
      "58 val 10 883\n",
      "Phase: val. Epoch: 58. Loss: 0.06639260053634644\n",
      "58 val 11 884\n",
      "Phase: val. Epoch: 58. Loss: 0.034883469343185425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 train 0 12\n",
      "Phase: train. Epoch: 59. Loss: 0.09641852229833603\n",
      "59 train 1 24\n",
      "Phase: train. Epoch: 59. Loss: 0.06278087943792343\n",
      "59 train 2 36\n",
      "Phase: train. Epoch: 59. Loss: 0.07940180599689484\n",
      "59 train 3 48\n",
      "Phase: train. Epoch: 59. Loss: 0.07902170717716217\n",
      "59 train 4 60\n",
      "Phase: train. Epoch: 59. Loss: 0.07274520397186279\n",
      "59 train 5 72\n",
      "Phase: train. Epoch: 59. Loss: 0.09178664535284042\n",
      "59 train 6 84\n",
      "Phase: train. Epoch: 59. Loss: 0.11660677194595337\n",
      "59 train 7 96\n",
      "Phase: train. Epoch: 59. Loss: 0.07153870165348053\n",
      "59 train 8 108\n",
      "Phase: train. Epoch: 59. Loss: 0.07387465238571167\n",
      "59 train 9 120\n",
      "Phase: train. Epoch: 59. Loss: 0.0835283100605011\n",
      "59 train 10 132\n",
      "Phase: train. Epoch: 59. Loss: 0.06783206760883331\n",
      "59 train 11 144\n",
      "Phase: train. Epoch: 59. Loss: 0.08089503645896912\n",
      "59 train 12 156\n",
      "Phase: train. Epoch: 59. Loss: 0.10950717329978943\n",
      "59 train 13 168\n",
      "Phase: train. Epoch: 59. Loss: 0.08111895620822906\n",
      "59 train 14 180\n",
      "Phase: train. Epoch: 59. Loss: 0.08290213346481323\n",
      "59 train 15 192\n",
      "Phase: train. Epoch: 59. Loss: 0.09665345400571823\n",
      "59 train 16 204\n",
      "Phase: train. Epoch: 59. Loss: 0.10796700417995453\n",
      "59 train 17 216\n",
      "Phase: train. Epoch: 59. Loss: 0.08764888346195221\n",
      "59 train 18 228\n",
      "Phase: train. Epoch: 59. Loss: 0.07993369549512863\n",
      "59 train 19 240\n",
      "Phase: train. Epoch: 59. Loss: 0.08549119532108307\n",
      "59 train 20 252\n",
      "Phase: train. Epoch: 59. Loss: 0.10127266496419907\n",
      "59 train 21 264\n",
      "Phase: train. Epoch: 59. Loss: 0.0830792486667633\n",
      "59 train 22 276\n",
      "Phase: train. Epoch: 59. Loss: 0.12078729271888733\n",
      "59 train 23 288\n",
      "Phase: train. Epoch: 59. Loss: 0.07179689407348633\n",
      "59 train 24 300\n",
      "Phase: train. Epoch: 59. Loss: 0.08802974224090576\n",
      "59 train 25 312\n",
      "Phase: train. Epoch: 59. Loss: 0.09106460213661194\n",
      "59 train 26 324\n",
      "Phase: train. Epoch: 59. Loss: 0.09510904550552368\n",
      "59 train 27 336\n",
      "Phase: train. Epoch: 59. Loss: 0.08375905454158783\n",
      "59 train 28 348\n",
      "Phase: train. Epoch: 59. Loss: 0.0832868367433548\n",
      "59 train 29 360\n",
      "Phase: train. Epoch: 59. Loss: 0.09862403571605682\n",
      "59 train 30 372\n",
      "Phase: train. Epoch: 59. Loss: 0.07091239094734192\n",
      "59 train 31 384\n",
      "Phase: train. Epoch: 59. Loss: 0.07603238523006439\n",
      "59 train 32 396\n",
      "Phase: train. Epoch: 59. Loss: 0.07735806703567505\n",
      "59 train 33 408\n",
      "Phase: train. Epoch: 59. Loss: 0.06662808358669281\n",
      "59 train 34 420\n",
      "Phase: train. Epoch: 59. Loss: 0.10618387907743454\n",
      "59 train 35 432\n",
      "Phase: train. Epoch: 59. Loss: 0.06991603970527649\n",
      "59 train 36 444\n",
      "Phase: train. Epoch: 59. Loss: 0.05954183265566826\n",
      "59 train 37 456\n",
      "Phase: train. Epoch: 59. Loss: 0.07444597780704498\n",
      "59 train 38 468\n",
      "Phase: train. Epoch: 59. Loss: 0.08478271961212158\n",
      "59 train 39 480\n",
      "Phase: train. Epoch: 59. Loss: 0.10661449283361435\n",
      "59 train 40 492\n",
      "Phase: train. Epoch: 59. Loss: 0.070679672062397\n",
      "59 train 41 504\n",
      "Phase: train. Epoch: 59. Loss: 0.09825768321752548\n",
      "59 train 42 516\n",
      "Phase: train. Epoch: 59. Loss: 0.09155746549367905\n",
      "59 train 43 528\n",
      "Phase: train. Epoch: 59. Loss: 0.11536456644535065\n",
      "59 train 44 540\n",
      "Phase: train. Epoch: 59. Loss: 0.09448113292455673\n",
      "59 train 45 552\n",
      "Phase: train. Epoch: 59. Loss: 0.0758213922381401\n",
      "59 train 46 564\n",
      "Phase: train. Epoch: 59. Loss: 0.07178393006324768\n",
      "59 train 47 576\n",
      "Phase: train. Epoch: 59. Loss: 0.0652700811624527\n",
      "59 train 48 588\n",
      "Phase: train. Epoch: 59. Loss: 0.09755303710699081\n",
      "59 train 49 600\n",
      "Phase: train. Epoch: 59. Loss: 0.06196419149637222\n",
      "59 train 50 612\n",
      "Phase: train. Epoch: 59. Loss: 0.08283734321594238\n",
      "59 train 51 624\n",
      "Phase: train. Epoch: 59. Loss: 0.08205827325582504\n",
      "59 train 52 636\n",
      "Phase: train. Epoch: 59. Loss: 0.06450243294239044\n",
      "59 train 53 648\n",
      "Phase: train. Epoch: 59. Loss: 0.09465532004833221\n",
      "59 train 54 660\n",
      "Phase: train. Epoch: 59. Loss: 0.08992967009544373\n",
      "59 train 55 672\n",
      "Phase: train. Epoch: 59. Loss: 0.07715067267417908\n",
      "59 train 56 684\n",
      "Phase: train. Epoch: 59. Loss: 0.06271978467702866\n",
      "59 train 57 696\n",
      "Phase: train. Epoch: 59. Loss: 0.10980448126792908\n",
      "59 train 58 708\n",
      "Phase: train. Epoch: 59. Loss: 0.07618077099323273\n",
      "59 train 59 720\n",
      "Phase: train. Epoch: 59. Loss: 0.10715138912200928\n",
      "59 train 60 732\n",
      "Phase: train. Epoch: 59. Loss: 0.10160712897777557\n",
      "59 train 61 744\n",
      "Phase: train. Epoch: 59. Loss: 0.11305530369281769\n",
      "59 train 62 751\n",
      "Phase: train. Epoch: 59. Loss: 0.09194352477788925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 val 0 763\n",
      "Phase: val. Epoch: 59. Loss: 0.09542867541313171\n",
      "59 val 1 775\n",
      "Phase: val. Epoch: 59. Loss: 0.08427589386701584\n",
      "59 val 2 787\n",
      "Phase: val. Epoch: 59. Loss: 0.11214368045330048\n",
      "59 val 3 799\n",
      "Phase: val. Epoch: 59. Loss: 0.04935630410909653\n",
      "59 val 4 811\n",
      "Phase: val. Epoch: 59. Loss: 0.08358883857727051\n",
      "59 val 5 823\n",
      "Phase: val. Epoch: 59. Loss: 0.10145153105258942\n",
      "59 val 6 835\n",
      "Phase: val. Epoch: 59. Loss: 0.07491963356733322\n",
      "59 val 7 847\n",
      "Phase: val. Epoch: 59. Loss: 0.055921800434589386\n",
      "59 val 8 859\n",
      "Phase: val. Epoch: 59. Loss: 0.08407799899578094\n",
      "59 val 9 871\n",
      "Phase: val. Epoch: 59. Loss: 0.08231375366449356\n",
      "59 val 10 883\n",
      "Phase: val. Epoch: 59. Loss: 0.08222445100545883\n",
      "59 val 11 884\n",
      "Phase: val. Epoch: 59. Loss: 0.02752671018242836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 train 0 12\n",
      "Phase: train. Epoch: 60. Loss: 0.09651284664869308\n",
      "60 train 1 24\n",
      "Phase: train. Epoch: 60. Loss: 0.07613205164670944\n",
      "60 train 2 36\n",
      "Phase: train. Epoch: 60. Loss: 0.07603118568658829\n",
      "60 train 3 48\n",
      "Phase: train. Epoch: 60. Loss: 0.08602531999349594\n",
      "60 train 4 60\n",
      "Phase: train. Epoch: 60. Loss: 0.082039475440979\n",
      "60 train 5 72\n",
      "Phase: train. Epoch: 60. Loss: 0.08278421312570572\n",
      "60 train 6 84\n",
      "Phase: train. Epoch: 60. Loss: 0.09595461934804916\n",
      "60 train 7 96\n",
      "Phase: train. Epoch: 60. Loss: 0.08051038533449173\n",
      "60 train 8 108\n",
      "Phase: train. Epoch: 60. Loss: 0.08071045577526093\n",
      "60 train 9 120\n",
      "Phase: train. Epoch: 60. Loss: 0.09399987757205963\n",
      "60 train 10 132\n",
      "Phase: train. Epoch: 60. Loss: 0.08714946359395981\n",
      "60 train 11 144\n",
      "Phase: train. Epoch: 60. Loss: 0.08465346693992615\n",
      "60 train 12 156\n",
      "Phase: train. Epoch: 60. Loss: 0.06446923315525055\n",
      "60 train 13 168\n",
      "Phase: train. Epoch: 60. Loss: 0.0915619358420372\n",
      "60 train 14 180\n",
      "Phase: train. Epoch: 60. Loss: 0.09118978679180145\n",
      "60 train 15 192\n",
      "Phase: train. Epoch: 60. Loss: 0.05989520251750946\n",
      "60 train 16 204\n",
      "Phase: train. Epoch: 60. Loss: 0.0895717591047287\n",
      "60 train 17 216\n",
      "Phase: train. Epoch: 60. Loss: 0.07287400960922241\n",
      "60 train 18 228\n",
      "Phase: train. Epoch: 60. Loss: 0.07717188447713852\n",
      "60 train 19 240\n",
      "Phase: train. Epoch: 60. Loss: 0.08251677453517914\n",
      "60 train 20 252\n",
      "Phase: train. Epoch: 60. Loss: 0.07694436609745026\n",
      "60 train 21 264\n",
      "Phase: train. Epoch: 60. Loss: 0.13662505149841309\n",
      "60 train 22 276\n",
      "Phase: train. Epoch: 60. Loss: 0.08563282340765\n",
      "60 train 23 288\n",
      "Phase: train. Epoch: 60. Loss: 0.09687386453151703\n",
      "60 train 24 300\n",
      "Phase: train. Epoch: 60. Loss: 0.09211741387844086\n",
      "60 train 25 312\n",
      "Phase: train. Epoch: 60. Loss: 0.06956086307764053\n",
      "60 train 26 324\n",
      "Phase: train. Epoch: 60. Loss: 0.08212688565254211\n",
      "60 train 27 336\n",
      "Phase: train. Epoch: 60. Loss: 0.08774539828300476\n",
      "60 train 28 348\n",
      "Phase: train. Epoch: 60. Loss: 0.0893496498465538\n",
      "60 train 29 360\n",
      "Phase: train. Epoch: 60. Loss: 0.08706365525722504\n",
      "60 train 30 372\n",
      "Phase: train. Epoch: 60. Loss: 0.07224282622337341\n",
      "60 train 31 384\n",
      "Phase: train. Epoch: 60. Loss: 0.07216286659240723\n",
      "60 train 32 396\n",
      "Phase: train. Epoch: 60. Loss: 0.0857788547873497\n",
      "60 train 33 408\n",
      "Phase: train. Epoch: 60. Loss: 0.06656795740127563\n",
      "60 train 34 420\n",
      "Phase: train. Epoch: 60. Loss: 0.10140745341777802\n",
      "60 train 35 432\n",
      "Phase: train. Epoch: 60. Loss: 0.06544160097837448\n",
      "60 train 36 444\n",
      "Phase: train. Epoch: 60. Loss: 0.1065545529127121\n",
      "60 train 37 456\n",
      "Phase: train. Epoch: 60. Loss: 0.07389479130506516\n",
      "60 train 38 468\n",
      "Phase: train. Epoch: 60. Loss: 0.07496370375156403\n",
      "60 train 39 480\n",
      "Phase: train. Epoch: 60. Loss: 0.08270477503538132\n",
      "60 train 40 492\n",
      "Phase: train. Epoch: 60. Loss: 0.1036081463098526\n",
      "60 train 41 504\n",
      "Phase: train. Epoch: 60. Loss: 0.12088199704885483\n",
      "60 train 42 516\n",
      "Phase: train. Epoch: 60. Loss: 0.10722144693136215\n",
      "60 train 43 528\n",
      "Phase: train. Epoch: 60. Loss: 0.11901155114173889\n",
      "60 train 44 540\n",
      "Phase: train. Epoch: 60. Loss: 0.09472595900297165\n",
      "60 train 45 552\n",
      "Phase: train. Epoch: 60. Loss: 0.10387539863586426\n",
      "60 train 46 564\n",
      "Phase: train. Epoch: 60. Loss: 0.0910709947347641\n",
      "60 train 47 576\n",
      "Phase: train. Epoch: 60. Loss: 0.06819981336593628\n",
      "60 train 48 588\n",
      "Phase: train. Epoch: 60. Loss: 0.08599018305540085\n",
      "60 train 49 600\n",
      "Phase: train. Epoch: 60. Loss: 0.08803096413612366\n",
      "60 train 50 612\n",
      "Phase: train. Epoch: 60. Loss: 0.06899507343769073\n",
      "60 train 51 624\n",
      "Phase: train. Epoch: 60. Loss: 0.10558071732521057\n",
      "60 train 52 636\n",
      "Phase: train. Epoch: 60. Loss: 0.06940331310033798\n",
      "60 train 53 648\n",
      "Phase: train. Epoch: 60. Loss: 0.0605560764670372\n",
      "60 train 54 660\n",
      "Phase: train. Epoch: 60. Loss: 0.1108841523528099\n",
      "60 train 55 672\n",
      "Phase: train. Epoch: 60. Loss: 0.07324537634849548\n",
      "60 train 56 684\n",
      "Phase: train. Epoch: 60. Loss: 0.08178330957889557\n",
      "60 train 57 696\n",
      "Phase: train. Epoch: 60. Loss: 0.06934688985347748\n",
      "60 train 58 708\n",
      "Phase: train. Epoch: 60. Loss: 0.08668762445449829\n",
      "60 train 59 720\n",
      "Phase: train. Epoch: 60. Loss: 0.08920019865036011\n",
      "60 train 60 732\n",
      "Phase: train. Epoch: 60. Loss: 0.09628921747207642\n",
      "60 train 61 744\n",
      "Phase: train. Epoch: 60. Loss: 0.09853857010602951\n",
      "60 train 62 751\n",
      "Phase: train. Epoch: 60. Loss: 0.06274311989545822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 val 0 763\n",
      "Phase: val. Epoch: 60. Loss: 0.09348665177822113\n",
      "60 val 1 775\n",
      "Phase: val. Epoch: 60. Loss: 0.0938674807548523\n",
      "60 val 2 787\n",
      "Phase: val. Epoch: 60. Loss: 0.0888720378279686\n",
      "60 val 3 799\n",
      "Phase: val. Epoch: 60. Loss: 0.12743377685546875\n",
      "60 val 4 811\n",
      "Phase: val. Epoch: 60. Loss: 0.11295396089553833\n",
      "60 val 5 823\n",
      "Phase: val. Epoch: 60. Loss: 0.07639775425195694\n",
      "60 val 6 835\n",
      "Phase: val. Epoch: 60. Loss: 0.09229637682437897\n",
      "60 val 7 847\n",
      "Phase: val. Epoch: 60. Loss: 0.07711464166641235\n",
      "60 val 8 859\n",
      "Phase: val. Epoch: 60. Loss: 0.06740257143974304\n",
      "60 val 9 871\n",
      "Phase: val. Epoch: 60. Loss: 0.07119935005903244\n",
      "60 val 10 883\n",
      "Phase: val. Epoch: 60. Loss: 0.08492763340473175\n",
      "60 val 11 884\n",
      "Phase: val. Epoch: 60. Loss: 1.1639691591262817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 train 0 12\n",
      "Phase: train. Epoch: 61. Loss: 0.08422130346298218\n",
      "61 train 1 24\n",
      "Phase: train. Epoch: 61. Loss: 0.0705452486872673\n",
      "61 train 2 36\n",
      "Phase: train. Epoch: 61. Loss: 0.10001968592405319\n",
      "61 train 3 48\n",
      "Phase: train. Epoch: 61. Loss: 0.09518623352050781\n",
      "61 train 4 60\n",
      "Phase: train. Epoch: 61. Loss: 0.08049049228429794\n",
      "61 train 5 72\n",
      "Phase: train. Epoch: 61. Loss: 0.09696191549301147\n",
      "61 train 6 84\n",
      "Phase: train. Epoch: 61. Loss: 0.08216546475887299\n",
      "61 train 7 96\n",
      "Phase: train. Epoch: 61. Loss: 0.09092532098293304\n",
      "61 train 8 108\n",
      "Phase: train. Epoch: 61. Loss: 0.07496454566717148\n",
      "61 train 9 120\n",
      "Phase: train. Epoch: 61. Loss: 0.06658847630023956\n",
      "61 train 10 132\n",
      "Phase: train. Epoch: 61. Loss: 0.0810956135392189\n",
      "61 train 11 144\n",
      "Phase: train. Epoch: 61. Loss: 0.08075068891048431\n",
      "61 train 12 156\n",
      "Phase: train. Epoch: 61. Loss: 0.06976902484893799\n",
      "61 train 13 168\n",
      "Phase: train. Epoch: 61. Loss: 0.09027616679668427\n",
      "61 train 14 180\n",
      "Phase: train. Epoch: 61. Loss: 0.09509807825088501\n",
      "61 train 15 192\n",
      "Phase: train. Epoch: 61. Loss: 0.11917811632156372\n",
      "61 train 16 204\n",
      "Phase: train. Epoch: 61. Loss: 0.09367360174655914\n",
      "61 train 17 216\n",
      "Phase: train. Epoch: 61. Loss: 0.08196699619293213\n",
      "61 train 18 228\n",
      "Phase: train. Epoch: 61. Loss: 0.08815154433250427\n",
      "61 train 19 240\n",
      "Phase: train. Epoch: 61. Loss: 0.08026568591594696\n",
      "61 train 20 252\n",
      "Phase: train. Epoch: 61. Loss: 0.07769973576068878\n",
      "61 train 21 264\n",
      "Phase: train. Epoch: 61. Loss: 0.08871603012084961\n",
      "61 train 22 276\n",
      "Phase: train. Epoch: 61. Loss: 0.08004868030548096\n",
      "61 train 23 288\n",
      "Phase: train. Epoch: 61. Loss: 0.08329179883003235\n",
      "61 train 24 300\n",
      "Phase: train. Epoch: 61. Loss: 0.0706043541431427\n",
      "61 train 25 312\n",
      "Phase: train. Epoch: 61. Loss: 0.07088826596736908\n",
      "61 train 26 324\n",
      "Phase: train. Epoch: 61. Loss: 0.08763602375984192\n",
      "61 train 27 336\n",
      "Phase: train. Epoch: 61. Loss: 0.06243562698364258\n",
      "61 train 28 348\n",
      "Phase: train. Epoch: 61. Loss: 0.09587051719427109\n",
      "61 train 29 360\n",
      "Phase: train. Epoch: 61. Loss: 0.059450339525938034\n",
      "61 train 30 372\n",
      "Phase: train. Epoch: 61. Loss: 0.08243328332901001\n",
      "61 train 31 384\n",
      "Phase: train. Epoch: 61. Loss: 0.08939936012029648\n",
      "61 train 32 396\n",
      "Phase: train. Epoch: 61. Loss: 0.10166867822408676\n",
      "61 train 33 408\n",
      "Phase: train. Epoch: 61. Loss: 0.08389335870742798\n",
      "61 train 34 420\n",
      "Phase: train. Epoch: 61. Loss: 0.08747049421072006\n",
      "61 train 35 432\n",
      "Phase: train. Epoch: 61. Loss: 0.12244962900876999\n",
      "61 train 36 444\n",
      "Phase: train. Epoch: 61. Loss: 0.1184922456741333\n",
      "61 train 37 456\n",
      "Phase: train. Epoch: 61. Loss: 0.09037771821022034\n",
      "61 train 38 468\n",
      "Phase: train. Epoch: 61. Loss: 0.11813446134328842\n",
      "61 train 39 480\n",
      "Phase: train. Epoch: 61. Loss: 0.10554231703281403\n",
      "61 train 40 492\n",
      "Phase: train. Epoch: 61. Loss: 0.08825898170471191\n",
      "61 train 41 504\n",
      "Phase: train. Epoch: 61. Loss: 0.07960754632949829\n",
      "61 train 42 516\n",
      "Phase: train. Epoch: 61. Loss: 0.10442133992910385\n",
      "61 train 43 528\n",
      "Phase: train. Epoch: 61. Loss: 0.07328610867261887\n",
      "61 train 44 540\n",
      "Phase: train. Epoch: 61. Loss: 0.07925784587860107\n",
      "61 train 45 552\n",
      "Phase: train. Epoch: 61. Loss: 0.1349630206823349\n",
      "61 train 46 564\n",
      "Phase: train. Epoch: 61. Loss: 0.08788728713989258\n",
      "61 train 47 576\n",
      "Phase: train. Epoch: 61. Loss: 0.08851142972707748\n",
      "61 train 48 588\n",
      "Phase: train. Epoch: 61. Loss: 0.10009148716926575\n",
      "61 train 49 600\n",
      "Phase: train. Epoch: 61. Loss: 0.0998721718788147\n",
      "61 train 50 612\n",
      "Phase: train. Epoch: 61. Loss: 0.07601328194141388\n",
      "61 train 51 624\n",
      "Phase: train. Epoch: 61. Loss: 0.08346617221832275\n",
      "61 train 52 636\n",
      "Phase: train. Epoch: 61. Loss: 0.08152987062931061\n",
      "61 train 53 648\n",
      "Phase: train. Epoch: 61. Loss: 0.09066955745220184\n",
      "61 train 54 660\n",
      "Phase: train. Epoch: 61. Loss: 0.0806516706943512\n",
      "61 train 55 672\n",
      "Phase: train. Epoch: 61. Loss: 0.07076188921928406\n",
      "61 train 56 684\n",
      "Phase: train. Epoch: 61. Loss: 0.0818067118525505\n",
      "61 train 57 696\n",
      "Phase: train. Epoch: 61. Loss: 0.07331223785877228\n",
      "61 train 58 708\n",
      "Phase: train. Epoch: 61. Loss: 0.08490200340747833\n",
      "61 train 59 720\n",
      "Phase: train. Epoch: 61. Loss: 0.05544533580541611\n",
      "61 train 60 732\n",
      "Phase: train. Epoch: 61. Loss: 0.08674854040145874\n",
      "61 train 61 744\n",
      "Phase: train. Epoch: 61. Loss: 0.10413669049739838\n",
      "61 train 62 751\n",
      "Phase: train. Epoch: 61. Loss: 0.06334353983402252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 val 0 763\n",
      "Phase: val. Epoch: 61. Loss: 0.07394538819789886\n",
      "61 val 1 775\n",
      "Phase: val. Epoch: 61. Loss: 0.07556839287281036\n",
      "61 val 2 787\n",
      "Phase: val. Epoch: 61. Loss: 0.07600066065788269\n",
      "61 val 3 799\n",
      "Phase: val. Epoch: 61. Loss: 0.0836661085486412\n",
      "61 val 4 811\n",
      "Phase: val. Epoch: 61. Loss: 0.07011967897415161\n",
      "61 val 5 823\n",
      "Phase: val. Epoch: 61. Loss: 0.06837479770183563\n",
      "61 val 6 835\n",
      "Phase: val. Epoch: 61. Loss: 0.08864893764257431\n",
      "61 val 7 847\n",
      "Phase: val. Epoch: 61. Loss: 0.08351413905620575\n",
      "61 val 8 859\n",
      "Phase: val. Epoch: 61. Loss: 0.10152196884155273\n",
      "61 val 9 871\n",
      "Phase: val. Epoch: 61. Loss: 0.0906037837266922\n",
      "61 val 10 883\n",
      "Phase: val. Epoch: 61. Loss: 0.09880171716213226\n",
      "61 val 11 884\n",
      "Phase: val. Epoch: 61. Loss: 0.06170728802680969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 train 0 12\n",
      "Phase: train. Epoch: 62. Loss: 0.08565795421600342\n",
      "62 train 1 24\n",
      "Phase: train. Epoch: 62. Loss: 0.0803493857383728\n",
      "62 train 2 36\n",
      "Phase: train. Epoch: 62. Loss: 0.06532783061265945\n",
      "62 train 3 48\n",
      "Phase: train. Epoch: 62. Loss: 0.0926559567451477\n",
      "62 train 4 60\n",
      "Phase: train. Epoch: 62. Loss: 0.09874685853719711\n",
      "62 train 5 72\n",
      "Phase: train. Epoch: 62. Loss: 0.08291356265544891\n",
      "62 train 6 84\n",
      "Phase: train. Epoch: 62. Loss: 0.10219073295593262\n",
      "62 train 7 96\n",
      "Phase: train. Epoch: 62. Loss: 0.07434441149234772\n",
      "62 train 8 108\n",
      "Phase: train. Epoch: 62. Loss: 0.09744440019130707\n",
      "62 train 9 120\n",
      "Phase: train. Epoch: 62. Loss: 0.08748408406972885\n",
      "62 train 10 132\n",
      "Phase: train. Epoch: 62. Loss: 0.08176913857460022\n",
      "62 train 11 144\n",
      "Phase: train. Epoch: 62. Loss: 0.09253339469432831\n",
      "62 train 12 156\n",
      "Phase: train. Epoch: 62. Loss: 0.10383252799510956\n",
      "62 train 13 168\n",
      "Phase: train. Epoch: 62. Loss: 0.08423014730215073\n",
      "62 train 14 180\n",
      "Phase: train. Epoch: 62. Loss: 0.06834738701581955\n",
      "62 train 15 192\n",
      "Phase: train. Epoch: 62. Loss: 0.08697395026683807\n",
      "62 train 16 204\n",
      "Phase: train. Epoch: 62. Loss: 0.07338353246450424\n",
      "62 train 17 216\n",
      "Phase: train. Epoch: 62. Loss: 0.09777404367923737\n",
      "62 train 18 228\n",
      "Phase: train. Epoch: 62. Loss: 0.08222237229347229\n",
      "62 train 19 240\n",
      "Phase: train. Epoch: 62. Loss: 0.06997442245483398\n",
      "62 train 20 252\n",
      "Phase: train. Epoch: 62. Loss: 0.08089866489171982\n",
      "62 train 21 264\n",
      "Phase: train. Epoch: 62. Loss: 0.08749493956565857\n",
      "62 train 22 276\n",
      "Phase: train. Epoch: 62. Loss: 0.0650770515203476\n",
      "62 train 23 288\n",
      "Phase: train. Epoch: 62. Loss: 0.06753410398960114\n",
      "62 train 24 300\n",
      "Phase: train. Epoch: 62. Loss: 0.07951725274324417\n",
      "62 train 25 312\n",
      "Phase: train. Epoch: 62. Loss: 0.08062419295310974\n",
      "62 train 26 324\n",
      "Phase: train. Epoch: 62. Loss: 0.10000042617321014\n",
      "62 train 27 336\n",
      "Phase: train. Epoch: 62. Loss: 0.10211725533008575\n",
      "62 train 28 348\n",
      "Phase: train. Epoch: 62. Loss: 0.11274506896734238\n",
      "62 train 29 360\n",
      "Phase: train. Epoch: 62. Loss: 0.10474076122045517\n",
      "62 train 30 372\n",
      "Phase: train. Epoch: 62. Loss: 0.09016925096511841\n",
      "62 train 31 384\n",
      "Phase: train. Epoch: 62. Loss: 0.11156543344259262\n",
      "62 train 32 396\n",
      "Phase: train. Epoch: 62. Loss: 0.07339435815811157\n",
      "62 train 33 408\n",
      "Phase: train. Epoch: 62. Loss: 0.070515476167202\n",
      "62 train 34 420\n",
      "Phase: train. Epoch: 62. Loss: 0.09953731298446655\n",
      "62 train 35 432\n",
      "Phase: train. Epoch: 62. Loss: 0.10215513408184052\n",
      "62 train 36 444\n",
      "Phase: train. Epoch: 62. Loss: 0.07218372821807861\n",
      "62 train 37 456\n",
      "Phase: train. Epoch: 62. Loss: 0.06715182960033417\n",
      "62 train 38 468\n",
      "Phase: train. Epoch: 62. Loss: 0.0821000263094902\n",
      "62 train 39 480\n",
      "Phase: train. Epoch: 62. Loss: 0.07308971881866455\n",
      "62 train 40 492\n",
      "Phase: train. Epoch: 62. Loss: 0.1085161566734314\n",
      "62 train 41 504\n",
      "Phase: train. Epoch: 62. Loss: 0.07030001282691956\n",
      "62 train 42 516\n",
      "Phase: train. Epoch: 62. Loss: 0.10553567111492157\n",
      "62 train 43 528\n",
      "Phase: train. Epoch: 62. Loss: 0.07398644089698792\n",
      "62 train 44 540\n",
      "Phase: train. Epoch: 62. Loss: 0.08014043420553207\n",
      "62 train 45 552\n",
      "Phase: train. Epoch: 62. Loss: 0.0667608305811882\n",
      "62 train 46 564\n",
      "Phase: train. Epoch: 62. Loss: 0.09779336303472519\n",
      "62 train 47 576\n",
      "Phase: train. Epoch: 62. Loss: 0.10965991020202637\n",
      "62 train 48 588\n",
      "Phase: train. Epoch: 62. Loss: 0.09779481589794159\n",
      "62 train 49 600\n",
      "Phase: train. Epoch: 62. Loss: 0.09663605690002441\n",
      "62 train 50 612\n",
      "Phase: train. Epoch: 62. Loss: 0.0788390263915062\n",
      "62 train 51 624\n",
      "Phase: train. Epoch: 62. Loss: 0.08117605745792389\n",
      "62 train 52 636\n",
      "Phase: train. Epoch: 62. Loss: 0.10600753873586655\n",
      "62 train 53 648\n",
      "Phase: train. Epoch: 62. Loss: 0.07700829952955246\n",
      "62 train 54 660\n",
      "Phase: train. Epoch: 62. Loss: 0.06646271049976349\n",
      "62 train 55 672\n",
      "Phase: train. Epoch: 62. Loss: 0.11802419275045395\n",
      "62 train 56 684\n",
      "Phase: train. Epoch: 62. Loss: 0.06795208156108856\n",
      "62 train 57 696\n",
      "Phase: train. Epoch: 62. Loss: 0.08630047738552094\n",
      "62 train 58 708\n",
      "Phase: train. Epoch: 62. Loss: 0.09228744357824326\n",
      "62 train 59 720\n",
      "Phase: train. Epoch: 62. Loss: 0.08396735787391663\n",
      "62 train 60 732\n",
      "Phase: train. Epoch: 62. Loss: 0.0707612931728363\n",
      "62 train 61 744\n",
      "Phase: train. Epoch: 62. Loss: 0.08152899891138077\n",
      "62 train 62 751\n",
      "Phase: train. Epoch: 62. Loss: 0.14248912036418915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 val 0 763\n",
      "Phase: val. Epoch: 62. Loss: 0.05710792541503906\n",
      "62 val 1 775\n",
      "Phase: val. Epoch: 62. Loss: 0.09515899419784546\n",
      "62 val 2 787\n",
      "Phase: val. Epoch: 62. Loss: 0.05343624949455261\n",
      "62 val 3 799\n",
      "Phase: val. Epoch: 62. Loss: 0.09725433588027954\n",
      "62 val 4 811\n",
      "Phase: val. Epoch: 62. Loss: 0.10740560293197632\n",
      "62 val 5 823\n",
      "Phase: val. Epoch: 62. Loss: 0.08448135852813721\n",
      "62 val 6 835\n",
      "Phase: val. Epoch: 62. Loss: 0.09548287093639374\n",
      "62 val 7 847\n",
      "Phase: val. Epoch: 62. Loss: 0.07086403667926788\n",
      "62 val 8 859\n",
      "Phase: val. Epoch: 62. Loss: 0.07574807852506638\n",
      "62 val 9 871\n",
      "Phase: val. Epoch: 62. Loss: 0.08829528093338013\n",
      "62 val 10 883\n",
      "Phase: val. Epoch: 62. Loss: 0.08198799192905426\n",
      "62 val 11 884\n",
      "Phase: val. Epoch: 62. Loss: 0.17549730837345123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 train 0 12\n",
      "Phase: train. Epoch: 63. Loss: 0.09408265352249146\n",
      "63 train 1 24\n",
      "Phase: train. Epoch: 63. Loss: 0.08335642516613007\n",
      "63 train 2 36\n",
      "Phase: train. Epoch: 63. Loss: 0.07175150513648987\n",
      "63 train 3 48\n",
      "Phase: train. Epoch: 63. Loss: 0.07594161480665207\n",
      "63 train 4 60\n",
      "Phase: train. Epoch: 63. Loss: 0.07027082145214081\n",
      "63 train 5 72\n",
      "Phase: train. Epoch: 63. Loss: 0.09522511065006256\n",
      "63 train 6 84\n",
      "Phase: train. Epoch: 63. Loss: 0.09704726934432983\n",
      "63 train 7 96\n",
      "Phase: train. Epoch: 63. Loss: 0.08863545954227448\n",
      "63 train 8 108\n",
      "Phase: train. Epoch: 63. Loss: 0.062316324561834335\n",
      "63 train 9 120\n",
      "Phase: train. Epoch: 63. Loss: 0.12059357762336731\n",
      "63 train 10 132\n",
      "Phase: train. Epoch: 63. Loss: 0.07787396013736725\n",
      "63 train 11 144\n",
      "Phase: train. Epoch: 63. Loss: 0.08320344984531403\n",
      "63 train 12 156\n",
      "Phase: train. Epoch: 63. Loss: 0.08368104696273804\n",
      "63 train 13 168\n",
      "Phase: train. Epoch: 63. Loss: 0.07314060628414154\n",
      "63 train 14 180\n",
      "Phase: train. Epoch: 63. Loss: 0.08991135656833649\n",
      "63 train 15 192\n",
      "Phase: train. Epoch: 63. Loss: 0.07807586342096329\n",
      "63 train 16 204\n",
      "Phase: train. Epoch: 63. Loss: 0.0829886868596077\n",
      "63 train 17 216\n",
      "Phase: train. Epoch: 63. Loss: 0.08848437666893005\n",
      "63 train 18 228\n",
      "Phase: train. Epoch: 63. Loss: 0.11366390436887741\n",
      "63 train 19 240\n",
      "Phase: train. Epoch: 63. Loss: 0.07779695093631744\n",
      "63 train 20 252\n",
      "Phase: train. Epoch: 63. Loss: 0.09630128741264343\n",
      "63 train 21 264\n",
      "Phase: train. Epoch: 63. Loss: 0.08728816360235214\n",
      "63 train 22 276\n",
      "Phase: train. Epoch: 63. Loss: 0.09138807654380798\n",
      "63 train 23 288\n",
      "Phase: train. Epoch: 63. Loss: 0.12983505427837372\n",
      "63 train 24 300\n",
      "Phase: train. Epoch: 63. Loss: 0.09994524717330933\n",
      "63 train 25 312\n",
      "Phase: train. Epoch: 63. Loss: 0.0877794474363327\n",
      "63 train 26 324\n",
      "Phase: train. Epoch: 63. Loss: 0.10114994645118713\n",
      "63 train 27 336\n",
      "Phase: train. Epoch: 63. Loss: 0.07151192426681519\n",
      "63 train 28 348\n",
      "Phase: train. Epoch: 63. Loss: 0.0814325213432312\n",
      "63 train 29 360\n",
      "Phase: train. Epoch: 63. Loss: 0.10204349458217621\n",
      "63 train 30 372\n",
      "Phase: train. Epoch: 63. Loss: 0.10605137050151825\n",
      "63 train 31 384\n",
      "Phase: train. Epoch: 63. Loss: 0.06678213179111481\n",
      "63 train 32 396\n",
      "Phase: train. Epoch: 63. Loss: 0.09969313442707062\n",
      "63 train 33 408\n",
      "Phase: train. Epoch: 63. Loss: 0.08370902389287949\n",
      "63 train 34 420\n",
      "Phase: train. Epoch: 63. Loss: 0.08914031833410263\n",
      "63 train 35 432\n",
      "Phase: train. Epoch: 63. Loss: 0.06795559823513031\n",
      "63 train 36 444\n",
      "Phase: train. Epoch: 63. Loss: 0.07458101212978363\n",
      "63 train 37 456\n",
      "Phase: train. Epoch: 63. Loss: 0.0684998482465744\n",
      "63 train 38 468\n",
      "Phase: train. Epoch: 63. Loss: 0.0757804661989212\n",
      "63 train 39 480\n",
      "Phase: train. Epoch: 63. Loss: 0.061631202697753906\n",
      "63 train 40 492\n",
      "Phase: train. Epoch: 63. Loss: 0.08168453723192215\n",
      "63 train 41 504\n",
      "Phase: train. Epoch: 63. Loss: 0.08241574466228485\n",
      "63 train 42 516\n",
      "Phase: train. Epoch: 63. Loss: 0.08552297949790955\n",
      "63 train 43 528\n",
      "Phase: train. Epoch: 63. Loss: 0.08050771057605743\n",
      "63 train 44 540\n",
      "Phase: train. Epoch: 63. Loss: 0.09379170835018158\n",
      "63 train 45 552\n",
      "Phase: train. Epoch: 63. Loss: 0.0557420551776886\n",
      "63 train 46 564\n",
      "Phase: train. Epoch: 63. Loss: 0.09669692814350128\n",
      "63 train 47 576\n",
      "Phase: train. Epoch: 63. Loss: 0.09266002476215363\n",
      "63 train 48 588\n",
      "Phase: train. Epoch: 63. Loss: 0.10103631019592285\n",
      "63 train 49 600\n",
      "Phase: train. Epoch: 63. Loss: 0.09238815307617188\n",
      "63 train 50 612\n",
      "Phase: train. Epoch: 63. Loss: 0.0866699144244194\n",
      "63 train 51 624\n",
      "Phase: train. Epoch: 63. Loss: 0.08439280837774277\n",
      "63 train 52 636\n",
      "Phase: train. Epoch: 63. Loss: 0.09731924533843994\n",
      "63 train 53 648\n",
      "Phase: train. Epoch: 63. Loss: 0.09585905075073242\n",
      "63 train 54 660\n",
      "Phase: train. Epoch: 63. Loss: 0.06135515868663788\n",
      "63 train 55 672\n",
      "Phase: train. Epoch: 63. Loss: 0.09598924219608307\n",
      "63 train 56 684\n",
      "Phase: train. Epoch: 63. Loss: 0.10289981216192245\n",
      "63 train 57 696\n",
      "Phase: train. Epoch: 63. Loss: 0.07325269281864166\n",
      "63 train 58 708\n",
      "Phase: train. Epoch: 63. Loss: 0.05752307176589966\n",
      "63 train 59 720\n",
      "Phase: train. Epoch: 63. Loss: 0.09044480323791504\n",
      "63 train 60 732\n",
      "Phase: train. Epoch: 63. Loss: 0.08333860337734222\n",
      "63 train 61 744\n",
      "Phase: train. Epoch: 63. Loss: 0.11631891131401062\n",
      "63 train 62 751\n",
      "Phase: train. Epoch: 63. Loss: 0.09027094393968582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 val 0 763\n",
      "Phase: val. Epoch: 63. Loss: 0.11054937541484833\n",
      "63 val 1 775\n",
      "Phase: val. Epoch: 63. Loss: 0.06439948081970215\n",
      "63 val 2 787\n",
      "Phase: val. Epoch: 63. Loss: 0.07926350831985474\n",
      "63 val 3 799\n",
      "Phase: val. Epoch: 63. Loss: 0.11235283315181732\n",
      "63 val 4 811\n",
      "Phase: val. Epoch: 63. Loss: 0.06541217863559723\n",
      "63 val 5 823\n",
      "Phase: val. Epoch: 63. Loss: 0.06345048546791077\n",
      "63 val 6 835\n",
      "Phase: val. Epoch: 63. Loss: 0.07128462195396423\n",
      "63 val 7 847\n",
      "Phase: val. Epoch: 63. Loss: 0.06220698356628418\n",
      "63 val 8 859\n",
      "Phase: val. Epoch: 63. Loss: 0.13302218914031982\n",
      "63 val 9 871\n",
      "Phase: val. Epoch: 63. Loss: 0.0707765743136406\n",
      "63 val 10 883\n",
      "Phase: val. Epoch: 63. Loss: 0.08226685225963593\n",
      "63 val 11 884\n",
      "Phase: val. Epoch: 63. Loss: 0.02786077931523323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 train 0 12\n",
      "Phase: train. Epoch: 64. Loss: 0.07370256632566452\n",
      "64 train 1 24\n",
      "Phase: train. Epoch: 64. Loss: 0.09170006215572357\n",
      "64 train 2 36\n",
      "Phase: train. Epoch: 64. Loss: 0.0802254006266594\n",
      "64 train 3 48\n",
      "Phase: train. Epoch: 64. Loss: 0.08110886812210083\n",
      "64 train 4 60\n",
      "Phase: train. Epoch: 64. Loss: 0.10405545681715012\n",
      "64 train 5 72\n",
      "Phase: train. Epoch: 64. Loss: 0.08057505637407303\n",
      "64 train 6 84\n",
      "Phase: train. Epoch: 64. Loss: 0.07996992766857147\n",
      "64 train 7 96\n",
      "Phase: train. Epoch: 64. Loss: 0.09393948316574097\n",
      "64 train 8 108\n",
      "Phase: train. Epoch: 64. Loss: 0.09882703423500061\n",
      "64 train 9 120\n",
      "Phase: train. Epoch: 64. Loss: 0.07925926893949509\n",
      "64 train 10 132\n",
      "Phase: train. Epoch: 64. Loss: 0.06878688186407089\n",
      "64 train 11 144\n",
      "Phase: train. Epoch: 64. Loss: 0.08417733758687973\n",
      "64 train 12 156\n",
      "Phase: train. Epoch: 64. Loss: 0.08532416820526123\n",
      "64 train 13 168\n",
      "Phase: train. Epoch: 64. Loss: 0.08500291407108307\n",
      "64 train 14 180\n",
      "Phase: train. Epoch: 64. Loss: 0.07936232537031174\n",
      "64 train 15 192\n",
      "Phase: train. Epoch: 64. Loss: 0.08451293408870697\n",
      "64 train 16 204\n",
      "Phase: train. Epoch: 64. Loss: 0.08895470947027206\n",
      "64 train 17 216\n",
      "Phase: train. Epoch: 64. Loss: 0.11821061372756958\n",
      "64 train 18 228\n",
      "Phase: train. Epoch: 64. Loss: 0.06846369802951813\n",
      "64 train 19 240\n",
      "Phase: train. Epoch: 64. Loss: 0.08023682236671448\n",
      "64 train 20 252\n",
      "Phase: train. Epoch: 64. Loss: 0.09108027070760727\n",
      "64 train 21 264\n",
      "Phase: train. Epoch: 64. Loss: 0.11271528899669647\n",
      "64 train 22 276\n",
      "Phase: train. Epoch: 64. Loss: 0.07553272694349289\n",
      "64 train 23 288\n",
      "Phase: train. Epoch: 64. Loss: 0.08837460726499557\n",
      "64 train 24 300\n",
      "Phase: train. Epoch: 64. Loss: 0.07403135299682617\n",
      "64 train 25 312\n",
      "Phase: train. Epoch: 64. Loss: 0.08186602592468262\n",
      "64 train 26 324\n",
      "Phase: train. Epoch: 64. Loss: 0.0775362178683281\n",
      "64 train 27 336\n",
      "Phase: train. Epoch: 64. Loss: 0.0960414856672287\n",
      "64 train 28 348\n",
      "Phase: train. Epoch: 64. Loss: 0.08599750697612762\n",
      "64 train 29 360\n",
      "Phase: train. Epoch: 64. Loss: 0.08081318438053131\n",
      "64 train 30 372\n",
      "Phase: train. Epoch: 64. Loss: 0.08257444947957993\n",
      "64 train 31 384\n",
      "Phase: train. Epoch: 64. Loss: 0.07750394195318222\n",
      "64 train 32 396\n",
      "Phase: train. Epoch: 64. Loss: 0.05996058136224747\n",
      "64 train 33 408\n",
      "Phase: train. Epoch: 64. Loss: 0.11309047788381577\n",
      "64 train 34 420\n",
      "Phase: train. Epoch: 64. Loss: 0.08223512768745422\n",
      "64 train 35 432\n",
      "Phase: train. Epoch: 64. Loss: 0.08139174431562424\n",
      "64 train 36 444\n",
      "Phase: train. Epoch: 64. Loss: 0.10834597051143646\n",
      "64 train 37 456\n",
      "Phase: train. Epoch: 64. Loss: 0.08013751357793808\n",
      "64 train 38 468\n",
      "Phase: train. Epoch: 64. Loss: 0.07796035706996918\n",
      "64 train 39 480\n",
      "Phase: train. Epoch: 64. Loss: 0.1005667895078659\n",
      "64 train 40 492\n",
      "Phase: train. Epoch: 64. Loss: 0.0821177065372467\n",
      "64 train 41 504\n",
      "Phase: train. Epoch: 64. Loss: 0.08257680386304855\n",
      "64 train 42 516\n",
      "Phase: train. Epoch: 64. Loss: 0.12661199271678925\n",
      "64 train 43 528\n",
      "Phase: train. Epoch: 64. Loss: 0.09038351476192474\n",
      "64 train 44 540\n",
      "Phase: train. Epoch: 64. Loss: 0.07657657563686371\n",
      "64 train 45 552\n",
      "Phase: train. Epoch: 64. Loss: 0.09204402565956116\n",
      "64 train 46 564\n",
      "Phase: train. Epoch: 64. Loss: 0.07050088047981262\n",
      "64 train 47 576\n",
      "Phase: train. Epoch: 64. Loss: 0.06930021941661835\n",
      "64 train 48 588\n",
      "Phase: train. Epoch: 64. Loss: 0.07319645583629608\n",
      "64 train 49 600\n",
      "Phase: train. Epoch: 64. Loss: 0.08205939084291458\n",
      "64 train 50 612\n",
      "Phase: train. Epoch: 64. Loss: 0.12579447031021118\n",
      "64 train 51 624\n",
      "Phase: train. Epoch: 64. Loss: 0.07767094671726227\n",
      "64 train 52 636\n",
      "Phase: train. Epoch: 64. Loss: 0.07750892639160156\n",
      "64 train 53 648\n",
      "Phase: train. Epoch: 64. Loss: 0.08230531215667725\n",
      "64 train 54 660\n",
      "Phase: train. Epoch: 64. Loss: 0.0746612548828125\n",
      "64 train 55 672\n",
      "Phase: train. Epoch: 64. Loss: 0.11214083433151245\n",
      "64 train 56 684\n",
      "Phase: train. Epoch: 64. Loss: 0.09804316610097885\n",
      "64 train 57 696\n",
      "Phase: train. Epoch: 64. Loss: 0.0872230976819992\n",
      "64 train 58 708\n",
      "Phase: train. Epoch: 64. Loss: 0.09480434656143188\n",
      "64 train 59 720\n",
      "Phase: train. Epoch: 64. Loss: 0.08968287706375122\n",
      "64 train 60 732\n",
      "Phase: train. Epoch: 64. Loss: 0.06805703043937683\n",
      "64 train 61 744\n",
      "Phase: train. Epoch: 64. Loss: 0.08317530155181885\n",
      "64 train 62 751\n",
      "Phase: train. Epoch: 64. Loss: 0.07830546796321869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 val 0 763\n",
      "Phase: val. Epoch: 64. Loss: 0.07990698516368866\n",
      "64 val 1 775\n",
      "Phase: val. Epoch: 64. Loss: 0.06400559842586517\n",
      "64 val 2 787\n",
      "Phase: val. Epoch: 64. Loss: 0.08322149515151978\n",
      "64 val 3 799\n",
      "Phase: val. Epoch: 64. Loss: 0.07711924612522125\n",
      "64 val 4 811\n",
      "Phase: val. Epoch: 64. Loss: 0.11448037624359131\n",
      "64 val 5 823\n",
      "Phase: val. Epoch: 64. Loss: 0.11246808618307114\n",
      "64 val 6 835\n",
      "Phase: val. Epoch: 64. Loss: 0.0809478908777237\n",
      "64 val 7 847\n",
      "Phase: val. Epoch: 64. Loss: 0.05971818417310715\n",
      "64 val 8 859\n",
      "Phase: val. Epoch: 64. Loss: 0.06421595811843872\n",
      "64 val 9 871\n",
      "Phase: val. Epoch: 64. Loss: 0.10044781863689423\n",
      "64 val 10 883\n",
      "Phase: val. Epoch: 64. Loss: 0.07736624777317047\n",
      "64 val 11 884\n",
      "Phase: val. Epoch: 64. Loss: 0.2425859272480011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 train 0 12\n",
      "Phase: train. Epoch: 65. Loss: 0.0952591747045517\n",
      "65 train 1 24\n",
      "Phase: train. Epoch: 65. Loss: 0.09820888936519623\n",
      "65 train 2 36\n",
      "Phase: train. Epoch: 65. Loss: 0.06687511503696442\n",
      "65 train 3 48\n",
      "Phase: train. Epoch: 65. Loss: 0.0770481675863266\n",
      "65 train 4 60\n",
      "Phase: train. Epoch: 65. Loss: 0.08612941205501556\n",
      "65 train 5 72\n",
      "Phase: train. Epoch: 65. Loss: 0.09179064631462097\n",
      "65 train 6 84\n",
      "Phase: train. Epoch: 65. Loss: 0.0773712545633316\n",
      "65 train 7 96\n",
      "Phase: train. Epoch: 65. Loss: 0.09575022757053375\n",
      "65 train 8 108\n",
      "Phase: train. Epoch: 65. Loss: 0.08893793076276779\n",
      "65 train 9 120\n",
      "Phase: train. Epoch: 65. Loss: 0.0832555741071701\n",
      "65 train 10 132\n",
      "Phase: train. Epoch: 65. Loss: 0.10332803428173065\n",
      "65 train 11 144\n",
      "Phase: train. Epoch: 65. Loss: 0.07409268617630005\n",
      "65 train 12 156\n",
      "Phase: train. Epoch: 65. Loss: 0.0638098195195198\n",
      "65 train 13 168\n",
      "Phase: train. Epoch: 65. Loss: 0.08472522348165512\n",
      "65 train 14 180\n",
      "Phase: train. Epoch: 65. Loss: 0.06952831149101257\n",
      "65 train 15 192\n",
      "Phase: train. Epoch: 65. Loss: 0.10413818806409836\n",
      "65 train 16 204\n",
      "Phase: train. Epoch: 65. Loss: 0.06309208273887634\n",
      "65 train 17 216\n",
      "Phase: train. Epoch: 65. Loss: 0.09355257451534271\n",
      "65 train 18 228\n",
      "Phase: train. Epoch: 65. Loss: 0.07730501145124435\n",
      "65 train 19 240\n",
      "Phase: train. Epoch: 65. Loss: 0.09302537143230438\n",
      "65 train 20 252\n",
      "Phase: train. Epoch: 65. Loss: 0.1022840365767479\n",
      "65 train 21 264\n",
      "Phase: train. Epoch: 65. Loss: 0.09435904026031494\n",
      "65 train 22 276\n",
      "Phase: train. Epoch: 65. Loss: 0.09340746700763702\n",
      "65 train 23 288\n",
      "Phase: train. Epoch: 65. Loss: 0.08442066609859467\n",
      "65 train 24 300\n",
      "Phase: train. Epoch: 65. Loss: 0.09591738879680634\n",
      "65 train 25 312\n",
      "Phase: train. Epoch: 65. Loss: 0.08186889439821243\n",
      "65 train 26 324\n",
      "Phase: train. Epoch: 65. Loss: 0.10027045011520386\n",
      "65 train 27 336\n",
      "Phase: train. Epoch: 65. Loss: 0.075080007314682\n",
      "65 train 28 348\n",
      "Phase: train. Epoch: 65. Loss: 0.07095380872488022\n",
      "65 train 29 360\n",
      "Phase: train. Epoch: 65. Loss: 0.07488488405942917\n",
      "65 train 30 372\n",
      "Phase: train. Epoch: 65. Loss: 0.07916411012411118\n",
      "65 train 31 384\n",
      "Phase: train. Epoch: 65. Loss: 0.06851624697446823\n",
      "65 train 32 396\n",
      "Phase: train. Epoch: 65. Loss: 0.07022562623023987\n",
      "65 train 33 408\n",
      "Phase: train. Epoch: 65. Loss: 0.0888666957616806\n",
      "65 train 34 420\n",
      "Phase: train. Epoch: 65. Loss: 0.0903560146689415\n",
      "65 train 35 432\n",
      "Phase: train. Epoch: 65. Loss: 0.08773180842399597\n",
      "65 train 36 444\n",
      "Phase: train. Epoch: 65. Loss: 0.06927426159381866\n",
      "65 train 37 456\n",
      "Phase: train. Epoch: 65. Loss: 0.08304552733898163\n",
      "65 train 38 468\n",
      "Phase: train. Epoch: 65. Loss: 0.0941154882311821\n",
      "65 train 39 480\n",
      "Phase: train. Epoch: 65. Loss: 0.07552161812782288\n",
      "65 train 40 492\n",
      "Phase: train. Epoch: 65. Loss: 0.07765154540538788\n",
      "65 train 41 504\n",
      "Phase: train. Epoch: 65. Loss: 0.07224901020526886\n",
      "65 train 42 516\n",
      "Phase: train. Epoch: 65. Loss: 0.09377793967723846\n",
      "65 train 43 528\n",
      "Phase: train. Epoch: 65. Loss: 0.0863962173461914\n",
      "65 train 44 540\n",
      "Phase: train. Epoch: 65. Loss: 0.08280768245458603\n",
      "65 train 45 552\n",
      "Phase: train. Epoch: 65. Loss: 0.09365754574537277\n",
      "65 train 46 564\n",
      "Phase: train. Epoch: 65. Loss: 0.11967417597770691\n",
      "65 train 47 576\n",
      "Phase: train. Epoch: 65. Loss: 0.09082937240600586\n",
      "65 train 48 588\n",
      "Phase: train. Epoch: 65. Loss: 0.08258244395256042\n",
      "65 train 49 600\n",
      "Phase: train. Epoch: 65. Loss: 0.08451715111732483\n",
      "65 train 50 612\n",
      "Phase: train. Epoch: 65. Loss: 0.05844176933169365\n",
      "65 train 51 624\n",
      "Phase: train. Epoch: 65. Loss: 0.08653410524129868\n",
      "65 train 52 636\n",
      "Phase: train. Epoch: 65. Loss: 0.10207536816596985\n",
      "65 train 53 648\n",
      "Phase: train. Epoch: 65. Loss: 0.09236957877874374\n",
      "65 train 54 660\n",
      "Phase: train. Epoch: 65. Loss: 0.09713076800107956\n",
      "65 train 55 672\n",
      "Phase: train. Epoch: 65. Loss: 0.08668394386768341\n",
      "65 train 56 684\n",
      "Phase: train. Epoch: 65. Loss: 0.09294416755437851\n",
      "65 train 57 696\n",
      "Phase: train. Epoch: 65. Loss: 0.06513449549674988\n",
      "65 train 58 708\n",
      "Phase: train. Epoch: 65. Loss: 0.08756228536367416\n",
      "65 train 59 720\n",
      "Phase: train. Epoch: 65. Loss: 0.07852011173963547\n",
      "65 train 60 732\n",
      "Phase: train. Epoch: 65. Loss: 0.08205468952655792\n",
      "65 train 61 744\n",
      "Phase: train. Epoch: 65. Loss: 0.09559480845928192\n",
      "65 train 62 751\n",
      "Phase: train. Epoch: 65. Loss: 0.0707482099533081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 val 0 763\n",
      "Phase: val. Epoch: 65. Loss: 0.08608575910329819\n",
      "65 val 1 775\n",
      "Phase: val. Epoch: 65. Loss: 0.06247331202030182\n",
      "65 val 2 787\n",
      "Phase: val. Epoch: 65. Loss: 0.07482701539993286\n",
      "65 val 3 799\n",
      "Phase: val. Epoch: 65. Loss: 0.07742448896169662\n",
      "65 val 4 811\n",
      "Phase: val. Epoch: 65. Loss: 0.06311342120170593\n",
      "65 val 5 823\n",
      "Phase: val. Epoch: 65. Loss: 0.11309054493904114\n",
      "65 val 6 835\n",
      "Phase: val. Epoch: 65. Loss: 0.0991348847746849\n",
      "65 val 7 847\n",
      "Phase: val. Epoch: 65. Loss: 0.08593165129423141\n",
      "65 val 8 859\n",
      "Phase: val. Epoch: 65. Loss: 0.0740615501999855\n",
      "65 val 9 871\n",
      "Phase: val. Epoch: 65. Loss: 0.09309907257556915\n",
      "65 val 10 883\n",
      "Phase: val. Epoch: 65. Loss: 0.09063367545604706\n",
      "65 val 11 884\n",
      "Phase: val. Epoch: 65. Loss: 0.026085108518600464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 train 0 12\n",
      "Phase: train. Epoch: 66. Loss: 0.08900611102581024\n",
      "66 train 1 24\n",
      "Phase: train. Epoch: 66. Loss: 0.0828881561756134\n",
      "66 train 2 36\n",
      "Phase: train. Epoch: 66. Loss: 0.11320077627897263\n",
      "66 train 3 48\n",
      "Phase: train. Epoch: 66. Loss: 0.10659679025411606\n",
      "66 train 4 60\n",
      "Phase: train. Epoch: 66. Loss: 0.08210834115743637\n",
      "66 train 5 72\n",
      "Phase: train. Epoch: 66. Loss: 0.08419758081436157\n",
      "66 train 6 84\n",
      "Phase: train. Epoch: 66. Loss: 0.10079902410507202\n",
      "66 train 7 96\n",
      "Phase: train. Epoch: 66. Loss: 0.07416556775569916\n",
      "66 train 8 108\n",
      "Phase: train. Epoch: 66. Loss: 0.08256018161773682\n",
      "66 train 9 120\n",
      "Phase: train. Epoch: 66. Loss: 0.08277375251054764\n",
      "66 train 10 132\n",
      "Phase: train. Epoch: 66. Loss: 0.07073242962360382\n",
      "66 train 11 144\n",
      "Phase: train. Epoch: 66. Loss: 0.08444135636091232\n",
      "66 train 12 156\n",
      "Phase: train. Epoch: 66. Loss: 0.06876745074987411\n",
      "66 train 13 168\n",
      "Phase: train. Epoch: 66. Loss: 0.1224663108587265\n",
      "66 train 14 180\n",
      "Phase: train. Epoch: 66. Loss: 0.0659216120839119\n",
      "66 train 15 192\n",
      "Phase: train. Epoch: 66. Loss: 0.07087491452693939\n",
      "66 train 16 204\n",
      "Phase: train. Epoch: 66. Loss: 0.10334713757038116\n",
      "66 train 17 216\n",
      "Phase: train. Epoch: 66. Loss: 0.09281782805919647\n",
      "66 train 18 228\n",
      "Phase: train. Epoch: 66. Loss: 0.1212393268942833\n",
      "66 train 19 240\n",
      "Phase: train. Epoch: 66. Loss: 0.08541115373373032\n",
      "66 train 20 252\n",
      "Phase: train. Epoch: 66. Loss: 0.07561951875686646\n",
      "66 train 21 264\n",
      "Phase: train. Epoch: 66. Loss: 0.06475266069173813\n",
      "66 train 22 276\n",
      "Phase: train. Epoch: 66. Loss: 0.12578146159648895\n",
      "66 train 23 288\n",
      "Phase: train. Epoch: 66. Loss: 0.07445792853832245\n",
      "66 train 24 300\n",
      "Phase: train. Epoch: 66. Loss: 0.08049973100423813\n",
      "66 train 25 312\n",
      "Phase: train. Epoch: 66. Loss: 0.09121998399496078\n",
      "66 train 26 324\n",
      "Phase: train. Epoch: 66. Loss: 0.07166914641857147\n",
      "66 train 27 336\n",
      "Phase: train. Epoch: 66. Loss: 0.09442009776830673\n",
      "66 train 28 348\n",
      "Phase: train. Epoch: 66. Loss: 0.08064116537570953\n",
      "66 train 29 360\n",
      "Phase: train. Epoch: 66. Loss: 0.08352907747030258\n",
      "66 train 30 372\n",
      "Phase: train. Epoch: 66. Loss: 0.08728639781475067\n",
      "66 train 31 384\n",
      "Phase: train. Epoch: 66. Loss: 0.06276802718639374\n",
      "66 train 32 396\n",
      "Phase: train. Epoch: 66. Loss: 0.10128980875015259\n",
      "66 train 33 408\n",
      "Phase: train. Epoch: 66. Loss: 0.06549414247274399\n",
      "66 train 34 420\n",
      "Phase: train. Epoch: 66. Loss: 0.11251688003540039\n",
      "66 train 35 432\n",
      "Phase: train. Epoch: 66. Loss: 0.11076956987380981\n",
      "66 train 36 444\n",
      "Phase: train. Epoch: 66. Loss: 0.07426699250936508\n",
      "66 train 37 456\n",
      "Phase: train. Epoch: 66. Loss: 0.08354014158248901\n",
      "66 train 38 468\n",
      "Phase: train. Epoch: 66. Loss: 0.0828031376004219\n",
      "66 train 39 480\n",
      "Phase: train. Epoch: 66. Loss: 0.07784421741962433\n",
      "66 train 40 492\n",
      "Phase: train. Epoch: 66. Loss: 0.056488730013370514\n",
      "66 train 41 504\n",
      "Phase: train. Epoch: 66. Loss: 0.10588189214468002\n",
      "66 train 42 516\n",
      "Phase: train. Epoch: 66. Loss: 0.0769379734992981\n",
      "66 train 43 528\n",
      "Phase: train. Epoch: 66. Loss: 0.06813076138496399\n",
      "66 train 44 540\n",
      "Phase: train. Epoch: 66. Loss: 0.07230016589164734\n",
      "66 train 45 552\n",
      "Phase: train. Epoch: 66. Loss: 0.08904621005058289\n",
      "66 train 46 564\n",
      "Phase: train. Epoch: 66. Loss: 0.0921737477183342\n",
      "66 train 47 576\n",
      "Phase: train. Epoch: 66. Loss: 0.09330698102712631\n",
      "66 train 48 588\n",
      "Phase: train. Epoch: 66. Loss: 0.07818052172660828\n",
      "66 train 49 600\n",
      "Phase: train. Epoch: 66. Loss: 0.09725332260131836\n",
      "66 train 50 612\n",
      "Phase: train. Epoch: 66. Loss: 0.06770963221788406\n",
      "66 train 51 624\n",
      "Phase: train. Epoch: 66. Loss: 0.09219696372747421\n",
      "66 train 52 636\n",
      "Phase: train. Epoch: 66. Loss: 0.06822957843542099\n",
      "66 train 53 648\n",
      "Phase: train. Epoch: 66. Loss: 0.1107148751616478\n",
      "66 train 54 660\n",
      "Phase: train. Epoch: 66. Loss: 0.06422512233257294\n",
      "66 train 55 672\n",
      "Phase: train. Epoch: 66. Loss: 0.06229674071073532\n",
      "66 train 56 684\n",
      "Phase: train. Epoch: 66. Loss: 0.10038115829229355\n",
      "66 train 57 696\n",
      "Phase: train. Epoch: 66. Loss: 0.10525733232498169\n",
      "66 train 58 708\n",
      "Phase: train. Epoch: 66. Loss: 0.07200288772583008\n",
      "66 train 59 720\n",
      "Phase: train. Epoch: 66. Loss: 0.11504501104354858\n",
      "66 train 60 732\n",
      "Phase: train. Epoch: 66. Loss: 0.0875948891043663\n",
      "66 train 61 744\n",
      "Phase: train. Epoch: 66. Loss: 0.06855760514736176\n",
      "66 train 62 751\n",
      "Phase: train. Epoch: 66. Loss: 0.061851195991039276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 val 0 763\n",
      "Phase: val. Epoch: 66. Loss: 0.1067035123705864\n",
      "66 val 1 775\n",
      "Phase: val. Epoch: 66. Loss: 0.09192712604999542\n",
      "66 val 2 787\n",
      "Phase: val. Epoch: 66. Loss: 0.07365100085735321\n",
      "66 val 3 799\n",
      "Phase: val. Epoch: 66. Loss: 0.1079135537147522\n",
      "66 val 4 811\n",
      "Phase: val. Epoch: 66. Loss: 0.080899178981781\n",
      "66 val 5 823\n",
      "Phase: val. Epoch: 66. Loss: 0.06986650824546814\n",
      "66 val 6 835\n",
      "Phase: val. Epoch: 66. Loss: 0.07011397927999496\n",
      "66 val 7 847\n",
      "Phase: val. Epoch: 66. Loss: 0.0827760249376297\n",
      "66 val 8 859\n",
      "Phase: val. Epoch: 66. Loss: 0.06307514011859894\n",
      "66 val 9 871\n",
      "Phase: val. Epoch: 66. Loss: 0.09481893479824066\n",
      "66 val 10 883\n",
      "Phase: val. Epoch: 66. Loss: 0.0884784683585167\n",
      "66 val 11 884\n",
      "Phase: val. Epoch: 66. Loss: 0.02735770493745804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 train 0 12\n",
      "Phase: train. Epoch: 67. Loss: 0.08630054444074631\n",
      "67 train 1 24\n",
      "Phase: train. Epoch: 67. Loss: 0.06359320133924484\n",
      "67 train 2 36\n",
      "Phase: train. Epoch: 67. Loss: 0.09275142848491669\n",
      "67 train 3 48\n",
      "Phase: train. Epoch: 67. Loss: 0.08052419871091843\n",
      "67 train 4 60\n",
      "Phase: train. Epoch: 67. Loss: 0.07746858149766922\n",
      "67 train 5 72\n",
      "Phase: train. Epoch: 67. Loss: 0.07433170825242996\n",
      "67 train 6 84\n",
      "Phase: train. Epoch: 67. Loss: 0.07236047089099884\n",
      "67 train 7 96\n",
      "Phase: train. Epoch: 67. Loss: 0.09321367740631104\n",
      "67 train 8 108\n",
      "Phase: train. Epoch: 67. Loss: 0.07355930656194687\n",
      "67 train 9 120\n",
      "Phase: train. Epoch: 67. Loss: 0.08637954294681549\n",
      "67 train 10 132\n",
      "Phase: train. Epoch: 67. Loss: 0.06827092170715332\n",
      "67 train 11 144\n",
      "Phase: train. Epoch: 67. Loss: 0.10671129077672958\n",
      "67 train 12 156\n",
      "Phase: train. Epoch: 67. Loss: 0.06814040243625641\n",
      "67 train 13 168\n",
      "Phase: train. Epoch: 67. Loss: 0.09144755452871323\n",
      "67 train 14 180\n",
      "Phase: train. Epoch: 67. Loss: 0.07216614484786987\n",
      "67 train 15 192\n",
      "Phase: train. Epoch: 67. Loss: 0.05864598602056503\n",
      "67 train 16 204\n",
      "Phase: train. Epoch: 67. Loss: 0.08200904726982117\n",
      "67 train 17 216\n",
      "Phase: train. Epoch: 67. Loss: 0.08346366137266159\n",
      "67 train 18 228\n",
      "Phase: train. Epoch: 67. Loss: 0.07031311839818954\n",
      "67 train 19 240\n",
      "Phase: train. Epoch: 67. Loss: 0.10082991421222687\n",
      "67 train 20 252\n",
      "Phase: train. Epoch: 67. Loss: 0.08674715459346771\n",
      "67 train 21 264\n",
      "Phase: train. Epoch: 67. Loss: 0.09445227682590485\n",
      "67 train 22 276\n",
      "Phase: train. Epoch: 67. Loss: 0.09369824826717377\n",
      "67 train 23 288\n",
      "Phase: train. Epoch: 67. Loss: 0.10711055248975754\n",
      "67 train 24 300\n",
      "Phase: train. Epoch: 67. Loss: 0.06508196890354156\n",
      "67 train 25 312\n",
      "Phase: train. Epoch: 67. Loss: 0.08817166090011597\n",
      "67 train 26 324\n",
      "Phase: train. Epoch: 67. Loss: 0.07648219913244247\n",
      "67 train 27 336\n",
      "Phase: train. Epoch: 67. Loss: 0.09662671387195587\n",
      "67 train 28 348\n",
      "Phase: train. Epoch: 67. Loss: 0.08083038032054901\n",
      "67 train 29 360\n",
      "Phase: train. Epoch: 67. Loss: 0.07566022872924805\n",
      "67 train 30 372\n",
      "Phase: train. Epoch: 67. Loss: 0.07714767754077911\n",
      "67 train 31 384\n",
      "Phase: train. Epoch: 67. Loss: 0.08202683925628662\n",
      "67 train 32 396\n",
      "Phase: train. Epoch: 67. Loss: 0.08073163032531738\n",
      "67 train 33 408\n",
      "Phase: train. Epoch: 67. Loss: 0.07127753645181656\n",
      "67 train 34 420\n",
      "Phase: train. Epoch: 67. Loss: 0.0789533257484436\n",
      "67 train 35 432\n",
      "Phase: train. Epoch: 67. Loss: 0.079530268907547\n",
      "67 train 36 444\n",
      "Phase: train. Epoch: 67. Loss: 0.09651879966259003\n",
      "67 train 37 456\n",
      "Phase: train. Epoch: 67. Loss: 0.07471508532762527\n",
      "67 train 38 468\n",
      "Phase: train. Epoch: 67. Loss: 0.07791642844676971\n",
      "67 train 39 480\n",
      "Phase: train. Epoch: 67. Loss: 0.09620992839336395\n",
      "67 train 40 492\n",
      "Phase: train. Epoch: 67. Loss: 0.09653248637914658\n",
      "67 train 41 504\n",
      "Phase: train. Epoch: 67. Loss: 0.08183927834033966\n",
      "67 train 42 516\n",
      "Phase: train. Epoch: 67. Loss: 0.0877532809972763\n",
      "67 train 43 528\n",
      "Phase: train. Epoch: 67. Loss: 0.08537612855434418\n",
      "67 train 44 540\n",
      "Phase: train. Epoch: 67. Loss: 0.09118342399597168\n",
      "67 train 45 552\n",
      "Phase: train. Epoch: 67. Loss: 0.08249656111001968\n",
      "67 train 46 564\n",
      "Phase: train. Epoch: 67. Loss: 0.07847334444522858\n",
      "67 train 47 576\n",
      "Phase: train. Epoch: 67. Loss: 0.11338894069194794\n",
      "67 train 48 588\n",
      "Phase: train. Epoch: 67. Loss: 0.08144163340330124\n",
      "67 train 49 600\n",
      "Phase: train. Epoch: 67. Loss: 0.08889897912740707\n",
      "67 train 50 612\n",
      "Phase: train. Epoch: 67. Loss: 0.08829476684331894\n",
      "67 train 51 624\n",
      "Phase: train. Epoch: 67. Loss: 0.07743129134178162\n",
      "67 train 52 636\n",
      "Phase: train. Epoch: 67. Loss: 0.07031393051147461\n",
      "67 train 53 648\n",
      "Phase: train. Epoch: 67. Loss: 0.0729721337556839\n",
      "67 train 54 660\n",
      "Phase: train. Epoch: 67. Loss: 0.07487332820892334\n",
      "67 train 55 672\n",
      "Phase: train. Epoch: 67. Loss: 0.09770697355270386\n",
      "67 train 56 684\n",
      "Phase: train. Epoch: 67. Loss: 0.10165979713201523\n",
      "67 train 57 696\n",
      "Phase: train. Epoch: 67. Loss: 0.10069122910499573\n",
      "67 train 58 708\n",
      "Phase: train. Epoch: 67. Loss: 0.10283126682043076\n",
      "67 train 59 720\n",
      "Phase: train. Epoch: 67. Loss: 0.10834120213985443\n",
      "67 train 60 732\n",
      "Phase: train. Epoch: 67. Loss: 0.06659279763698578\n",
      "67 train 61 744\n",
      "Phase: train. Epoch: 67. Loss: 0.08294405788183212\n",
      "67 train 62 751\n",
      "Phase: train. Epoch: 67. Loss: 0.09115003049373627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 val 0 763\n",
      "Phase: val. Epoch: 67. Loss: 0.06664881110191345\n",
      "67 val 1 775\n",
      "Phase: val. Epoch: 67. Loss: 0.06359896063804626\n",
      "67 val 2 787\n",
      "Phase: val. Epoch: 67. Loss: 0.05688823387026787\n",
      "67 val 3 799\n",
      "Phase: val. Epoch: 67. Loss: 0.10137736052274704\n",
      "67 val 4 811\n",
      "Phase: val. Epoch: 67. Loss: 0.09095831960439682\n",
      "67 val 5 823\n",
      "Phase: val. Epoch: 67. Loss: 0.08528943359851837\n",
      "67 val 6 835\n",
      "Phase: val. Epoch: 67. Loss: 0.08289852738380432\n",
      "67 val 7 847\n",
      "Phase: val. Epoch: 67. Loss: 0.0756811797618866\n",
      "67 val 8 859\n",
      "Phase: val. Epoch: 67. Loss: 0.08008897304534912\n",
      "67 val 9 871\n",
      "Phase: val. Epoch: 67. Loss: 0.07508546113967896\n",
      "67 val 10 883\n",
      "Phase: val. Epoch: 67. Loss: 0.10298921912908554\n",
      "67 val 11 884\n",
      "Phase: val. Epoch: 67. Loss: 0.02678360417485237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 train 0 12\n",
      "Phase: train. Epoch: 68. Loss: 0.111720971763134\n",
      "68 train 1 24\n",
      "Phase: train. Epoch: 68. Loss: 0.11379345506429672\n",
      "68 train 2 36\n",
      "Phase: train. Epoch: 68. Loss: 0.0678296834230423\n",
      "68 train 3 48\n",
      "Phase: train. Epoch: 68. Loss: 0.07834091782569885\n",
      "68 train 4 60\n",
      "Phase: train. Epoch: 68. Loss: 0.07782986760139465\n",
      "68 train 5 72\n",
      "Phase: train. Epoch: 68. Loss: 0.12073998153209686\n",
      "68 train 6 84\n",
      "Phase: train. Epoch: 68. Loss: 0.06710758060216904\n",
      "68 train 7 96\n",
      "Phase: train. Epoch: 68. Loss: 0.13366514444351196\n",
      "68 train 8 108\n",
      "Phase: train. Epoch: 68. Loss: 0.08169200271368027\n",
      "68 train 9 120\n",
      "Phase: train. Epoch: 68. Loss: 0.07079187035560608\n",
      "68 train 10 132\n",
      "Phase: train. Epoch: 68. Loss: 0.06923221051692963\n",
      "68 train 11 144\n",
      "Phase: train. Epoch: 68. Loss: 0.06261961162090302\n",
      "68 train 12 156\n",
      "Phase: train. Epoch: 68. Loss: 0.09790927171707153\n",
      "68 train 13 168\n",
      "Phase: train. Epoch: 68. Loss: 0.10325076431035995\n",
      "68 train 14 180\n",
      "Phase: train. Epoch: 68. Loss: 0.09972882270812988\n",
      "68 train 15 192\n",
      "Phase: train. Epoch: 68. Loss: 0.09672658890485764\n",
      "68 train 16 204\n",
      "Phase: train. Epoch: 68. Loss: 0.08617105334997177\n",
      "68 train 17 216\n",
      "Phase: train. Epoch: 68. Loss: 0.09020300954580307\n",
      "68 train 18 228\n",
      "Phase: train. Epoch: 68. Loss: 0.10627731680870056\n",
      "68 train 19 240\n",
      "Phase: train. Epoch: 68. Loss: 0.08510058373212814\n",
      "68 train 20 252\n",
      "Phase: train. Epoch: 68. Loss: 0.0908193290233612\n",
      "68 train 21 264\n",
      "Phase: train. Epoch: 68. Loss: 0.09610836207866669\n",
      "68 train 22 276\n",
      "Phase: train. Epoch: 68. Loss: 0.1009930893778801\n",
      "68 train 23 288\n",
      "Phase: train. Epoch: 68. Loss: 0.08748164027929306\n",
      "68 train 24 300\n",
      "Phase: train. Epoch: 68. Loss: 0.09521852433681488\n",
      "68 train 25 312\n",
      "Phase: train. Epoch: 68. Loss: 0.09033327549695969\n",
      "68 train 26 324\n",
      "Phase: train. Epoch: 68. Loss: 0.0892237052321434\n",
      "68 train 27 336\n",
      "Phase: train. Epoch: 68. Loss: 0.07522456347942352\n",
      "68 train 28 348\n",
      "Phase: train. Epoch: 68. Loss: 0.0884881466627121\n",
      "68 train 29 360\n",
      "Phase: train. Epoch: 68. Loss: 0.08825507014989853\n",
      "68 train 30 372\n",
      "Phase: train. Epoch: 68. Loss: 0.08450587838888168\n",
      "68 train 31 384\n",
      "Phase: train. Epoch: 68. Loss: 0.06832539290189743\n",
      "68 train 32 396\n",
      "Phase: train. Epoch: 68. Loss: 0.06578441709280014\n",
      "68 train 33 408\n",
      "Phase: train. Epoch: 68. Loss: 0.07150843739509583\n",
      "68 train 34 420\n",
      "Phase: train. Epoch: 68. Loss: 0.07121635973453522\n",
      "68 train 35 432\n",
      "Phase: train. Epoch: 68. Loss: 0.07133867591619492\n",
      "68 train 36 444\n",
      "Phase: train. Epoch: 68. Loss: 0.06808091700077057\n",
      "68 train 37 456\n",
      "Phase: train. Epoch: 68. Loss: 0.09703066200017929\n",
      "68 train 38 468\n",
      "Phase: train. Epoch: 68. Loss: 0.07633578032255173\n",
      "68 train 39 480\n",
      "Phase: train. Epoch: 68. Loss: 0.07732847332954407\n",
      "68 train 40 492\n",
      "Phase: train. Epoch: 68. Loss: 0.08229625970125198\n",
      "68 train 41 504\n",
      "Phase: train. Epoch: 68. Loss: 0.08112045377492905\n",
      "68 train 42 516\n",
      "Phase: train. Epoch: 68. Loss: 0.06870098412036896\n",
      "68 train 43 528\n",
      "Phase: train. Epoch: 68. Loss: 0.0905938372015953\n",
      "68 train 44 540\n",
      "Phase: train. Epoch: 68. Loss: 0.08735428750514984\n",
      "68 train 45 552\n",
      "Phase: train. Epoch: 68. Loss: 0.10496070235967636\n",
      "68 train 46 564\n",
      "Phase: train. Epoch: 68. Loss: 0.0713573694229126\n",
      "68 train 47 576\n",
      "Phase: train. Epoch: 68. Loss: 0.08493885397911072\n",
      "68 train 48 588\n",
      "Phase: train. Epoch: 68. Loss: 0.082667276263237\n",
      "68 train 49 600\n",
      "Phase: train. Epoch: 68. Loss: 0.10670869052410126\n",
      "68 train 50 612\n",
      "Phase: train. Epoch: 68. Loss: 0.07030738890171051\n",
      "68 train 51 624\n",
      "Phase: train. Epoch: 68. Loss: 0.07904702425003052\n",
      "68 train 52 636\n",
      "Phase: train. Epoch: 68. Loss: 0.05933130532503128\n",
      "68 train 53 648\n",
      "Phase: train. Epoch: 68. Loss: 0.08917681872844696\n",
      "68 train 54 660\n",
      "Phase: train. Epoch: 68. Loss: 0.0972309336066246\n",
      "68 train 55 672\n",
      "Phase: train. Epoch: 68. Loss: 0.08891844749450684\n",
      "68 train 56 684\n",
      "Phase: train. Epoch: 68. Loss: 0.09178394079208374\n",
      "68 train 57 696\n",
      "Phase: train. Epoch: 68. Loss: 0.09912087023258209\n",
      "68 train 58 708\n",
      "Phase: train. Epoch: 68. Loss: 0.08619718998670578\n",
      "68 train 59 720\n",
      "Phase: train. Epoch: 68. Loss: 0.08927647769451141\n",
      "68 train 60 732\n",
      "Phase: train. Epoch: 68. Loss: 0.07135812938213348\n",
      "68 train 61 744\n",
      "Phase: train. Epoch: 68. Loss: 0.08397023379802704\n",
      "68 train 62 751\n",
      "Phase: train. Epoch: 68. Loss: 0.0806005448102951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 val 0 763\n",
      "Phase: val. Epoch: 68. Loss: 0.10958220064640045\n",
      "68 val 1 775\n",
      "Phase: val. Epoch: 68. Loss: 0.08750055730342865\n",
      "68 val 2 787\n",
      "Phase: val. Epoch: 68. Loss: 0.06875696778297424\n",
      "68 val 3 799\n",
      "Phase: val. Epoch: 68. Loss: 0.08324922621250153\n",
      "68 val 4 811\n",
      "Phase: val. Epoch: 68. Loss: 0.06547635793685913\n",
      "68 val 5 823\n",
      "Phase: val. Epoch: 68. Loss: 0.05635342001914978\n",
      "68 val 6 835\n",
      "Phase: val. Epoch: 68. Loss: 0.09946559369564056\n",
      "68 val 7 847\n",
      "Phase: val. Epoch: 68. Loss: 0.07687658071517944\n",
      "68 val 8 859\n",
      "Phase: val. Epoch: 68. Loss: 0.06827845424413681\n",
      "68 val 9 871\n",
      "Phase: val. Epoch: 68. Loss: 0.074266716837883\n",
      "68 val 10 883\n",
      "Phase: val. Epoch: 68. Loss: 0.09985028207302094\n",
      "68 val 11 884\n",
      "Phase: val. Epoch: 68. Loss: 0.02569841593503952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 train 0 12\n",
      "Phase: train. Epoch: 69. Loss: 0.08303985744714737\n",
      "69 train 1 24\n",
      "Phase: train. Epoch: 69. Loss: 0.08249928802251816\n",
      "69 train 2 36\n",
      "Phase: train. Epoch: 69. Loss: 0.11218687146902084\n",
      "69 train 3 48\n",
      "Phase: train. Epoch: 69. Loss: 0.05730524659156799\n",
      "69 train 4 60\n",
      "Phase: train. Epoch: 69. Loss: 0.07697007060050964\n",
      "69 train 5 72\n",
      "Phase: train. Epoch: 69. Loss: 0.06522756069898605\n",
      "69 train 6 84\n",
      "Phase: train. Epoch: 69. Loss: 0.08116261661052704\n",
      "69 train 7 96\n",
      "Phase: train. Epoch: 69. Loss: 0.07313033938407898\n",
      "69 train 8 108\n",
      "Phase: train. Epoch: 69. Loss: 0.09871222823858261\n",
      "69 train 9 120\n",
      "Phase: train. Epoch: 69. Loss: 0.09722911566495895\n",
      "69 train 10 132\n",
      "Phase: train. Epoch: 69. Loss: 0.08806862682104111\n",
      "69 train 11 144\n",
      "Phase: train. Epoch: 69. Loss: 0.08746354281902313\n",
      "69 train 12 156\n",
      "Phase: train. Epoch: 69. Loss: 0.05336935073137283\n",
      "69 train 13 168\n",
      "Phase: train. Epoch: 69. Loss: 0.11562236398458481\n",
      "69 train 14 180\n",
      "Phase: train. Epoch: 69. Loss: 0.0909164771437645\n",
      "69 train 15 192\n",
      "Phase: train. Epoch: 69. Loss: 0.10041674971580505\n",
      "69 train 16 204\n",
      "Phase: train. Epoch: 69. Loss: 0.06621072441339493\n",
      "69 train 17 216\n",
      "Phase: train. Epoch: 69. Loss: 0.081240713596344\n",
      "69 train 18 228\n",
      "Phase: train. Epoch: 69. Loss: 0.11196240037679672\n",
      "69 train 19 240\n",
      "Phase: train. Epoch: 69. Loss: 0.10253405570983887\n",
      "69 train 20 252\n",
      "Phase: train. Epoch: 69. Loss: 0.07072438299655914\n",
      "69 train 21 264\n",
      "Phase: train. Epoch: 69. Loss: 0.06962071359157562\n",
      "69 train 22 276\n",
      "Phase: train. Epoch: 69. Loss: 0.07276706397533417\n",
      "69 train 23 288\n",
      "Phase: train. Epoch: 69. Loss: 0.093939408659935\n",
      "69 train 24 300\n",
      "Phase: train. Epoch: 69. Loss: 0.07597899436950684\n",
      "69 train 25 312\n",
      "Phase: train. Epoch: 69. Loss: 0.0701301097869873\n",
      "69 train 26 324\n",
      "Phase: train. Epoch: 69. Loss: 0.08692726492881775\n",
      "69 train 27 336\n",
      "Phase: train. Epoch: 69. Loss: 0.07411913573741913\n",
      "69 train 28 348\n",
      "Phase: train. Epoch: 69. Loss: 0.05549820885062218\n",
      "69 train 29 360\n",
      "Phase: train. Epoch: 69. Loss: 0.10951654613018036\n",
      "69 train 30 372\n",
      "Phase: train. Epoch: 69. Loss: 0.07462315261363983\n",
      "69 train 31 384\n",
      "Phase: train. Epoch: 69. Loss: 0.10264866799116135\n",
      "69 train 32 396\n",
      "Phase: train. Epoch: 69. Loss: 0.08408346027135849\n",
      "69 train 33 408\n",
      "Phase: train. Epoch: 69. Loss: 0.1160234808921814\n",
      "69 train 34 420\n",
      "Phase: train. Epoch: 69. Loss: 0.09444000571966171\n",
      "69 train 35 432\n",
      "Phase: train. Epoch: 69. Loss: 0.09683531522750854\n",
      "69 train 36 444\n",
      "Phase: train. Epoch: 69. Loss: 0.07169589400291443\n",
      "69 train 37 456\n",
      "Phase: train. Epoch: 69. Loss: 0.06926415860652924\n",
      "69 train 38 468\n",
      "Phase: train. Epoch: 69. Loss: 0.10074774920940399\n",
      "69 train 39 480\n",
      "Phase: train. Epoch: 69. Loss: 0.1054249033331871\n",
      "69 train 40 492\n",
      "Phase: train. Epoch: 69. Loss: 0.10867178440093994\n",
      "69 train 41 504\n",
      "Phase: train. Epoch: 69. Loss: 0.06624023616313934\n",
      "69 train 42 516\n",
      "Phase: train. Epoch: 69. Loss: 0.13455280661582947\n",
      "69 train 43 528\n",
      "Phase: train. Epoch: 69. Loss: 0.06252609938383102\n",
      "69 train 44 540\n",
      "Phase: train. Epoch: 69. Loss: 0.13429616391658783\n",
      "69 train 45 552\n",
      "Phase: train. Epoch: 69. Loss: 0.0860200971364975\n",
      "69 train 46 564\n",
      "Phase: train. Epoch: 69. Loss: 0.08492682874202728\n",
      "69 train 47 576\n",
      "Phase: train. Epoch: 69. Loss: 0.09247264266014099\n",
      "69 train 48 588\n",
      "Phase: train. Epoch: 69. Loss: 0.0775587186217308\n",
      "69 train 49 600\n",
      "Phase: train. Epoch: 69. Loss: 0.06506490707397461\n",
      "69 train 50 612\n",
      "Phase: train. Epoch: 69. Loss: 0.08706667274236679\n",
      "69 train 51 624\n",
      "Phase: train. Epoch: 69. Loss: 0.06868203729391098\n",
      "69 train 52 636\n",
      "Phase: train. Epoch: 69. Loss: 0.08337082713842392\n",
      "69 train 53 648\n",
      "Phase: train. Epoch: 69. Loss: 0.08058646321296692\n",
      "69 train 54 660\n",
      "Phase: train. Epoch: 69. Loss: 0.08411741256713867\n",
      "69 train 55 672\n",
      "Phase: train. Epoch: 69. Loss: 0.07056063413619995\n",
      "69 train 56 684\n",
      "Phase: train. Epoch: 69. Loss: 0.07475538551807404\n",
      "69 train 57 696\n",
      "Phase: train. Epoch: 69. Loss: 0.09956373274326324\n",
      "69 train 58 708\n",
      "Phase: train. Epoch: 69. Loss: 0.08539562672376633\n",
      "69 train 59 720\n",
      "Phase: train. Epoch: 69. Loss: 0.1166018471121788\n",
      "69 train 60 732\n",
      "Phase: train. Epoch: 69. Loss: 0.06256913393735886\n",
      "69 train 61 744\n",
      "Phase: train. Epoch: 69. Loss: 0.09180992096662521\n",
      "69 train 62 751\n",
      "Phase: train. Epoch: 69. Loss: 0.12553618848323822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 val 0 763\n",
      "Phase: val. Epoch: 69. Loss: 0.09373998641967773\n",
      "69 val 1 775\n",
      "Phase: val. Epoch: 69. Loss: 0.0569181852042675\n",
      "69 val 2 787\n",
      "Phase: val. Epoch: 69. Loss: 0.08067373931407928\n",
      "69 val 3 799\n",
      "Phase: val. Epoch: 69. Loss: 0.08473089337348938\n",
      "69 val 4 811\n",
      "Phase: val. Epoch: 69. Loss: 0.06376203894615173\n",
      "69 val 5 823\n",
      "Phase: val. Epoch: 69. Loss: 0.07886280119419098\n",
      "69 val 6 835\n",
      "Phase: val. Epoch: 69. Loss: 0.06285422295331955\n",
      "69 val 7 847\n",
      "Phase: val. Epoch: 69. Loss: 0.11988990008831024\n",
      "69 val 8 859\n",
      "Phase: val. Epoch: 69. Loss: 0.09188346564769745\n",
      "69 val 9 871\n",
      "Phase: val. Epoch: 69. Loss: 0.08219897747039795\n",
      "69 val 10 883\n",
      "Phase: val. Epoch: 69. Loss: 0.053598545491695404\n",
      "69 val 11 884\n",
      "Phase: val. Epoch: 69. Loss: 0.10440362244844437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 train 0 12\n",
      "Phase: train. Epoch: 70. Loss: 0.08152183890342712\n",
      "70 train 1 24\n",
      "Phase: train. Epoch: 70. Loss: 0.07870887219905853\n",
      "70 train 2 36\n",
      "Phase: train. Epoch: 70. Loss: 0.08238083124160767\n",
      "70 train 3 48\n",
      "Phase: train. Epoch: 70. Loss: 0.07509498298168182\n",
      "70 train 4 60\n",
      "Phase: train. Epoch: 70. Loss: 0.10595416277647018\n",
      "70 train 5 72\n",
      "Phase: train. Epoch: 70. Loss: 0.07200293242931366\n",
      "70 train 6 84\n",
      "Phase: train. Epoch: 70. Loss: 0.07422923296689987\n",
      "70 train 7 96\n",
      "Phase: train. Epoch: 70. Loss: 0.07477982342243195\n",
      "70 train 8 108\n",
      "Phase: train. Epoch: 70. Loss: 0.08492126315832138\n",
      "70 train 9 120\n",
      "Phase: train. Epoch: 70. Loss: 0.08775079250335693\n",
      "70 train 10 132\n",
      "Phase: train. Epoch: 70. Loss: 0.09577273577451706\n",
      "70 train 11 144\n",
      "Phase: train. Epoch: 70. Loss: 0.11305537819862366\n",
      "70 train 12 156\n",
      "Phase: train. Epoch: 70. Loss: 0.07542164623737335\n",
      "70 train 13 168\n",
      "Phase: train. Epoch: 70. Loss: 0.0821840912103653\n",
      "70 train 14 180\n",
      "Phase: train. Epoch: 70. Loss: 0.09695786237716675\n",
      "70 train 15 192\n",
      "Phase: train. Epoch: 70. Loss: 0.0732036605477333\n",
      "70 train 16 204\n",
      "Phase: train. Epoch: 70. Loss: 0.07623899728059769\n",
      "70 train 17 216\n",
      "Phase: train. Epoch: 70. Loss: 0.08796077221632004\n",
      "70 train 18 228\n",
      "Phase: train. Epoch: 70. Loss: 0.07133927941322327\n",
      "70 train 19 240\n",
      "Phase: train. Epoch: 70. Loss: 0.07766768336296082\n",
      "70 train 20 252\n",
      "Phase: train. Epoch: 70. Loss: 0.08290758728981018\n",
      "70 train 21 264\n",
      "Phase: train. Epoch: 70. Loss: 0.0823521688580513\n",
      "70 train 22 276\n",
      "Phase: train. Epoch: 70. Loss: 0.08623345196247101\n",
      "70 train 23 288\n",
      "Phase: train. Epoch: 70. Loss: 0.09097417443990707\n",
      "70 train 24 300\n",
      "Phase: train. Epoch: 70. Loss: 0.07630876451730728\n",
      "70 train 25 312\n",
      "Phase: train. Epoch: 70. Loss: 0.11113085597753525\n",
      "70 train 26 324\n",
      "Phase: train. Epoch: 70. Loss: 0.07498656213283539\n",
      "70 train 27 336\n",
      "Phase: train. Epoch: 70. Loss: 0.08108572661876678\n",
      "70 train 28 348\n",
      "Phase: train. Epoch: 70. Loss: 0.07234480232000351\n",
      "70 train 29 360\n",
      "Phase: train. Epoch: 70. Loss: 0.08463287353515625\n",
      "70 train 30 372\n",
      "Phase: train. Epoch: 70. Loss: 0.09345406293869019\n",
      "70 train 31 384\n",
      "Phase: train. Epoch: 70. Loss: 0.07907650619745255\n",
      "70 train 32 396\n",
      "Phase: train. Epoch: 70. Loss: 0.08249083161354065\n",
      "70 train 33 408\n",
      "Phase: train. Epoch: 70. Loss: 0.06894899904727936\n",
      "70 train 34 420\n",
      "Phase: train. Epoch: 70. Loss: 0.09926267713308334\n",
      "70 train 35 432\n",
      "Phase: train. Epoch: 70. Loss: 0.07887155562639236\n",
      "70 train 36 444\n",
      "Phase: train. Epoch: 70. Loss: 0.06746180355548859\n",
      "70 train 37 456\n",
      "Phase: train. Epoch: 70. Loss: 0.07389308512210846\n",
      "70 train 38 468\n",
      "Phase: train. Epoch: 70. Loss: 0.07887739688158035\n",
      "70 train 39 480\n",
      "Phase: train. Epoch: 70. Loss: 0.06270776689052582\n",
      "70 train 40 492\n",
      "Phase: train. Epoch: 70. Loss: 0.1131458505988121\n",
      "70 train 41 504\n",
      "Phase: train. Epoch: 70. Loss: 0.10075835883617401\n",
      "70 train 42 516\n",
      "Phase: train. Epoch: 70. Loss: 0.10368802398443222\n",
      "70 train 43 528\n",
      "Phase: train. Epoch: 70. Loss: 0.09633199870586395\n",
      "70 train 44 540\n",
      "Phase: train. Epoch: 70. Loss: 0.07923513650894165\n",
      "70 train 45 552\n",
      "Phase: train. Epoch: 70. Loss: 0.0771818682551384\n",
      "70 train 46 564\n",
      "Phase: train. Epoch: 70. Loss: 0.09489142149686813\n",
      "70 train 47 576\n",
      "Phase: train. Epoch: 70. Loss: 0.08042942732572556\n",
      "70 train 48 588\n",
      "Phase: train. Epoch: 70. Loss: 0.09594007581472397\n",
      "70 train 49 600\n",
      "Phase: train. Epoch: 70. Loss: 0.10364340245723724\n",
      "70 train 50 612\n",
      "Phase: train. Epoch: 70. Loss: 0.07929876446723938\n",
      "70 train 51 624\n",
      "Phase: train. Epoch: 70. Loss: 0.08627155423164368\n",
      "70 train 52 636\n",
      "Phase: train. Epoch: 70. Loss: 0.07164059579372406\n",
      "70 train 53 648\n",
      "Phase: train. Epoch: 70. Loss: 0.07321932911872864\n",
      "70 train 54 660\n",
      "Phase: train. Epoch: 70. Loss: 0.08738287538290024\n",
      "70 train 55 672\n",
      "Phase: train. Epoch: 70. Loss: 0.07694832980632782\n",
      "70 train 56 684\n",
      "Phase: train. Epoch: 70. Loss: 0.10099127143621445\n",
      "70 train 57 696\n",
      "Phase: train. Epoch: 70. Loss: 0.07139885425567627\n",
      "70 train 58 708\n",
      "Phase: train. Epoch: 70. Loss: 0.06916365027427673\n",
      "70 train 59 720\n",
      "Phase: train. Epoch: 70. Loss: 0.0764993280172348\n",
      "70 train 60 732\n",
      "Phase: train. Epoch: 70. Loss: 0.11510853469371796\n",
      "70 train 61 744\n",
      "Phase: train. Epoch: 70. Loss: 0.08868043124675751\n",
      "70 train 62 751\n",
      "Phase: train. Epoch: 70. Loss: 0.11408835649490356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 val 0 763\n",
      "Phase: val. Epoch: 70. Loss: 0.0930132195353508\n",
      "70 val 1 775\n",
      "Phase: val. Epoch: 70. Loss: 0.0939629003405571\n",
      "70 val 2 787\n",
      "Phase: val. Epoch: 70. Loss: 0.07400543987751007\n",
      "70 val 3 799\n",
      "Phase: val. Epoch: 70. Loss: 0.07987530529499054\n",
      "70 val 4 811\n",
      "Phase: val. Epoch: 70. Loss: 0.08303798735141754\n",
      "70 val 5 823\n",
      "Phase: val. Epoch: 70. Loss: 0.07902838289737701\n",
      "70 val 6 835\n",
      "Phase: val. Epoch: 70. Loss: 0.09684976190328598\n",
      "70 val 7 847\n",
      "Phase: val. Epoch: 70. Loss: 0.07512199878692627\n",
      "70 val 8 859\n",
      "Phase: val. Epoch: 70. Loss: 0.10926163196563721\n",
      "70 val 9 871\n",
      "Phase: val. Epoch: 70. Loss: 0.06010400503873825\n",
      "70 val 10 883\n",
      "Phase: val. Epoch: 70. Loss: 0.0793696939945221\n",
      "70 val 11 884\n",
      "Phase: val. Epoch: 70. Loss: 0.20768152177333832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 train 0 12\n",
      "Phase: train. Epoch: 71. Loss: 0.08659178018569946\n",
      "71 train 1 24\n",
      "Phase: train. Epoch: 71. Loss: 0.07234655320644379\n",
      "71 train 2 36\n",
      "Phase: train. Epoch: 71. Loss: 0.08489859104156494\n",
      "71 train 3 48\n",
      "Phase: train. Epoch: 71. Loss: 0.08224157989025116\n",
      "71 train 4 60\n",
      "Phase: train. Epoch: 71. Loss: 0.06822405755519867\n",
      "71 train 5 72\n",
      "Phase: train. Epoch: 71. Loss: 0.08028599619865417\n",
      "71 train 6 84\n",
      "Phase: train. Epoch: 71. Loss: 0.09179285168647766\n",
      "71 train 7 96\n",
      "Phase: train. Epoch: 71. Loss: 0.06586824357509613\n",
      "71 train 8 108\n",
      "Phase: train. Epoch: 71. Loss: 0.10820423066616058\n",
      "71 train 9 120\n",
      "Phase: train. Epoch: 71. Loss: 0.1042548418045044\n",
      "71 train 10 132\n",
      "Phase: train. Epoch: 71. Loss: 0.08948787301778793\n",
      "71 train 11 144\n",
      "Phase: train. Epoch: 71. Loss: 0.0824311301112175\n",
      "71 train 12 156\n",
      "Phase: train. Epoch: 71. Loss: 0.09540069103240967\n",
      "71 train 13 168\n",
      "Phase: train. Epoch: 71. Loss: 0.08845371007919312\n",
      "71 train 14 180\n",
      "Phase: train. Epoch: 71. Loss: 0.09587305784225464\n",
      "71 train 15 192\n",
      "Phase: train. Epoch: 71. Loss: 0.09749864041805267\n",
      "71 train 16 204\n",
      "Phase: train. Epoch: 71. Loss: 0.09503112733364105\n",
      "71 train 17 216\n",
      "Phase: train. Epoch: 71. Loss: 0.06120654568076134\n",
      "71 train 18 228\n",
      "Phase: train. Epoch: 71. Loss: 0.0706867203116417\n",
      "71 train 19 240\n",
      "Phase: train. Epoch: 71. Loss: 0.09014873206615448\n",
      "71 train 20 252\n",
      "Phase: train. Epoch: 71. Loss: 0.08341258019208908\n",
      "71 train 21 264\n",
      "Phase: train. Epoch: 71. Loss: 0.09739311784505844\n",
      "71 train 22 276\n",
      "Phase: train. Epoch: 71. Loss: 0.0992908701300621\n",
      "71 train 23 288\n",
      "Phase: train. Epoch: 71. Loss: 0.10978560894727707\n",
      "71 train 24 300\n",
      "Phase: train. Epoch: 71. Loss: 0.08525644987821579\n",
      "71 train 25 312\n",
      "Phase: train. Epoch: 71. Loss: 0.07921995967626572\n",
      "71 train 26 324\n",
      "Phase: train. Epoch: 71. Loss: 0.07876881211996078\n",
      "71 train 27 336\n",
      "Phase: train. Epoch: 71. Loss: 0.08803509920835495\n",
      "71 train 28 348\n",
      "Phase: train. Epoch: 71. Loss: 0.06128751486539841\n",
      "71 train 29 360\n",
      "Phase: train. Epoch: 71. Loss: 0.08249619603157043\n",
      "71 train 30 372\n",
      "Phase: train. Epoch: 71. Loss: 0.08026835322380066\n",
      "71 train 31 384\n",
      "Phase: train. Epoch: 71. Loss: 0.07406190037727356\n",
      "71 train 32 396\n",
      "Phase: train. Epoch: 71. Loss: 0.0746927261352539\n",
      "71 train 33 408\n",
      "Phase: train. Epoch: 71. Loss: 0.08975304663181305\n",
      "71 train 34 420\n",
      "Phase: train. Epoch: 71. Loss: 0.07876183092594147\n",
      "71 train 35 432\n",
      "Phase: train. Epoch: 71. Loss: 0.08104457706212997\n",
      "71 train 36 444\n",
      "Phase: train. Epoch: 71. Loss: 0.07418428361415863\n",
      "71 train 37 456\n",
      "Phase: train. Epoch: 71. Loss: 0.08143040537834167\n",
      "71 train 38 468\n",
      "Phase: train. Epoch: 71. Loss: 0.0856669470667839\n",
      "71 train 39 480\n",
      "Phase: train. Epoch: 71. Loss: 0.08598960936069489\n",
      "71 train 40 492\n",
      "Phase: train. Epoch: 71. Loss: 0.06255683302879333\n",
      "71 train 41 504\n",
      "Phase: train. Epoch: 71. Loss: 0.0924304723739624\n",
      "71 train 42 516\n",
      "Phase: train. Epoch: 71. Loss: 0.087577223777771\n",
      "71 train 43 528\n",
      "Phase: train. Epoch: 71. Loss: 0.06411916017532349\n",
      "71 train 44 540\n",
      "Phase: train. Epoch: 71. Loss: 0.07534333318471909\n",
      "71 train 45 552\n",
      "Phase: train. Epoch: 71. Loss: 0.07329114526510239\n",
      "71 train 46 564\n",
      "Phase: train. Epoch: 71. Loss: 0.07554103434085846\n",
      "71 train 47 576\n",
      "Phase: train. Epoch: 71. Loss: 0.07690373808145523\n",
      "71 train 48 588\n",
      "Phase: train. Epoch: 71. Loss: 0.09136363118886948\n",
      "71 train 49 600\n",
      "Phase: train. Epoch: 71. Loss: 0.12833726406097412\n",
      "71 train 50 612\n",
      "Phase: train. Epoch: 71. Loss: 0.0836084634065628\n",
      "71 train 51 624\n",
      "Phase: train. Epoch: 71. Loss: 0.07099822908639908\n",
      "71 train 52 636\n",
      "Phase: train. Epoch: 71. Loss: 0.09516508877277374\n",
      "71 train 53 648\n",
      "Phase: train. Epoch: 71. Loss: 0.06701933592557907\n",
      "71 train 54 660\n",
      "Phase: train. Epoch: 71. Loss: 0.0789896547794342\n",
      "71 train 55 672\n",
      "Phase: train. Epoch: 71. Loss: 0.09124681353569031\n",
      "71 train 56 684\n",
      "Phase: train. Epoch: 71. Loss: 0.0876961350440979\n",
      "71 train 57 696\n",
      "Phase: train. Epoch: 71. Loss: 0.08303698152303696\n",
      "71 train 58 708\n",
      "Phase: train. Epoch: 71. Loss: 0.09495866298675537\n",
      "71 train 59 720\n",
      "Phase: train. Epoch: 71. Loss: 0.10513754934072495\n",
      "71 train 60 732\n",
      "Phase: train. Epoch: 71. Loss: 0.08325421810150146\n",
      "71 train 61 744\n",
      "Phase: train. Epoch: 71. Loss: 0.07506140321493149\n",
      "71 train 62 751\n",
      "Phase: train. Epoch: 71. Loss: 0.06277141720056534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 val 0 763\n",
      "Phase: val. Epoch: 71. Loss: 0.10713084787130356\n",
      "71 val 1 775\n",
      "Phase: val. Epoch: 71. Loss: 0.07364855706691742\n",
      "71 val 2 787\n",
      "Phase: val. Epoch: 71. Loss: 0.08743220567703247\n",
      "71 val 3 799\n",
      "Phase: val. Epoch: 71. Loss: 0.08236890286207199\n",
      "71 val 4 811\n",
      "Phase: val. Epoch: 71. Loss: 0.06934410333633423\n",
      "71 val 5 823\n",
      "Phase: val. Epoch: 71. Loss: 0.10387777537107468\n",
      "71 val 6 835\n",
      "Phase: val. Epoch: 71. Loss: 0.11642259359359741\n",
      "71 val 7 847\n",
      "Phase: val. Epoch: 71. Loss: 0.06686821579933167\n",
      "71 val 8 859\n",
      "Phase: val. Epoch: 71. Loss: 0.07545074820518494\n",
      "71 val 9 871\n",
      "Phase: val. Epoch: 71. Loss: 0.0677880197763443\n",
      "71 val 10 883\n",
      "Phase: val. Epoch: 71. Loss: 0.08956141769886017\n",
      "71 val 11 884\n",
      "Phase: val. Epoch: 71. Loss: 0.05486761033535004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 train 0 12\n",
      "Phase: train. Epoch: 72. Loss: 0.08160176128149033\n",
      "72 train 1 24\n",
      "Phase: train. Epoch: 72. Loss: 0.10318367183208466\n",
      "72 train 2 36\n",
      "Phase: train. Epoch: 72. Loss: 0.08367042243480682\n",
      "72 train 3 48\n",
      "Phase: train. Epoch: 72. Loss: 0.08207055926322937\n",
      "72 train 4 60\n",
      "Phase: train. Epoch: 72. Loss: 0.06851747632026672\n",
      "72 train 5 72\n",
      "Phase: train. Epoch: 72. Loss: 0.09573881328105927\n",
      "72 train 6 84\n",
      "Phase: train. Epoch: 72. Loss: 0.08139254152774811\n",
      "72 train 7 96\n",
      "Phase: train. Epoch: 72. Loss: 0.06990522891283035\n",
      "72 train 8 108\n",
      "Phase: train. Epoch: 72. Loss: 0.06807350367307663\n",
      "72 train 9 120\n",
      "Phase: train. Epoch: 72. Loss: 0.0673583447933197\n",
      "72 train 10 132\n",
      "Phase: train. Epoch: 72. Loss: 0.07750380039215088\n",
      "72 train 11 144\n",
      "Phase: train. Epoch: 72. Loss: 0.06286077201366425\n",
      "72 train 12 156\n",
      "Phase: train. Epoch: 72. Loss: 0.07925157994031906\n",
      "72 train 13 168\n",
      "Phase: train. Epoch: 72. Loss: 0.11139502376317978\n",
      "72 train 14 180\n",
      "Phase: train. Epoch: 72. Loss: 0.09245922416448593\n",
      "72 train 15 192\n",
      "Phase: train. Epoch: 72. Loss: 0.08724117279052734\n",
      "72 train 16 204\n",
      "Phase: train. Epoch: 72. Loss: 0.07232855260372162\n",
      "72 train 17 216\n",
      "Phase: train. Epoch: 72. Loss: 0.07854514569044113\n",
      "72 train 18 228\n",
      "Phase: train. Epoch: 72. Loss: 0.0857098326086998\n",
      "72 train 19 240\n",
      "Phase: train. Epoch: 72. Loss: 0.0814887285232544\n",
      "72 train 20 252\n",
      "Phase: train. Epoch: 72. Loss: 0.0978519544005394\n",
      "72 train 21 264\n",
      "Phase: train. Epoch: 72. Loss: 0.09081301093101501\n",
      "72 train 22 276\n",
      "Phase: train. Epoch: 72. Loss: 0.08682959526777267\n",
      "72 train 23 288\n",
      "Phase: train. Epoch: 72. Loss: 0.08898864686489105\n",
      "72 train 24 300\n",
      "Phase: train. Epoch: 72. Loss: 0.09035034477710724\n",
      "72 train 25 312\n",
      "Phase: train. Epoch: 72. Loss: 0.09033916890621185\n",
      "72 train 26 324\n",
      "Phase: train. Epoch: 72. Loss: 0.08160245418548584\n",
      "72 train 27 336\n",
      "Phase: train. Epoch: 72. Loss: 0.08193518966436386\n",
      "72 train 28 348\n",
      "Phase: train. Epoch: 72. Loss: 0.08151378482580185\n",
      "72 train 29 360\n",
      "Phase: train. Epoch: 72. Loss: 0.09561213105916977\n",
      "72 train 30 372\n",
      "Phase: train. Epoch: 72. Loss: 0.08266782760620117\n",
      "72 train 31 384\n",
      "Phase: train. Epoch: 72. Loss: 0.1063680499792099\n",
      "72 train 32 396\n",
      "Phase: train. Epoch: 72. Loss: 0.091957688331604\n",
      "72 train 33 408\n",
      "Phase: train. Epoch: 72. Loss: 0.06586907804012299\n",
      "72 train 34 420\n",
      "Phase: train. Epoch: 72. Loss: 0.08515896648168564\n",
      "72 train 35 432\n",
      "Phase: train. Epoch: 72. Loss: 0.12381666153669357\n",
      "72 train 36 444\n",
      "Phase: train. Epoch: 72. Loss: 0.08379854261875153\n",
      "72 train 37 456\n",
      "Phase: train. Epoch: 72. Loss: 0.07166235893964767\n",
      "72 train 38 468\n",
      "Phase: train. Epoch: 72. Loss: 0.10208939760923386\n",
      "72 train 39 480\n",
      "Phase: train. Epoch: 72. Loss: 0.0657956451177597\n",
      "72 train 40 492\n",
      "Phase: train. Epoch: 72. Loss: 0.07531257718801498\n",
      "72 train 41 504\n",
      "Phase: train. Epoch: 72. Loss: 0.09438323974609375\n",
      "72 train 42 516\n",
      "Phase: train. Epoch: 72. Loss: 0.06821984052658081\n",
      "72 train 43 528\n",
      "Phase: train. Epoch: 72. Loss: 0.10502996295690536\n",
      "72 train 44 540\n",
      "Phase: train. Epoch: 72. Loss: 0.10133618116378784\n",
      "72 train 45 552\n",
      "Phase: train. Epoch: 72. Loss: 0.08592542260885239\n",
      "72 train 46 564\n",
      "Phase: train. Epoch: 72. Loss: 0.09012763202190399\n",
      "72 train 47 576\n",
      "Phase: train. Epoch: 72. Loss: 0.07844498008489609\n",
      "72 train 48 588\n",
      "Phase: train. Epoch: 72. Loss: 0.09544786065816879\n",
      "72 train 49 600\n",
      "Phase: train. Epoch: 72. Loss: 0.09689734876155853\n",
      "72 train 50 612\n",
      "Phase: train. Epoch: 72. Loss: 0.11544587463140488\n",
      "72 train 51 624\n",
      "Phase: train. Epoch: 72. Loss: 0.09704788774251938\n",
      "72 train 52 636\n",
      "Phase: train. Epoch: 72. Loss: 0.08529766649007797\n",
      "72 train 53 648\n",
      "Phase: train. Epoch: 72. Loss: 0.0690697431564331\n",
      "72 train 54 660\n",
      "Phase: train. Epoch: 72. Loss: 0.1071651503443718\n",
      "72 train 55 672\n",
      "Phase: train. Epoch: 72. Loss: 0.07726766169071198\n",
      "72 train 56 684\n",
      "Phase: train. Epoch: 72. Loss: 0.06143386289477348\n",
      "72 train 57 696\n",
      "Phase: train. Epoch: 72. Loss: 0.06962071359157562\n",
      "72 train 58 708\n",
      "Phase: train. Epoch: 72. Loss: 0.07078030705451965\n",
      "72 train 59 720\n",
      "Phase: train. Epoch: 72. Loss: 0.08277492970228195\n",
      "72 train 60 732\n",
      "Phase: train. Epoch: 72. Loss: 0.06744733452796936\n",
      "72 train 61 744\n",
      "Phase: train. Epoch: 72. Loss: 0.09049688279628754\n",
      "72 train 62 751\n",
      "Phase: train. Epoch: 72. Loss: 0.061682142317295074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 val 0 763\n",
      "Phase: val. Epoch: 72. Loss: 0.09119882434606552\n",
      "72 val 1 775\n",
      "Phase: val. Epoch: 72. Loss: 0.0814436823129654\n",
      "72 val 2 787\n",
      "Phase: val. Epoch: 72. Loss: 0.07238936424255371\n",
      "72 val 3 799\n",
      "Phase: val. Epoch: 72. Loss: 0.07490841299295425\n",
      "72 val 4 811\n",
      "Phase: val. Epoch: 72. Loss: 0.07515935599803925\n",
      "72 val 5 823\n",
      "Phase: val. Epoch: 72. Loss: 0.050265293568372726\n",
      "72 val 6 835\n",
      "Phase: val. Epoch: 72. Loss: 0.09910651296377182\n",
      "72 val 7 847\n",
      "Phase: val. Epoch: 72. Loss: 0.08186879754066467\n",
      "72 val 8 859\n",
      "Phase: val. Epoch: 72. Loss: 0.13474586606025696\n",
      "72 val 9 871\n",
      "Phase: val. Epoch: 72. Loss: 0.0855928286910057\n",
      "72 val 10 883\n",
      "Phase: val. Epoch: 72. Loss: 0.08938797563314438\n",
      "72 val 11 884\n",
      "Phase: val. Epoch: 72. Loss: 0.08498577028512955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 train 0 12\n",
      "Phase: train. Epoch: 73. Loss: 0.060798030346632004\n",
      "73 train 1 24\n",
      "Phase: train. Epoch: 73. Loss: 0.1250326931476593\n",
      "73 train 2 36\n",
      "Phase: train. Epoch: 73. Loss: 0.09270995855331421\n",
      "73 train 3 48\n",
      "Phase: train. Epoch: 73. Loss: 0.07911436259746552\n",
      "73 train 4 60\n",
      "Phase: train. Epoch: 73. Loss: 0.07927902042865753\n",
      "73 train 5 72\n",
      "Phase: train. Epoch: 73. Loss: 0.06184939295053482\n",
      "73 train 6 84\n",
      "Phase: train. Epoch: 73. Loss: 0.0827525407075882\n",
      "73 train 7 96\n",
      "Phase: train. Epoch: 73. Loss: 0.10496658086776733\n",
      "73 train 8 108\n",
      "Phase: train. Epoch: 73. Loss: 0.08558228611946106\n",
      "73 train 9 120\n",
      "Phase: train. Epoch: 73. Loss: 0.06797274947166443\n",
      "73 train 10 132\n",
      "Phase: train. Epoch: 73. Loss: 0.09234428405761719\n",
      "73 train 11 144\n",
      "Phase: train. Epoch: 73. Loss: 0.07781790941953659\n",
      "73 train 12 156\n",
      "Phase: train. Epoch: 73. Loss: 0.09453027695417404\n",
      "73 train 13 168\n",
      "Phase: train. Epoch: 73. Loss: 0.07245190441608429\n",
      "73 train 14 180\n",
      "Phase: train. Epoch: 73. Loss: 0.08455091714859009\n",
      "73 train 15 192\n",
      "Phase: train. Epoch: 73. Loss: 0.08570964634418488\n",
      "73 train 16 204\n",
      "Phase: train. Epoch: 73. Loss: 0.10718242824077606\n",
      "73 train 17 216\n",
      "Phase: train. Epoch: 73. Loss: 0.10333167016506195\n",
      "73 train 18 228\n",
      "Phase: train. Epoch: 73. Loss: 0.08369837701320648\n",
      "73 train 19 240\n",
      "Phase: train. Epoch: 73. Loss: 0.07010272145271301\n",
      "73 train 20 252\n",
      "Phase: train. Epoch: 73. Loss: 0.07038452476263046\n",
      "73 train 21 264\n",
      "Phase: train. Epoch: 73. Loss: 0.07590122520923615\n",
      "73 train 22 276\n",
      "Phase: train. Epoch: 73. Loss: 0.08183199167251587\n",
      "73 train 23 288\n",
      "Phase: train. Epoch: 73. Loss: 0.07403750717639923\n",
      "73 train 24 300\n",
      "Phase: train. Epoch: 73. Loss: 0.07691312581300735\n",
      "73 train 25 312\n",
      "Phase: train. Epoch: 73. Loss: 0.0826692208647728\n",
      "73 train 26 324\n",
      "Phase: train. Epoch: 73. Loss: 0.09533830732107162\n",
      "73 train 27 336\n",
      "Phase: train. Epoch: 73. Loss: 0.06366581469774246\n",
      "73 train 28 348\n",
      "Phase: train. Epoch: 73. Loss: 0.0850931853055954\n",
      "73 train 29 360\n",
      "Phase: train. Epoch: 73. Loss: 0.10501762479543686\n",
      "73 train 30 372\n",
      "Phase: train. Epoch: 73. Loss: 0.09365354478359222\n",
      "73 train 31 384\n",
      "Phase: train. Epoch: 73. Loss: 0.09183962643146515\n",
      "73 train 32 396\n",
      "Phase: train. Epoch: 73. Loss: 0.13064920902252197\n",
      "73 train 33 408\n",
      "Phase: train. Epoch: 73. Loss: 0.09078436344861984\n",
      "73 train 34 420\n",
      "Phase: train. Epoch: 73. Loss: 0.09450104832649231\n",
      "73 train 35 432\n",
      "Phase: train. Epoch: 73. Loss: 0.0921461433172226\n",
      "73 train 36 444\n",
      "Phase: train. Epoch: 73. Loss: 0.08900515735149384\n",
      "73 train 37 456\n",
      "Phase: train. Epoch: 73. Loss: 0.08166472613811493\n",
      "73 train 38 468\n",
      "Phase: train. Epoch: 73. Loss: 0.07505692541599274\n",
      "73 train 39 480\n",
      "Phase: train. Epoch: 73. Loss: 0.10099372267723083\n",
      "73 train 40 492\n",
      "Phase: train. Epoch: 73. Loss: 0.07175558805465698\n",
      "73 train 41 504\n",
      "Phase: train. Epoch: 73. Loss: 0.08681680262088776\n",
      "73 train 42 516\n",
      "Phase: train. Epoch: 73. Loss: 0.08285751193761826\n",
      "73 train 43 528\n",
      "Phase: train. Epoch: 73. Loss: 0.07748378813266754\n",
      "73 train 44 540\n",
      "Phase: train. Epoch: 73. Loss: 0.0861077755689621\n",
      "73 train 45 552\n",
      "Phase: train. Epoch: 73. Loss: 0.09616413712501526\n",
      "73 train 46 564\n",
      "Phase: train. Epoch: 73. Loss: 0.092934750020504\n",
      "73 train 47 576\n",
      "Phase: train. Epoch: 73. Loss: 0.0803191214799881\n",
      "73 train 48 588\n",
      "Phase: train. Epoch: 73. Loss: 0.08140546083450317\n",
      "73 train 49 600\n",
      "Phase: train. Epoch: 73. Loss: 0.08012758195400238\n",
      "73 train 50 612\n",
      "Phase: train. Epoch: 73. Loss: 0.07277575135231018\n",
      "73 train 51 624\n",
      "Phase: train. Epoch: 73. Loss: 0.08589854091405869\n",
      "73 train 52 636\n",
      "Phase: train. Epoch: 73. Loss: 0.07557624578475952\n",
      "73 train 53 648\n",
      "Phase: train. Epoch: 73. Loss: 0.08391078561544418\n",
      "73 train 54 660\n",
      "Phase: train. Epoch: 73. Loss: 0.07384687662124634\n",
      "73 train 55 672\n",
      "Phase: train. Epoch: 73. Loss: 0.10291185230016708\n",
      "73 train 56 684\n",
      "Phase: train. Epoch: 73. Loss: 0.09044941514730453\n",
      "73 train 57 696\n",
      "Phase: train. Epoch: 73. Loss: 0.08394595235586166\n",
      "73 train 58 708\n",
      "Phase: train. Epoch: 73. Loss: 0.061950087547302246\n",
      "73 train 59 720\n",
      "Phase: train. Epoch: 73. Loss: 0.0712646096944809\n",
      "73 train 60 732\n",
      "Phase: train. Epoch: 73. Loss: 0.07400979846715927\n",
      "73 train 61 744\n",
      "Phase: train. Epoch: 73. Loss: 0.0668826624751091\n",
      "73 train 62 751\n",
      "Phase: train. Epoch: 73. Loss: 0.09434045106172562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 val 0 763\n",
      "Phase: val. Epoch: 73. Loss: 0.07331516593694687\n",
      "73 val 1 775\n",
      "Phase: val. Epoch: 73. Loss: 0.05944705009460449\n",
      "73 val 2 787\n",
      "Phase: val. Epoch: 73. Loss: 0.12242042273283005\n",
      "73 val 3 799\n",
      "Phase: val. Epoch: 73. Loss: 0.06565260142087936\n",
      "73 val 4 811\n",
      "Phase: val. Epoch: 73. Loss: 0.08697742223739624\n",
      "73 val 5 823\n",
      "Phase: val. Epoch: 73. Loss: 0.14394761621952057\n",
      "73 val 6 835\n",
      "Phase: val. Epoch: 73. Loss: 0.07110704481601715\n",
      "73 val 7 847\n",
      "Phase: val. Epoch: 73. Loss: 0.10037102550268173\n",
      "73 val 8 859\n",
      "Phase: val. Epoch: 73. Loss: 0.0806514248251915\n",
      "73 val 9 871\n",
      "Phase: val. Epoch: 73. Loss: 0.0840003490447998\n",
      "73 val 10 883\n",
      "Phase: val. Epoch: 73. Loss: 0.08860411494970322\n",
      "73 val 11 884\n",
      "Phase: val. Epoch: 73. Loss: 0.11898671090602875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 train 0 12\n",
      "Phase: train. Epoch: 74. Loss: 0.05679306387901306\n",
      "74 train 1 24\n",
      "Phase: train. Epoch: 74. Loss: 0.12039463222026825\n",
      "74 train 2 36\n",
      "Phase: train. Epoch: 74. Loss: 0.09041758626699448\n",
      "74 train 3 48\n",
      "Phase: train. Epoch: 74. Loss: 0.07216602563858032\n",
      "74 train 4 60\n",
      "Phase: train. Epoch: 74. Loss: 0.07671447098255157\n",
      "74 train 5 72\n",
      "Phase: train. Epoch: 74. Loss: 0.07599383592605591\n",
      "74 train 6 84\n",
      "Phase: train. Epoch: 74. Loss: 0.07155153900384903\n",
      "74 train 7 96\n",
      "Phase: train. Epoch: 74. Loss: 0.08007834851741791\n",
      "74 train 8 108\n",
      "Phase: train. Epoch: 74. Loss: 0.07749353349208832\n",
      "74 train 9 120\n",
      "Phase: train. Epoch: 74. Loss: 0.10097642987966537\n",
      "74 train 10 132\n",
      "Phase: train. Epoch: 74. Loss: 0.0965738296508789\n",
      "74 train 11 144\n",
      "Phase: train. Epoch: 74. Loss: 0.08563393354415894\n",
      "74 train 12 156\n",
      "Phase: train. Epoch: 74. Loss: 0.08786599338054657\n",
      "74 train 13 168\n",
      "Phase: train. Epoch: 74. Loss: 0.0577705018222332\n",
      "74 train 14 180\n",
      "Phase: train. Epoch: 74. Loss: 0.12097936868667603\n",
      "74 train 15 192\n",
      "Phase: train. Epoch: 74. Loss: 0.08590971678495407\n",
      "74 train 16 204\n",
      "Phase: train. Epoch: 74. Loss: 0.05534946918487549\n",
      "74 train 17 216\n",
      "Phase: train. Epoch: 74. Loss: 0.11418671905994415\n",
      "74 train 18 228\n",
      "Phase: train. Epoch: 74. Loss: 0.08180107176303864\n",
      "74 train 19 240\n",
      "Phase: train. Epoch: 74. Loss: 0.08446186780929565\n",
      "74 train 20 252\n",
      "Phase: train. Epoch: 74. Loss: 0.07657668739557266\n",
      "74 train 21 264\n",
      "Phase: train. Epoch: 74. Loss: 0.09269523620605469\n",
      "74 train 22 276\n",
      "Phase: train. Epoch: 74. Loss: 0.0787556990981102\n",
      "74 train 23 288\n",
      "Phase: train. Epoch: 74. Loss: 0.08992769569158554\n",
      "74 train 24 300\n",
      "Phase: train. Epoch: 74. Loss: 0.09164446592330933\n",
      "74 train 25 312\n",
      "Phase: train. Epoch: 74. Loss: 0.13736578822135925\n",
      "74 train 26 324\n",
      "Phase: train. Epoch: 74. Loss: 0.1027643084526062\n",
      "74 train 27 336\n",
      "Phase: train. Epoch: 74. Loss: 0.06359299272298813\n",
      "74 train 28 348\n",
      "Phase: train. Epoch: 74. Loss: 0.07632746547460556\n",
      "74 train 29 360\n",
      "Phase: train. Epoch: 74. Loss: 0.07370174676179886\n",
      "74 train 30 372\n",
      "Phase: train. Epoch: 74. Loss: 0.08552976697683334\n",
      "74 train 31 384\n",
      "Phase: train. Epoch: 74. Loss: 0.08071692287921906\n",
      "74 train 32 396\n",
      "Phase: train. Epoch: 74. Loss: 0.08468453586101532\n",
      "74 train 33 408\n",
      "Phase: train. Epoch: 74. Loss: 0.07105857133865356\n",
      "74 train 34 420\n",
      "Phase: train. Epoch: 74. Loss: 0.07570954412221909\n",
      "74 train 35 432\n",
      "Phase: train. Epoch: 74. Loss: 0.07396119832992554\n",
      "74 train 36 444\n",
      "Phase: train. Epoch: 74. Loss: 0.10107503831386566\n",
      "74 train 37 456\n",
      "Phase: train. Epoch: 74. Loss: 0.08190227299928665\n",
      "74 train 38 468\n",
      "Phase: train. Epoch: 74. Loss: 0.07329311966896057\n",
      "74 train 39 480\n",
      "Phase: train. Epoch: 74. Loss: 0.08693674206733704\n",
      "74 train 40 492\n",
      "Phase: train. Epoch: 74. Loss: 0.08788727223873138\n",
      "74 train 41 504\n",
      "Phase: train. Epoch: 74. Loss: 0.11238797008991241\n",
      "74 train 42 516\n",
      "Phase: train. Epoch: 74. Loss: 0.07082165777683258\n",
      "74 train 43 528\n",
      "Phase: train. Epoch: 74. Loss: 0.07406532764434814\n",
      "74 train 44 540\n",
      "Phase: train. Epoch: 74. Loss: 0.09525519609451294\n",
      "74 train 45 552\n",
      "Phase: train. Epoch: 74. Loss: 0.06952700018882751\n",
      "74 train 46 564\n",
      "Phase: train. Epoch: 74. Loss: 0.07493452727794647\n",
      "74 train 47 576\n",
      "Phase: train. Epoch: 74. Loss: 0.09101033210754395\n",
      "74 train 48 588\n",
      "Phase: train. Epoch: 74. Loss: 0.080152228474617\n",
      "74 train 49 600\n",
      "Phase: train. Epoch: 74. Loss: 0.07708083093166351\n",
      "74 train 50 612\n",
      "Phase: train. Epoch: 74. Loss: 0.09573811292648315\n",
      "74 train 51 624\n",
      "Phase: train. Epoch: 74. Loss: 0.09247072786092758\n",
      "74 train 52 636\n",
      "Phase: train. Epoch: 74. Loss: 0.13980203866958618\n",
      "74 train 53 648\n",
      "Phase: train. Epoch: 74. Loss: 0.0717773586511612\n",
      "74 train 54 660\n",
      "Phase: train. Epoch: 74. Loss: 0.06719376146793365\n",
      "74 train 55 672\n",
      "Phase: train. Epoch: 74. Loss: 0.10412349551916122\n",
      "74 train 56 684\n",
      "Phase: train. Epoch: 74. Loss: 0.09254001080989838\n",
      "74 train 57 696\n",
      "Phase: train. Epoch: 74. Loss: 0.07182694971561432\n",
      "74 train 58 708\n",
      "Phase: train. Epoch: 74. Loss: 0.10984480381011963\n",
      "74 train 59 720\n",
      "Phase: train. Epoch: 74. Loss: 0.10973730683326721\n",
      "74 train 60 732\n",
      "Phase: train. Epoch: 74. Loss: 0.09658762812614441\n",
      "74 train 61 744\n",
      "Phase: train. Epoch: 74. Loss: 0.09379260241985321\n",
      "74 train 62 751\n",
      "Phase: train. Epoch: 74. Loss: 0.0566541850566864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 val 0 763\n",
      "Phase: val. Epoch: 74. Loss: 0.09665443748235703\n",
      "74 val 1 775\n",
      "Phase: val. Epoch: 74. Loss: 0.09229417890310287\n",
      "74 val 2 787\n",
      "Phase: val. Epoch: 74. Loss: 0.08092769235372543\n",
      "74 val 3 799\n",
      "Phase: val. Epoch: 74. Loss: 0.06706929951906204\n",
      "74 val 4 811\n",
      "Phase: val. Epoch: 74. Loss: 0.06365221738815308\n",
      "74 val 5 823\n",
      "Phase: val. Epoch: 74. Loss: 0.07963351905345917\n",
      "74 val 6 835\n",
      "Phase: val. Epoch: 74. Loss: 0.0967981219291687\n",
      "74 val 7 847\n",
      "Phase: val. Epoch: 74. Loss: 0.0674525797367096\n",
      "74 val 8 859\n",
      "Phase: val. Epoch: 74. Loss: 0.07158242911100388\n",
      "74 val 9 871\n",
      "Phase: val. Epoch: 74. Loss: 0.09180474281311035\n",
      "74 val 10 883\n",
      "Phase: val. Epoch: 74. Loss: 0.07863369584083557\n",
      "74 val 11 884\n",
      "Phase: val. Epoch: 74. Loss: 0.10937294363975525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 train 0 12\n",
      "Phase: train. Epoch: 75. Loss: 0.07635815441608429\n",
      "75 train 1 24\n",
      "Phase: train. Epoch: 75. Loss: 0.11221926659345627\n",
      "75 train 2 36\n",
      "Phase: train. Epoch: 75. Loss: 0.07728970050811768\n",
      "75 train 3 48\n",
      "Phase: train. Epoch: 75. Loss: 0.06846959888935089\n",
      "75 train 4 60\n",
      "Phase: train. Epoch: 75. Loss: 0.07151897251605988\n",
      "75 train 5 72\n",
      "Phase: train. Epoch: 75. Loss: 0.06424327194690704\n",
      "75 train 6 84\n",
      "Phase: train. Epoch: 75. Loss: 0.09152331203222275\n",
      "75 train 7 96\n",
      "Phase: train. Epoch: 75. Loss: 0.08037552237510681\n",
      "75 train 8 108\n",
      "Phase: train. Epoch: 75. Loss: 0.07436351478099823\n",
      "75 train 9 120\n",
      "Phase: train. Epoch: 75. Loss: 0.09435175359249115\n",
      "75 train 10 132\n",
      "Phase: train. Epoch: 75. Loss: 0.08506235480308533\n",
      "75 train 11 144\n",
      "Phase: train. Epoch: 75. Loss: 0.07539987564086914\n",
      "75 train 12 156\n",
      "Phase: train. Epoch: 75. Loss: 0.08631561696529388\n",
      "75 train 13 168\n",
      "Phase: train. Epoch: 75. Loss: 0.07741628587245941\n",
      "75 train 14 180\n",
      "Phase: train. Epoch: 75. Loss: 0.0851675495505333\n",
      "75 train 15 192\n",
      "Phase: train. Epoch: 75. Loss: 0.10338860750198364\n",
      "75 train 16 204\n",
      "Phase: train. Epoch: 75. Loss: 0.0932852178812027\n",
      "75 train 17 216\n",
      "Phase: train. Epoch: 75. Loss: 0.08494915068149567\n",
      "75 train 18 228\n",
      "Phase: train. Epoch: 75. Loss: 0.08737822622060776\n",
      "75 train 19 240\n",
      "Phase: train. Epoch: 75. Loss: 0.08064872026443481\n",
      "75 train 20 252\n",
      "Phase: train. Epoch: 75. Loss: 0.05432867631316185\n",
      "75 train 21 264\n",
      "Phase: train. Epoch: 75. Loss: 0.11134602129459381\n",
      "75 train 22 276\n",
      "Phase: train. Epoch: 75. Loss: 0.07064458727836609\n",
      "75 train 23 288\n",
      "Phase: train. Epoch: 75. Loss: 0.06515061855316162\n",
      "75 train 24 300\n",
      "Phase: train. Epoch: 75. Loss: 0.0761989951133728\n",
      "75 train 25 312\n",
      "Phase: train. Epoch: 75. Loss: 0.07826828211545944\n",
      "75 train 26 324\n",
      "Phase: train. Epoch: 75. Loss: 0.07130834460258484\n",
      "75 train 27 336\n",
      "Phase: train. Epoch: 75. Loss: 0.12282630056142807\n",
      "75 train 28 348\n",
      "Phase: train. Epoch: 75. Loss: 0.11477634310722351\n",
      "75 train 29 360\n",
      "Phase: train. Epoch: 75. Loss: 0.0876597985625267\n",
      "75 train 30 372\n",
      "Phase: train. Epoch: 75. Loss: 0.09609566628932953\n",
      "75 train 31 384\n",
      "Phase: train. Epoch: 75. Loss: 0.049312882125377655\n",
      "75 train 32 396\n",
      "Phase: train. Epoch: 75. Loss: 0.07223289459943771\n",
      "75 train 33 408\n",
      "Phase: train. Epoch: 75. Loss: 0.10056950896978378\n",
      "75 train 34 420\n",
      "Phase: train. Epoch: 75. Loss: 0.12136624753475189\n",
      "75 train 35 432\n",
      "Phase: train. Epoch: 75. Loss: 0.07358494400978088\n",
      "75 train 36 444\n",
      "Phase: train. Epoch: 75. Loss: 0.07187316566705704\n",
      "75 train 37 456\n",
      "Phase: train. Epoch: 75. Loss: 0.11388089507818222\n",
      "75 train 38 468\n",
      "Phase: train. Epoch: 75. Loss: 0.09550303220748901\n",
      "75 train 39 480\n",
      "Phase: train. Epoch: 75. Loss: 0.08296258747577667\n",
      "75 train 40 492\n",
      "Phase: train. Epoch: 75. Loss: 0.09988674521446228\n",
      "75 train 41 504\n",
      "Phase: train. Epoch: 75. Loss: 0.0904754027724266\n",
      "75 train 42 516\n",
      "Phase: train. Epoch: 75. Loss: 0.09426575899124146\n",
      "75 train 43 528\n",
      "Phase: train. Epoch: 75. Loss: 0.07120400667190552\n",
      "75 train 44 540\n",
      "Phase: train. Epoch: 75. Loss: 0.08593650907278061\n",
      "75 train 45 552\n",
      "Phase: train. Epoch: 75. Loss: 0.07537078857421875\n",
      "75 train 46 564\n",
      "Phase: train. Epoch: 75. Loss: 0.06108476221561432\n",
      "75 train 47 576\n",
      "Phase: train. Epoch: 75. Loss: 0.055879682302474976\n",
      "75 train 48 588\n",
      "Phase: train. Epoch: 75. Loss: 0.08235730975866318\n",
      "75 train 49 600\n",
      "Phase: train. Epoch: 75. Loss: 0.06431695818901062\n",
      "75 train 50 612\n",
      "Phase: train. Epoch: 75. Loss: 0.08322124928236008\n",
      "75 train 51 624\n",
      "Phase: train. Epoch: 75. Loss: 0.0870368704199791\n",
      "75 train 52 636\n",
      "Phase: train. Epoch: 75. Loss: 0.10028251260519028\n",
      "75 train 53 648\n",
      "Phase: train. Epoch: 75. Loss: 0.09033755958080292\n",
      "75 train 54 660\n",
      "Phase: train. Epoch: 75. Loss: 0.0997251495718956\n",
      "75 train 55 672\n",
      "Phase: train. Epoch: 75. Loss: 0.09266501665115356\n",
      "75 train 56 684\n",
      "Phase: train. Epoch: 75. Loss: 0.1239880919456482\n",
      "75 train 57 696\n",
      "Phase: train. Epoch: 75. Loss: 0.07073193788528442\n",
      "75 train 58 708\n",
      "Phase: train. Epoch: 75. Loss: 0.06553906202316284\n",
      "75 train 59 720\n",
      "Phase: train. Epoch: 75. Loss: 0.09405793249607086\n",
      "75 train 60 732\n",
      "Phase: train. Epoch: 75. Loss: 0.08602167665958405\n",
      "75 train 61 744\n",
      "Phase: train. Epoch: 75. Loss: 0.10097303241491318\n",
      "75 train 62 751\n",
      "Phase: train. Epoch: 75. Loss: 0.07797510921955109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 val 0 763\n",
      "Phase: val. Epoch: 75. Loss: 0.11172214150428772\n",
      "75 val 1 775\n",
      "Phase: val. Epoch: 75. Loss: 0.08053448796272278\n",
      "75 val 2 787\n",
      "Phase: val. Epoch: 75. Loss: 0.06025533005595207\n",
      "75 val 3 799\n",
      "Phase: val. Epoch: 75. Loss: 0.08592469990253448\n",
      "75 val 4 811\n",
      "Phase: val. Epoch: 75. Loss: 0.07411099970340729\n",
      "75 val 5 823\n",
      "Phase: val. Epoch: 75. Loss: 0.08685745298862457\n",
      "75 val 6 835\n",
      "Phase: val. Epoch: 75. Loss: 0.08676287531852722\n",
      "75 val 7 847\n",
      "Phase: val. Epoch: 75. Loss: 0.09234128892421722\n",
      "75 val 8 859\n",
      "Phase: val. Epoch: 75. Loss: 0.07253729552030563\n",
      "75 val 9 871\n",
      "Phase: val. Epoch: 75. Loss: 0.09324173629283905\n",
      "75 val 10 883\n",
      "Phase: val. Epoch: 75. Loss: 0.09472334384918213\n",
      "75 val 11 884\n",
      "Phase: val. Epoch: 75. Loss: 0.025939399376511574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 train 0 12\n",
      "Phase: train. Epoch: 76. Loss: 0.11555598676204681\n",
      "76 train 1 24\n",
      "Phase: train. Epoch: 76. Loss: 0.08917677402496338\n",
      "76 train 2 36\n",
      "Phase: train. Epoch: 76. Loss: 0.0797782838344574\n",
      "76 train 3 48\n",
      "Phase: train. Epoch: 76. Loss: 0.09163063764572144\n",
      "76 train 4 60\n",
      "Phase: train. Epoch: 76. Loss: 0.06769943237304688\n",
      "76 train 5 72\n",
      "Phase: train. Epoch: 76. Loss: 0.07537996768951416\n",
      "76 train 6 84\n",
      "Phase: train. Epoch: 76. Loss: 0.08684088289737701\n",
      "76 train 7 96\n",
      "Phase: train. Epoch: 76. Loss: 0.07718051970005035\n",
      "76 train 8 108\n",
      "Phase: train. Epoch: 76. Loss: 0.07468177378177643\n",
      "76 train 9 120\n",
      "Phase: train. Epoch: 76. Loss: 0.08214569091796875\n",
      "76 train 10 132\n",
      "Phase: train. Epoch: 76. Loss: 0.06916560232639313\n",
      "76 train 11 144\n",
      "Phase: train. Epoch: 76. Loss: 0.079691581428051\n",
      "76 train 12 156\n",
      "Phase: train. Epoch: 76. Loss: 0.08631223440170288\n",
      "76 train 13 168\n",
      "Phase: train. Epoch: 76. Loss: 0.07848730683326721\n",
      "76 train 14 180\n",
      "Phase: train. Epoch: 76. Loss: 0.07060819864273071\n",
      "76 train 15 192\n",
      "Phase: train. Epoch: 76. Loss: 0.07951755821704865\n",
      "76 train 16 204\n",
      "Phase: train. Epoch: 76. Loss: 0.0841166228055954\n",
      "76 train 17 216\n",
      "Phase: train. Epoch: 76. Loss: 0.14527828991413116\n",
      "76 train 18 228\n",
      "Phase: train. Epoch: 76. Loss: 0.09362052381038666\n",
      "76 train 19 240\n",
      "Phase: train. Epoch: 76. Loss: 0.07238182425498962\n",
      "76 train 20 252\n",
      "Phase: train. Epoch: 76. Loss: 0.08254273980855942\n",
      "76 train 21 264\n",
      "Phase: train. Epoch: 76. Loss: 0.07639007270336151\n",
      "76 train 22 276\n",
      "Phase: train. Epoch: 76. Loss: 0.08081214874982834\n",
      "76 train 23 288\n",
      "Phase: train. Epoch: 76. Loss: 0.09131364524364471\n",
      "76 train 24 300\n",
      "Phase: train. Epoch: 76. Loss: 0.10217560827732086\n",
      "76 train 25 312\n",
      "Phase: train. Epoch: 76. Loss: 0.0919593870639801\n",
      "76 train 26 324\n",
      "Phase: train. Epoch: 76. Loss: 0.06820644438266754\n",
      "76 train 27 336\n",
      "Phase: train. Epoch: 76. Loss: 0.07404004037380219\n",
      "76 train 28 348\n",
      "Phase: train. Epoch: 76. Loss: 0.1323561817407608\n",
      "76 train 29 360\n",
      "Phase: train. Epoch: 76. Loss: 0.08680909126996994\n",
      "76 train 30 372\n",
      "Phase: train. Epoch: 76. Loss: 0.08656974881887436\n",
      "76 train 31 384\n",
      "Phase: train. Epoch: 76. Loss: 0.08189278841018677\n",
      "76 train 32 396\n",
      "Phase: train. Epoch: 76. Loss: 0.067253977060318\n",
      "76 train 33 408\n",
      "Phase: train. Epoch: 76. Loss: 0.09727822244167328\n",
      "76 train 34 420\n",
      "Phase: train. Epoch: 76. Loss: 0.08343570679426193\n",
      "76 train 35 432\n",
      "Phase: train. Epoch: 76. Loss: 0.1152210533618927\n",
      "76 train 36 444\n",
      "Phase: train. Epoch: 76. Loss: 0.09880205243825912\n",
      "76 train 37 456\n",
      "Phase: train. Epoch: 76. Loss: 0.09847724437713623\n",
      "76 train 38 468\n",
      "Phase: train. Epoch: 76. Loss: 0.07221309840679169\n",
      "76 train 39 480\n",
      "Phase: train. Epoch: 76. Loss: 0.08542400598526001\n",
      "76 train 40 492\n",
      "Phase: train. Epoch: 76. Loss: 0.09259003400802612\n",
      "76 train 41 504\n",
      "Phase: train. Epoch: 76. Loss: 0.08903567492961884\n",
      "76 train 42 516\n",
      "Phase: train. Epoch: 76. Loss: 0.07990577071905136\n",
      "76 train 43 528\n",
      "Phase: train. Epoch: 76. Loss: 0.07554490119218826\n",
      "76 train 44 540\n",
      "Phase: train. Epoch: 76. Loss: 0.07059233635663986\n",
      "76 train 45 552\n",
      "Phase: train. Epoch: 76. Loss: 0.07300213724374771\n",
      "76 train 46 564\n",
      "Phase: train. Epoch: 76. Loss: 0.10322067141532898\n",
      "76 train 47 576\n",
      "Phase: train. Epoch: 76. Loss: 0.09286467730998993\n",
      "76 train 48 588\n",
      "Phase: train. Epoch: 76. Loss: 0.07349623739719391\n",
      "76 train 49 600\n",
      "Phase: train. Epoch: 76. Loss: 0.08960896730422974\n",
      "76 train 50 612\n",
      "Phase: train. Epoch: 76. Loss: 0.08198198676109314\n",
      "76 train 51 624\n",
      "Phase: train. Epoch: 76. Loss: 0.080960214138031\n",
      "76 train 52 636\n",
      "Phase: train. Epoch: 76. Loss: 0.10840737074613571\n",
      "76 train 53 648\n",
      "Phase: train. Epoch: 76. Loss: 0.07872439175844193\n",
      "76 train 54 660\n",
      "Phase: train. Epoch: 76. Loss: 0.0708482563495636\n",
      "76 train 55 672\n",
      "Phase: train. Epoch: 76. Loss: 0.08818253874778748\n",
      "76 train 56 684\n",
      "Phase: train. Epoch: 76. Loss: 0.06336972117424011\n",
      "76 train 57 696\n",
      "Phase: train. Epoch: 76. Loss: 0.10813111066818237\n",
      "76 train 58 708\n",
      "Phase: train. Epoch: 76. Loss: 0.07923923432826996\n",
      "76 train 59 720\n",
      "Phase: train. Epoch: 76. Loss: 0.10743701457977295\n",
      "76 train 60 732\n",
      "Phase: train. Epoch: 76. Loss: 0.09009939432144165\n",
      "76 train 61 744\n",
      "Phase: train. Epoch: 76. Loss: 0.07145722210407257\n",
      "76 train 62 751\n",
      "Phase: train. Epoch: 76. Loss: 0.05749087035655975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 val 0 763\n",
      "Phase: val. Epoch: 76. Loss: 0.05909724161028862\n",
      "76 val 1 775\n",
      "Phase: val. Epoch: 76. Loss: 0.06332138180732727\n",
      "76 val 2 787\n",
      "Phase: val. Epoch: 76. Loss: 0.05332842469215393\n",
      "76 val 3 799\n",
      "Phase: val. Epoch: 76. Loss: 0.08581024408340454\n",
      "76 val 4 811\n",
      "Phase: val. Epoch: 76. Loss: 0.0896298810839653\n",
      "76 val 5 823\n",
      "Phase: val. Epoch: 76. Loss: 0.07268377393484116\n",
      "76 val 6 835\n",
      "Phase: val. Epoch: 76. Loss: 0.09316138923168182\n",
      "76 val 7 847\n",
      "Phase: val. Epoch: 76. Loss: 0.12925821542739868\n",
      "76 val 8 859\n",
      "Phase: val. Epoch: 76. Loss: 0.06550951302051544\n",
      "76 val 9 871\n",
      "Phase: val. Epoch: 76. Loss: 0.11957567930221558\n",
      "76 val 10 883\n",
      "Phase: val. Epoch: 76. Loss: 0.08891241997480392\n",
      "76 val 11 884\n",
      "Phase: val. Epoch: 76. Loss: 0.026252955198287964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 train 0 12\n",
      "Phase: train. Epoch: 77. Loss: 0.08510659635066986\n",
      "77 train 1 24\n",
      "Phase: train. Epoch: 77. Loss: 0.08847993612289429\n",
      "77 train 2 36\n",
      "Phase: train. Epoch: 77. Loss: 0.10630147159099579\n",
      "77 train 3 48\n",
      "Phase: train. Epoch: 77. Loss: 0.08121885359287262\n",
      "77 train 4 60\n",
      "Phase: train. Epoch: 77. Loss: 0.0742209255695343\n",
      "77 train 5 72\n",
      "Phase: train. Epoch: 77. Loss: 0.09423800557851791\n",
      "77 train 6 84\n",
      "Phase: train. Epoch: 77. Loss: 0.12365925312042236\n",
      "77 train 7 96\n",
      "Phase: train. Epoch: 77. Loss: 0.09063731878995895\n",
      "77 train 8 108\n",
      "Phase: train. Epoch: 77. Loss: 0.07604280114173889\n",
      "77 train 9 120\n",
      "Phase: train. Epoch: 77. Loss: 0.07817023992538452\n",
      "77 train 10 132\n",
      "Phase: train. Epoch: 77. Loss: 0.10388872027397156\n",
      "77 train 11 144\n",
      "Phase: train. Epoch: 77. Loss: 0.08628584444522858\n",
      "77 train 12 156\n",
      "Phase: train. Epoch: 77. Loss: 0.08254324644804001\n",
      "77 train 13 168\n",
      "Phase: train. Epoch: 77. Loss: 0.07756050676107407\n",
      "77 train 14 180\n",
      "Phase: train. Epoch: 77. Loss: 0.06807190179824829\n",
      "77 train 15 192\n",
      "Phase: train. Epoch: 77. Loss: 0.09588846564292908\n",
      "77 train 16 204\n",
      "Phase: train. Epoch: 77. Loss: 0.07469119131565094\n",
      "77 train 17 216\n",
      "Phase: train. Epoch: 77. Loss: 0.09428714215755463\n",
      "77 train 18 228\n",
      "Phase: train. Epoch: 77. Loss: 0.11446762084960938\n",
      "77 train 19 240\n",
      "Phase: train. Epoch: 77. Loss: 0.07178877294063568\n",
      "77 train 20 252\n",
      "Phase: train. Epoch: 77. Loss: 0.07937894761562347\n",
      "77 train 21 264\n",
      "Phase: train. Epoch: 77. Loss: 0.08924239873886108\n",
      "77 train 22 276\n",
      "Phase: train. Epoch: 77. Loss: 0.08393736183643341\n",
      "77 train 23 288\n",
      "Phase: train. Epoch: 77. Loss: 0.08633586019277573\n",
      "77 train 24 300\n",
      "Phase: train. Epoch: 77. Loss: 0.07651527971029282\n",
      "77 train 25 312\n",
      "Phase: train. Epoch: 77. Loss: 0.08526628464460373\n",
      "77 train 26 324\n",
      "Phase: train. Epoch: 77. Loss: 0.06744974106550217\n",
      "77 train 27 336\n",
      "Phase: train. Epoch: 77. Loss: 0.08575411140918732\n",
      "77 train 28 348\n",
      "Phase: train. Epoch: 77. Loss: 0.06811460107564926\n",
      "77 train 29 360\n",
      "Phase: train. Epoch: 77. Loss: 0.05621042847633362\n",
      "77 train 30 372\n",
      "Phase: train. Epoch: 77. Loss: 0.1124497503042221\n",
      "77 train 31 384\n",
      "Phase: train. Epoch: 77. Loss: 0.07097397744655609\n",
      "77 train 32 396\n",
      "Phase: train. Epoch: 77. Loss: 0.08133865892887115\n",
      "77 train 33 408\n",
      "Phase: train. Epoch: 77. Loss: 0.07885174453258514\n",
      "77 train 34 420\n",
      "Phase: train. Epoch: 77. Loss: 0.06730988621711731\n",
      "77 train 35 432\n",
      "Phase: train. Epoch: 77. Loss: 0.09321783483028412\n",
      "77 train 36 444\n",
      "Phase: train. Epoch: 77. Loss: 0.09684698283672333\n",
      "77 train 37 456\n",
      "Phase: train. Epoch: 77. Loss: 0.09931200742721558\n",
      "77 train 38 468\n",
      "Phase: train. Epoch: 77. Loss: 0.09118056297302246\n",
      "77 train 39 480\n",
      "Phase: train. Epoch: 77. Loss: 0.06999112665653229\n",
      "77 train 40 492\n",
      "Phase: train. Epoch: 77. Loss: 0.0887555181980133\n",
      "77 train 41 504\n",
      "Phase: train. Epoch: 77. Loss: 0.07678299397230148\n",
      "77 train 42 516\n",
      "Phase: train. Epoch: 77. Loss: 0.1021520346403122\n",
      "77 train 43 528\n",
      "Phase: train. Epoch: 77. Loss: 0.07118187844753265\n",
      "77 train 44 540\n",
      "Phase: train. Epoch: 77. Loss: 0.07516950368881226\n",
      "77 train 45 552\n",
      "Phase: train. Epoch: 77. Loss: 0.0779971182346344\n",
      "77 train 46 564\n",
      "Phase: train. Epoch: 77. Loss: 0.06527933478355408\n",
      "77 train 47 576\n",
      "Phase: train. Epoch: 77. Loss: 0.09205774962902069\n",
      "77 train 48 588\n",
      "Phase: train. Epoch: 77. Loss: 0.06814508140087128\n",
      "77 train 49 600\n",
      "Phase: train. Epoch: 77. Loss: 0.09838686883449554\n",
      "77 train 50 612\n",
      "Phase: train. Epoch: 77. Loss: 0.07989208400249481\n",
      "77 train 51 624\n",
      "Phase: train. Epoch: 77. Loss: 0.0603976808488369\n",
      "77 train 52 636\n",
      "Phase: train. Epoch: 77. Loss: 0.07839373499155045\n",
      "77 train 53 648\n",
      "Phase: train. Epoch: 77. Loss: 0.08443406224250793\n",
      "77 train 54 660\n",
      "Phase: train. Epoch: 77. Loss: 0.07321862876415253\n",
      "77 train 55 672\n",
      "Phase: train. Epoch: 77. Loss: 0.09085550904273987\n",
      "77 train 56 684\n",
      "Phase: train. Epoch: 77. Loss: 0.08391225337982178\n",
      "77 train 57 696\n",
      "Phase: train. Epoch: 77. Loss: 0.10277044773101807\n",
      "77 train 58 708\n",
      "Phase: train. Epoch: 77. Loss: 0.0693463608622551\n",
      "77 train 59 720\n",
      "Phase: train. Epoch: 77. Loss: 0.08626167476177216\n",
      "77 train 60 732\n",
      "Phase: train. Epoch: 77. Loss: 0.09915357828140259\n",
      "77 train 61 744\n",
      "Phase: train. Epoch: 77. Loss: 0.07988736033439636\n",
      "77 train 62 751\n",
      "Phase: train. Epoch: 77. Loss: 0.06677871197462082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 val 0 763\n",
      "Phase: val. Epoch: 77. Loss: 0.08345450460910797\n",
      "77 val 1 775\n",
      "Phase: val. Epoch: 77. Loss: 0.05529416352510452\n",
      "77 val 2 787\n",
      "Phase: val. Epoch: 77. Loss: 0.06565850228071213\n",
      "77 val 3 799\n",
      "Phase: val. Epoch: 77. Loss: 0.06571026891469955\n",
      "77 val 4 811\n",
      "Phase: val. Epoch: 77. Loss: 0.0559820756316185\n",
      "77 val 5 823\n",
      "Phase: val. Epoch: 77. Loss: 0.0912850946187973\n",
      "77 val 6 835\n",
      "Phase: val. Epoch: 77. Loss: 0.10440095514059067\n",
      "77 val 7 847\n",
      "Phase: val. Epoch: 77. Loss: 0.08209839463233948\n",
      "77 val 8 859\n",
      "Phase: val. Epoch: 77. Loss: 0.09776046127080917\n",
      "77 val 9 871\n",
      "Phase: val. Epoch: 77. Loss: 0.10093815624713898\n",
      "77 val 10 883\n",
      "Phase: val. Epoch: 77. Loss: 0.10610794275999069\n",
      "77 val 11 884\n",
      "Phase: val. Epoch: 77. Loss: 0.02671879157423973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 train 0 12\n",
      "Phase: train. Epoch: 78. Loss: 0.08182483911514282\n",
      "78 train 1 24\n",
      "Phase: train. Epoch: 78. Loss: 0.09008397161960602\n",
      "78 train 2 36\n",
      "Phase: train. Epoch: 78. Loss: 0.07576283812522888\n",
      "78 train 3 48\n",
      "Phase: train. Epoch: 78. Loss: 0.0718991607427597\n",
      "78 train 4 60\n",
      "Phase: train. Epoch: 78. Loss: 0.08297277987003326\n",
      "78 train 5 72\n",
      "Phase: train. Epoch: 78. Loss: 0.11255737394094467\n",
      "78 train 6 84\n",
      "Phase: train. Epoch: 78. Loss: 0.10757081210613251\n",
      "78 train 7 96\n",
      "Phase: train. Epoch: 78. Loss: 0.07734227180480957\n",
      "78 train 8 108\n",
      "Phase: train. Epoch: 78. Loss: 0.08534517139196396\n",
      "78 train 9 120\n",
      "Phase: train. Epoch: 78. Loss: 0.116965651512146\n",
      "78 train 10 132\n",
      "Phase: train. Epoch: 78. Loss: 0.07112519443035126\n",
      "78 train 11 144\n",
      "Phase: train. Epoch: 78. Loss: 0.06050528585910797\n",
      "78 train 12 156\n",
      "Phase: train. Epoch: 78. Loss: 0.11564020812511444\n",
      "78 train 13 168\n",
      "Phase: train. Epoch: 78. Loss: 0.06951379776000977\n",
      "78 train 14 180\n",
      "Phase: train. Epoch: 78. Loss: 0.0817030668258667\n",
      "78 train 15 192\n",
      "Phase: train. Epoch: 78. Loss: 0.07373255491256714\n",
      "78 train 16 204\n",
      "Phase: train. Epoch: 78. Loss: 0.09983320534229279\n",
      "78 train 17 216\n",
      "Phase: train. Epoch: 78. Loss: 0.0713018923997879\n",
      "78 train 18 228\n",
      "Phase: train. Epoch: 78. Loss: 0.05773181468248367\n",
      "78 train 19 240\n",
      "Phase: train. Epoch: 78. Loss: 0.10395215451717377\n",
      "78 train 20 252\n",
      "Phase: train. Epoch: 78. Loss: 0.07100044935941696\n",
      "78 train 21 264\n",
      "Phase: train. Epoch: 78. Loss: 0.06315362453460693\n",
      "78 train 22 276\n",
      "Phase: train. Epoch: 78. Loss: 0.06792229413986206\n",
      "78 train 23 288\n",
      "Phase: train. Epoch: 78. Loss: 0.0835854634642601\n",
      "78 train 24 300\n",
      "Phase: train. Epoch: 78. Loss: 0.06954386085271835\n",
      "78 train 25 312\n",
      "Phase: train. Epoch: 78. Loss: 0.09291934967041016\n",
      "78 train 26 324\n",
      "Phase: train. Epoch: 78. Loss: 0.09742866456508636\n",
      "78 train 27 336\n",
      "Phase: train. Epoch: 78. Loss: 0.07482308149337769\n",
      "78 train 28 348\n",
      "Phase: train. Epoch: 78. Loss: 0.12437252700328827\n",
      "78 train 29 360\n",
      "Phase: train. Epoch: 78. Loss: 0.07525917887687683\n",
      "78 train 30 372\n",
      "Phase: train. Epoch: 78. Loss: 0.10720857232809067\n",
      "78 train 31 384\n",
      "Phase: train. Epoch: 78. Loss: 0.07004791498184204\n",
      "78 train 32 396\n",
      "Phase: train. Epoch: 78. Loss: 0.12290871143341064\n",
      "78 train 33 408\n",
      "Phase: train. Epoch: 78. Loss: 0.09018202126026154\n",
      "78 train 34 420\n",
      "Phase: train. Epoch: 78. Loss: 0.0910368487238884\n",
      "78 train 35 432\n",
      "Phase: train. Epoch: 78. Loss: 0.12435247749090195\n",
      "78 train 36 444\n",
      "Phase: train. Epoch: 78. Loss: 0.0580584816634655\n",
      "78 train 37 456\n",
      "Phase: train. Epoch: 78. Loss: 0.07261466979980469\n",
      "78 train 38 468\n",
      "Phase: train. Epoch: 78. Loss: 0.07730679214000702\n",
      "78 train 39 480\n",
      "Phase: train. Epoch: 78. Loss: 0.09557422250509262\n",
      "78 train 40 492\n",
      "Phase: train. Epoch: 78. Loss: 0.06954479217529297\n",
      "78 train 41 504\n",
      "Phase: train. Epoch: 78. Loss: 0.09895392507314682\n",
      "78 train 42 516\n",
      "Phase: train. Epoch: 78. Loss: 0.06069473177194595\n",
      "78 train 43 528\n",
      "Phase: train. Epoch: 78. Loss: 0.08898713439702988\n",
      "78 train 44 540\n",
      "Phase: train. Epoch: 78. Loss: 0.06558090448379517\n",
      "78 train 45 552\n",
      "Phase: train. Epoch: 78. Loss: 0.09315161406993866\n",
      "78 train 46 564\n",
      "Phase: train. Epoch: 78. Loss: 0.08605432510375977\n",
      "78 train 47 576\n",
      "Phase: train. Epoch: 78. Loss: 0.07425829768180847\n",
      "78 train 48 588\n",
      "Phase: train. Epoch: 78. Loss: 0.09291183948516846\n",
      "78 train 49 600\n",
      "Phase: train. Epoch: 78. Loss: 0.07561246305704117\n",
      "78 train 50 612\n",
      "Phase: train. Epoch: 78. Loss: 0.08021029829978943\n",
      "78 train 51 624\n",
      "Phase: train. Epoch: 78. Loss: 0.06754651665687561\n",
      "78 train 52 636\n",
      "Phase: train. Epoch: 78. Loss: 0.0866888090968132\n",
      "78 train 53 648\n",
      "Phase: train. Epoch: 78. Loss: 0.06237052008509636\n",
      "78 train 54 660\n",
      "Phase: train. Epoch: 78. Loss: 0.08079707622528076\n",
      "78 train 55 672\n",
      "Phase: train. Epoch: 78. Loss: 0.09135236591100693\n",
      "78 train 56 684\n",
      "Phase: train. Epoch: 78. Loss: 0.07885759323835373\n",
      "78 train 57 696\n",
      "Phase: train. Epoch: 78. Loss: 0.09214484691619873\n",
      "78 train 58 708\n",
      "Phase: train. Epoch: 78. Loss: 0.07158234715461731\n",
      "78 train 59 720\n",
      "Phase: train. Epoch: 78. Loss: 0.08922451734542847\n",
      "78 train 60 732\n",
      "Phase: train. Epoch: 78. Loss: 0.08655880391597748\n",
      "78 train 61 744\n",
      "Phase: train. Epoch: 78. Loss: 0.06997304409742355\n",
      "78 train 62 751\n",
      "Phase: train. Epoch: 78. Loss: 0.10244956612586975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 val 0 763\n",
      "Phase: val. Epoch: 78. Loss: 0.0799844041466713\n",
      "78 val 1 775\n",
      "Phase: val. Epoch: 78. Loss: 0.06283707916736603\n",
      "78 val 2 787\n",
      "Phase: val. Epoch: 78. Loss: 0.08450113236904144\n",
      "78 val 3 799\n",
      "Phase: val. Epoch: 78. Loss: 0.09630809724330902\n",
      "78 val 4 811\n",
      "Phase: val. Epoch: 78. Loss: 0.10138051956892014\n",
      "78 val 5 823\n",
      "Phase: val. Epoch: 78. Loss: 0.10000471770763397\n",
      "78 val 6 835\n",
      "Phase: val. Epoch: 78. Loss: 0.08213047683238983\n",
      "78 val 7 847\n",
      "Phase: val. Epoch: 78. Loss: 0.11129143834114075\n",
      "78 val 8 859\n",
      "Phase: val. Epoch: 78. Loss: 0.07891564816236496\n",
      "78 val 9 871\n",
      "Phase: val. Epoch: 78. Loss: 0.1013207733631134\n",
      "78 val 10 883\n",
      "Phase: val. Epoch: 78. Loss: 0.05222884938120842\n",
      "78 val 11 884\n",
      "Phase: val. Epoch: 78. Loss: 0.13298305869102478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 train 0 12\n",
      "Phase: train. Epoch: 79. Loss: 0.08833706378936768\n",
      "79 train 1 24\n",
      "Phase: train. Epoch: 79. Loss: 0.08107365667819977\n",
      "79 train 2 36\n",
      "Phase: train. Epoch: 79. Loss: 0.07061617821455002\n",
      "79 train 3 48\n",
      "Phase: train. Epoch: 79. Loss: 0.08541345596313477\n",
      "79 train 4 60\n",
      "Phase: train. Epoch: 79. Loss: 0.057724013924598694\n",
      "79 train 5 72\n",
      "Phase: train. Epoch: 79. Loss: 0.07529963552951813\n",
      "79 train 6 84\n",
      "Phase: train. Epoch: 79. Loss: 0.09688125550746918\n",
      "79 train 7 96\n",
      "Phase: train. Epoch: 79. Loss: 0.06679132580757141\n",
      "79 train 8 108\n",
      "Phase: train. Epoch: 79. Loss: 0.0782306119799614\n",
      "79 train 9 120\n",
      "Phase: train. Epoch: 79. Loss: 0.07723143696784973\n",
      "79 train 10 132\n",
      "Phase: train. Epoch: 79. Loss: 0.06740730255842209\n",
      "79 train 11 144\n",
      "Phase: train. Epoch: 79. Loss: 0.08510172367095947\n",
      "79 train 12 156\n",
      "Phase: train. Epoch: 79. Loss: 0.08113348484039307\n",
      "79 train 13 168\n",
      "Phase: train. Epoch: 79. Loss: 0.09254305064678192\n",
      "79 train 14 180\n",
      "Phase: train. Epoch: 79. Loss: 0.09856830537319183\n",
      "79 train 15 192\n",
      "Phase: train. Epoch: 79. Loss: 0.10171645879745483\n",
      "79 train 16 204\n",
      "Phase: train. Epoch: 79. Loss: 0.08646776527166367\n",
      "79 train 17 216\n",
      "Phase: train. Epoch: 79. Loss: 0.09085631370544434\n",
      "79 train 18 228\n",
      "Phase: train. Epoch: 79. Loss: 0.06754183769226074\n",
      "79 train 19 240\n",
      "Phase: train. Epoch: 79. Loss: 0.08742757886648178\n",
      "79 train 20 252\n",
      "Phase: train. Epoch: 79. Loss: 0.07449396699666977\n",
      "79 train 21 264\n",
      "Phase: train. Epoch: 79. Loss: 0.08094324171543121\n",
      "79 train 22 276\n",
      "Phase: train. Epoch: 79. Loss: 0.10940508544445038\n",
      "79 train 23 288\n",
      "Phase: train. Epoch: 79. Loss: 0.07363756000995636\n",
      "79 train 24 300\n",
      "Phase: train. Epoch: 79. Loss: 0.06125865504145622\n",
      "79 train 25 312\n",
      "Phase: train. Epoch: 79. Loss: 0.06731890887022018\n",
      "79 train 26 324\n",
      "Phase: train. Epoch: 79. Loss: 0.08368232846260071\n",
      "79 train 27 336\n",
      "Phase: train. Epoch: 79. Loss: 0.1068793386220932\n",
      "79 train 28 348\n",
      "Phase: train. Epoch: 79. Loss: 0.09732221066951752\n",
      "79 train 29 360\n",
      "Phase: train. Epoch: 79. Loss: 0.0941820964217186\n",
      "79 train 30 372\n",
      "Phase: train. Epoch: 79. Loss: 0.07884944975376129\n",
      "79 train 31 384\n",
      "Phase: train. Epoch: 79. Loss: 0.06679436564445496\n",
      "79 train 32 396\n",
      "Phase: train. Epoch: 79. Loss: 0.0643603727221489\n",
      "79 train 33 408\n",
      "Phase: train. Epoch: 79. Loss: 0.07758951187133789\n",
      "79 train 34 420\n",
      "Phase: train. Epoch: 79. Loss: 0.06261874735355377\n",
      "79 train 35 432\n",
      "Phase: train. Epoch: 79. Loss: 0.07885952293872833\n",
      "79 train 36 444\n",
      "Phase: train. Epoch: 79. Loss: 0.08132229000329971\n",
      "79 train 37 456\n",
      "Phase: train. Epoch: 79. Loss: 0.10924079269170761\n",
      "79 train 38 468\n",
      "Phase: train. Epoch: 79. Loss: 0.10507708787918091\n",
      "79 train 39 480\n",
      "Phase: train. Epoch: 79. Loss: 0.09537255018949509\n",
      "79 train 40 492\n",
      "Phase: train. Epoch: 79. Loss: 0.1004520058631897\n",
      "79 train 41 504\n",
      "Phase: train. Epoch: 79. Loss: 0.08066128939390182\n",
      "79 train 42 516\n",
      "Phase: train. Epoch: 79. Loss: 0.0774870216846466\n",
      "79 train 43 528\n",
      "Phase: train. Epoch: 79. Loss: 0.08049517869949341\n",
      "79 train 44 540\n",
      "Phase: train. Epoch: 79. Loss: 0.14499785006046295\n",
      "79 train 45 552\n",
      "Phase: train. Epoch: 79. Loss: 0.08277799934148788\n",
      "79 train 46 564\n",
      "Phase: train. Epoch: 79. Loss: 0.08187472820281982\n",
      "79 train 47 576\n",
      "Phase: train. Epoch: 79. Loss: 0.09442746639251709\n",
      "79 train 48 588\n",
      "Phase: train. Epoch: 79. Loss: 0.09573980420827866\n",
      "79 train 49 600\n",
      "Phase: train. Epoch: 79. Loss: 0.07927004992961884\n",
      "79 train 50 612\n",
      "Phase: train. Epoch: 79. Loss: 0.09761720150709152\n",
      "79 train 51 624\n",
      "Phase: train. Epoch: 79. Loss: 0.07692113518714905\n",
      "79 train 52 636\n",
      "Phase: train. Epoch: 79. Loss: 0.06464093178510666\n",
      "79 train 53 648\n",
      "Phase: train. Epoch: 79. Loss: 0.07365978509187698\n",
      "79 train 54 660\n",
      "Phase: train. Epoch: 79. Loss: 0.09311822056770325\n",
      "79 train 55 672\n",
      "Phase: train. Epoch: 79. Loss: 0.0631813257932663\n",
      "79 train 56 684\n",
      "Phase: train. Epoch: 79. Loss: 0.07011084258556366\n",
      "79 train 57 696\n",
      "Phase: train. Epoch: 79. Loss: 0.113003671169281\n",
      "79 train 58 708\n",
      "Phase: train. Epoch: 79. Loss: 0.10325738042593002\n",
      "79 train 59 720\n",
      "Phase: train. Epoch: 79. Loss: 0.07793685793876648\n",
      "79 train 60 732\n",
      "Phase: train. Epoch: 79. Loss: 0.07681354880332947\n",
      "79 train 61 744\n",
      "Phase: train. Epoch: 79. Loss: 0.08018018305301666\n",
      "79 train 62 751\n",
      "Phase: train. Epoch: 79. Loss: 0.08065709471702576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 val 0 763\n",
      "Phase: val. Epoch: 79. Loss: 0.09066524356603622\n",
      "79 val 1 775\n",
      "Phase: val. Epoch: 79. Loss: 0.08382266759872437\n",
      "79 val 2 787\n",
      "Phase: val. Epoch: 79. Loss: 0.09487994015216827\n",
      "79 val 3 799\n",
      "Phase: val. Epoch: 79. Loss: 0.08097417652606964\n",
      "79 val 4 811\n",
      "Phase: val. Epoch: 79. Loss: 0.07281644642353058\n",
      "79 val 5 823\n",
      "Phase: val. Epoch: 79. Loss: 0.04579376429319382\n",
      "79 val 6 835\n",
      "Phase: val. Epoch: 79. Loss: 0.10897841304540634\n",
      "79 val 7 847\n",
      "Phase: val. Epoch: 79. Loss: 0.09228336066007614\n",
      "79 val 8 859\n",
      "Phase: val. Epoch: 79. Loss: 0.0648404061794281\n",
      "79 val 9 871\n",
      "Phase: val. Epoch: 79. Loss: 0.07709958404302597\n",
      "79 val 10 883\n",
      "Phase: val. Epoch: 79. Loss: 0.09231263399124146\n",
      "79 val 11 884\n",
      "Phase: val. Epoch: 79. Loss: 0.08825080096721649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 train 0 12\n",
      "Phase: train. Epoch: 80. Loss: 0.09454908221960068\n",
      "80 train 1 24\n",
      "Phase: train. Epoch: 80. Loss: 0.08322618901729584\n",
      "80 train 2 36\n",
      "Phase: train. Epoch: 80. Loss: 0.07847204059362411\n",
      "80 train 3 48\n",
      "Phase: train. Epoch: 80. Loss: 0.06553264707326889\n",
      "80 train 4 60\n",
      "Phase: train. Epoch: 80. Loss: 0.09036928415298462\n",
      "80 train 5 72\n",
      "Phase: train. Epoch: 80. Loss: 0.13764822483062744\n",
      "80 train 6 84\n",
      "Phase: train. Epoch: 80. Loss: 0.07901754975318909\n",
      "80 train 7 96\n",
      "Phase: train. Epoch: 80. Loss: 0.09955122321844101\n",
      "80 train 8 108\n",
      "Phase: train. Epoch: 80. Loss: 0.08196892589330673\n",
      "80 train 9 120\n",
      "Phase: train. Epoch: 80. Loss: 0.07189187407493591\n",
      "80 train 10 132\n",
      "Phase: train. Epoch: 80. Loss: 0.07369846105575562\n",
      "80 train 11 144\n",
      "Phase: train. Epoch: 80. Loss: 0.08008924126625061\n",
      "80 train 12 156\n",
      "Phase: train. Epoch: 80. Loss: 0.08096768707036972\n",
      "80 train 13 168\n",
      "Phase: train. Epoch: 80. Loss: 0.09308868646621704\n",
      "80 train 14 180\n",
      "Phase: train. Epoch: 80. Loss: 0.06838148832321167\n",
      "80 train 15 192\n",
      "Phase: train. Epoch: 80. Loss: 0.08526043593883514\n",
      "80 train 16 204\n",
      "Phase: train. Epoch: 80. Loss: 0.10502289235591888\n",
      "80 train 17 216\n",
      "Phase: train. Epoch: 80. Loss: 0.10711562633514404\n",
      "80 train 18 228\n",
      "Phase: train. Epoch: 80. Loss: 0.09714289754629135\n",
      "80 train 19 240\n",
      "Phase: train. Epoch: 80. Loss: 0.10295280069112778\n",
      "80 train 20 252\n",
      "Phase: train. Epoch: 80. Loss: 0.099504254758358\n",
      "80 train 21 264\n",
      "Phase: train. Epoch: 80. Loss: 0.07105696201324463\n",
      "80 train 22 276\n",
      "Phase: train. Epoch: 80. Loss: 0.0994185209274292\n",
      "80 train 23 288\n",
      "Phase: train. Epoch: 80. Loss: 0.08552028983831406\n",
      "80 train 24 300\n",
      "Phase: train. Epoch: 80. Loss: 0.06377387046813965\n",
      "80 train 25 312\n",
      "Phase: train. Epoch: 80. Loss: 0.08483362197875977\n",
      "80 train 26 324\n",
      "Phase: train. Epoch: 80. Loss: 0.06469985097646713\n",
      "80 train 27 336\n",
      "Phase: train. Epoch: 80. Loss: 0.07732368260622025\n",
      "80 train 28 348\n",
      "Phase: train. Epoch: 80. Loss: 0.06400861591100693\n",
      "80 train 29 360\n",
      "Phase: train. Epoch: 80. Loss: 0.08528774976730347\n",
      "80 train 30 372\n",
      "Phase: train. Epoch: 80. Loss: 0.09776616096496582\n",
      "80 train 31 384\n",
      "Phase: train. Epoch: 80. Loss: 0.09392009675502777\n",
      "80 train 32 396\n",
      "Phase: train. Epoch: 80. Loss: 0.06603361666202545\n",
      "80 train 33 408\n",
      "Phase: train. Epoch: 80. Loss: 0.07907207310199738\n",
      "80 train 34 420\n",
      "Phase: train. Epoch: 80. Loss: 0.08011176437139511\n",
      "80 train 35 432\n",
      "Phase: train. Epoch: 80. Loss: 0.06558513641357422\n",
      "80 train 36 444\n",
      "Phase: train. Epoch: 80. Loss: 0.0879623144865036\n",
      "80 train 37 456\n",
      "Phase: train. Epoch: 80. Loss: 0.11920245736837387\n",
      "80 train 38 468\n",
      "Phase: train. Epoch: 80. Loss: 0.08812640607357025\n",
      "80 train 39 480\n",
      "Phase: train. Epoch: 80. Loss: 0.08303682506084442\n",
      "80 train 40 492\n",
      "Phase: train. Epoch: 80. Loss: 0.07190966606140137\n",
      "80 train 41 504\n",
      "Phase: train. Epoch: 80. Loss: 0.09282436966896057\n",
      "80 train 42 516\n",
      "Phase: train. Epoch: 80. Loss: 0.08050807565450668\n",
      "80 train 43 528\n",
      "Phase: train. Epoch: 80. Loss: 0.08956753462553024\n",
      "80 train 44 540\n",
      "Phase: train. Epoch: 80. Loss: 0.09179630130529404\n",
      "80 train 45 552\n",
      "Phase: train. Epoch: 80. Loss: 0.09072715044021606\n",
      "80 train 46 564\n",
      "Phase: train. Epoch: 80. Loss: 0.08101179450750351\n",
      "80 train 47 576\n",
      "Phase: train. Epoch: 80. Loss: 0.07016988843679428\n",
      "80 train 48 588\n",
      "Phase: train. Epoch: 80. Loss: 0.0920896828174591\n",
      "80 train 49 600\n",
      "Phase: train. Epoch: 80. Loss: 0.09140895307064056\n",
      "80 train 50 612\n",
      "Phase: train. Epoch: 80. Loss: 0.07089972496032715\n",
      "80 train 51 624\n",
      "Phase: train. Epoch: 80. Loss: 0.10141142457723618\n",
      "80 train 52 636\n",
      "Phase: train. Epoch: 80. Loss: 0.07111384719610214\n",
      "80 train 53 648\n",
      "Phase: train. Epoch: 80. Loss: 0.10803623497486115\n",
      "80 train 54 660\n",
      "Phase: train. Epoch: 80. Loss: 0.07476083934307098\n",
      "80 train 55 672\n",
      "Phase: train. Epoch: 80. Loss: 0.07961440831422806\n",
      "80 train 56 684\n",
      "Phase: train. Epoch: 80. Loss: 0.08157526701688766\n",
      "80 train 57 696\n",
      "Phase: train. Epoch: 80. Loss: 0.08254298567771912\n",
      "80 train 58 708\n",
      "Phase: train. Epoch: 80. Loss: 0.08103250712156296\n",
      "80 train 59 720\n",
      "Phase: train. Epoch: 80. Loss: 0.07203784584999084\n",
      "80 train 60 732\n",
      "Phase: train. Epoch: 80. Loss: 0.07504245638847351\n",
      "80 train 61 744\n",
      "Phase: train. Epoch: 80. Loss: 0.05536261200904846\n",
      "80 train 62 751\n",
      "Phase: train. Epoch: 80. Loss: 0.05568265914916992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 val 0 763\n",
      "Phase: val. Epoch: 80. Loss: 0.1038324236869812\n",
      "80 val 1 775\n",
      "Phase: val. Epoch: 80. Loss: 0.06639566272497177\n",
      "80 val 2 787\n",
      "Phase: val. Epoch: 80. Loss: 0.06436412781476974\n",
      "80 val 3 799\n",
      "Phase: val. Epoch: 80. Loss: 0.0892934501171112\n",
      "80 val 4 811\n",
      "Phase: val. Epoch: 80. Loss: 0.09643852710723877\n",
      "80 val 5 823\n",
      "Phase: val. Epoch: 80. Loss: 0.055801257491111755\n",
      "80 val 6 835\n",
      "Phase: val. Epoch: 80. Loss: 0.06438569724559784\n",
      "80 val 7 847\n",
      "Phase: val. Epoch: 80. Loss: 0.0924617275595665\n",
      "80 val 8 859\n",
      "Phase: val. Epoch: 80. Loss: 0.06780741363763809\n",
      "80 val 9 871\n",
      "Phase: val. Epoch: 80. Loss: 0.0897211879491806\n",
      "80 val 10 883\n",
      "Phase: val. Epoch: 80. Loss: 0.0739622488617897\n",
      "80 val 11 884\n",
      "Phase: val. Epoch: 80. Loss: 0.036966122686862946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 train 0 12\n",
      "Phase: train. Epoch: 81. Loss: 0.06661570817232132\n",
      "81 train 1 24\n",
      "Phase: train. Epoch: 81. Loss: 0.097865991294384\n",
      "81 train 2 36\n",
      "Phase: train. Epoch: 81. Loss: 0.07755666971206665\n",
      "81 train 3 48\n",
      "Phase: train. Epoch: 81. Loss: 0.08519822359085083\n",
      "81 train 4 60\n",
      "Phase: train. Epoch: 81. Loss: 0.12842050194740295\n",
      "81 train 5 72\n",
      "Phase: train. Epoch: 81. Loss: 0.1284661889076233\n",
      "81 train 6 84\n",
      "Phase: train. Epoch: 81. Loss: 0.07861242443323135\n",
      "81 train 7 96\n",
      "Phase: train. Epoch: 81. Loss: 0.06820997595787048\n",
      "81 train 8 108\n",
      "Phase: train. Epoch: 81. Loss: 0.08354952931404114\n",
      "81 train 9 120\n",
      "Phase: train. Epoch: 81. Loss: 0.07577216625213623\n",
      "81 train 10 132\n",
      "Phase: train. Epoch: 81. Loss: 0.08184195309877396\n",
      "81 train 11 144\n",
      "Phase: train. Epoch: 81. Loss: 0.09362946450710297\n",
      "81 train 12 156\n",
      "Phase: train. Epoch: 81. Loss: 0.10014373064041138\n",
      "81 train 13 168\n",
      "Phase: train. Epoch: 81. Loss: 0.07719980925321579\n",
      "81 train 14 180\n",
      "Phase: train. Epoch: 81. Loss: 0.10061230510473251\n",
      "81 train 15 192\n",
      "Phase: train. Epoch: 81. Loss: 0.08232533931732178\n",
      "81 train 16 204\n",
      "Phase: train. Epoch: 81. Loss: 0.08990617841482162\n",
      "81 train 17 216\n",
      "Phase: train. Epoch: 81. Loss: 0.10449196398258209\n",
      "81 train 18 228\n",
      "Phase: train. Epoch: 81. Loss: 0.07655012607574463\n",
      "81 train 19 240\n",
      "Phase: train. Epoch: 81. Loss: 0.08832082152366638\n",
      "81 train 20 252\n",
      "Phase: train. Epoch: 81. Loss: 0.07302066683769226\n",
      "81 train 21 264\n",
      "Phase: train. Epoch: 81. Loss: 0.0750555545091629\n",
      "81 train 22 276\n",
      "Phase: train. Epoch: 81. Loss: 0.06841005384922028\n",
      "81 train 23 288\n",
      "Phase: train. Epoch: 81. Loss: 0.08630292862653732\n",
      "81 train 24 300\n",
      "Phase: train. Epoch: 81. Loss: 0.08309708535671234\n",
      "81 train 25 312\n",
      "Phase: train. Epoch: 81. Loss: 0.08186478912830353\n",
      "81 train 26 324\n",
      "Phase: train. Epoch: 81. Loss: 0.08250385522842407\n",
      "81 train 27 336\n",
      "Phase: train. Epoch: 81. Loss: 0.0818258672952652\n",
      "81 train 28 348\n",
      "Phase: train. Epoch: 81. Loss: 0.08099524676799774\n",
      "81 train 29 360\n",
      "Phase: train. Epoch: 81. Loss: 0.07292132079601288\n",
      "81 train 30 372\n",
      "Phase: train. Epoch: 81. Loss: 0.06310947984457016\n",
      "81 train 31 384\n",
      "Phase: train. Epoch: 81. Loss: 0.06247059628367424\n",
      "81 train 32 396\n",
      "Phase: train. Epoch: 81. Loss: 0.07134564220905304\n",
      "81 train 33 408\n",
      "Phase: train. Epoch: 81. Loss: 0.08991864323616028\n",
      "81 train 34 420\n",
      "Phase: train. Epoch: 81. Loss: 0.10232996195554733\n",
      "81 train 35 432\n",
      "Phase: train. Epoch: 81. Loss: 0.0991022139787674\n",
      "81 train 36 444\n",
      "Phase: train. Epoch: 81. Loss: 0.11134874075651169\n",
      "81 train 37 456\n",
      "Phase: train. Epoch: 81. Loss: 0.10618617385625839\n",
      "81 train 38 468\n",
      "Phase: train. Epoch: 81. Loss: 0.08490481972694397\n",
      "81 train 39 480\n",
      "Phase: train. Epoch: 81. Loss: 0.09729562699794769\n",
      "81 train 40 492\n",
      "Phase: train. Epoch: 81. Loss: 0.07516415417194366\n",
      "81 train 41 504\n",
      "Phase: train. Epoch: 81. Loss: 0.08815281093120575\n",
      "81 train 42 516\n",
      "Phase: train. Epoch: 81. Loss: 0.057849954813718796\n",
      "81 train 43 528\n",
      "Phase: train. Epoch: 81. Loss: 0.08529394865036011\n",
      "81 train 44 540\n",
      "Phase: train. Epoch: 81. Loss: 0.08400522172451019\n",
      "81 train 45 552\n",
      "Phase: train. Epoch: 81. Loss: 0.09868848323822021\n",
      "81 train 46 564\n",
      "Phase: train. Epoch: 81. Loss: 0.08631329238414764\n",
      "81 train 47 576\n",
      "Phase: train. Epoch: 81. Loss: 0.07042959332466125\n",
      "81 train 48 588\n",
      "Phase: train. Epoch: 81. Loss: 0.09363319724798203\n",
      "81 train 49 600\n",
      "Phase: train. Epoch: 81. Loss: 0.06918742507696152\n",
      "81 train 50 612\n",
      "Phase: train. Epoch: 81. Loss: 0.08575475215911865\n",
      "81 train 51 624\n",
      "Phase: train. Epoch: 81. Loss: 0.07368189841508865\n",
      "81 train 52 636\n",
      "Phase: train. Epoch: 81. Loss: 0.083576500415802\n",
      "81 train 53 648\n",
      "Phase: train. Epoch: 81. Loss: 0.07776837050914764\n",
      "81 train 54 660\n",
      "Phase: train. Epoch: 81. Loss: 0.07866506278514862\n",
      "81 train 55 672\n",
      "Phase: train. Epoch: 81. Loss: 0.0642668604850769\n",
      "81 train 56 684\n",
      "Phase: train. Epoch: 81. Loss: 0.06391572952270508\n",
      "81 train 57 696\n",
      "Phase: train. Epoch: 81. Loss: 0.07133409380912781\n",
      "81 train 58 708\n",
      "Phase: train. Epoch: 81. Loss: 0.0926612839102745\n",
      "81 train 59 720\n",
      "Phase: train. Epoch: 81. Loss: 0.07192082703113556\n",
      "81 train 60 732\n",
      "Phase: train. Epoch: 81. Loss: 0.08249315619468689\n",
      "81 train 61 744\n",
      "Phase: train. Epoch: 81. Loss: 0.06264515221118927\n",
      "81 train 62 751\n",
      "Phase: train. Epoch: 81. Loss: 0.09247388690710068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 val 0 763\n",
      "Phase: val. Epoch: 81. Loss: 0.05041150376200676\n",
      "81 val 1 775\n",
      "Phase: val. Epoch: 81. Loss: 0.07577455788850784\n",
      "81 val 2 787\n",
      "Phase: val. Epoch: 81. Loss: 0.08175064623355865\n",
      "81 val 3 799\n",
      "Phase: val. Epoch: 81. Loss: 0.06185988709330559\n",
      "81 val 4 811\n",
      "Phase: val. Epoch: 81. Loss: 0.09923574328422546\n",
      "81 val 5 823\n",
      "Phase: val. Epoch: 81. Loss: 0.07183907926082611\n",
      "81 val 6 835\n",
      "Phase: val. Epoch: 81. Loss: 0.11468221992254257\n",
      "81 val 7 847\n",
      "Phase: val. Epoch: 81. Loss: 0.10104507207870483\n",
      "81 val 8 859\n",
      "Phase: val. Epoch: 81. Loss: 0.10398082435131073\n",
      "81 val 9 871\n",
      "Phase: val. Epoch: 81. Loss: 0.056943900883197784\n",
      "81 val 10 883\n",
      "Phase: val. Epoch: 81. Loss: 0.07917514443397522\n",
      "81 val 11 884\n",
      "Phase: val. Epoch: 81. Loss: 0.03353162109851837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 train 0 12\n",
      "Phase: train. Epoch: 82. Loss: 0.10867127031087875\n",
      "82 train 1 24\n",
      "Phase: train. Epoch: 82. Loss: 0.10763110965490341\n",
      "82 train 2 36\n",
      "Phase: train. Epoch: 82. Loss: 0.1012033224105835\n",
      "82 train 3 48\n",
      "Phase: train. Epoch: 82. Loss: 0.08112924546003342\n",
      "82 train 4 60\n",
      "Phase: train. Epoch: 82. Loss: 0.07813765853643417\n",
      "82 train 5 72\n",
      "Phase: train. Epoch: 82. Loss: 0.0840529054403305\n",
      "82 train 6 84\n",
      "Phase: train. Epoch: 82. Loss: 0.07303829491138458\n",
      "82 train 7 96\n",
      "Phase: train. Epoch: 82. Loss: 0.06732162833213806\n",
      "82 train 8 108\n",
      "Phase: train. Epoch: 82. Loss: 0.08778052777051926\n",
      "82 train 9 120\n",
      "Phase: train. Epoch: 82. Loss: 0.07588024437427521\n",
      "82 train 10 132\n",
      "Phase: train. Epoch: 82. Loss: 0.09974490851163864\n",
      "82 train 11 144\n",
      "Phase: train. Epoch: 82. Loss: 0.0918409451842308\n",
      "82 train 12 156\n",
      "Phase: train. Epoch: 82. Loss: 0.07365395128726959\n",
      "82 train 13 168\n",
      "Phase: train. Epoch: 82. Loss: 0.08504576981067657\n",
      "82 train 14 180\n",
      "Phase: train. Epoch: 82. Loss: 0.09876684844493866\n",
      "82 train 15 192\n",
      "Phase: train. Epoch: 82. Loss: 0.08633065223693848\n",
      "82 train 16 204\n",
      "Phase: train. Epoch: 82. Loss: 0.08737670630216599\n",
      "82 train 17 216\n",
      "Phase: train. Epoch: 82. Loss: 0.08462685346603394\n",
      "82 train 18 228\n",
      "Phase: train. Epoch: 82. Loss: 0.07913129031658173\n",
      "82 train 19 240\n",
      "Phase: train. Epoch: 82. Loss: 0.09493698924779892\n",
      "82 train 20 252\n",
      "Phase: train. Epoch: 82. Loss: 0.0678606629371643\n",
      "82 train 21 264\n",
      "Phase: train. Epoch: 82. Loss: 0.09689196944236755\n",
      "82 train 22 276\n",
      "Phase: train. Epoch: 82. Loss: 0.06521474570035934\n",
      "82 train 23 288\n",
      "Phase: train. Epoch: 82. Loss: 0.09258654713630676\n",
      "82 train 24 300\n",
      "Phase: train. Epoch: 82. Loss: 0.08237321674823761\n",
      "82 train 25 312\n",
      "Phase: train. Epoch: 82. Loss: 0.09935488551855087\n",
      "82 train 26 324\n",
      "Phase: train. Epoch: 82. Loss: 0.0651555135846138\n",
      "82 train 27 336\n",
      "Phase: train. Epoch: 82. Loss: 0.0876874327659607\n",
      "82 train 28 348\n",
      "Phase: train. Epoch: 82. Loss: 0.07150330394506454\n",
      "82 train 29 360\n",
      "Phase: train. Epoch: 82. Loss: 0.07593705505132675\n",
      "82 train 30 372\n",
      "Phase: train. Epoch: 82. Loss: 0.09457610547542572\n",
      "82 train 31 384\n",
      "Phase: train. Epoch: 82. Loss: 0.09445340931415558\n",
      "82 train 32 396\n",
      "Phase: train. Epoch: 82. Loss: 0.07964339852333069\n",
      "82 train 33 408\n",
      "Phase: train. Epoch: 82. Loss: 0.08807814121246338\n",
      "82 train 34 420\n",
      "Phase: train. Epoch: 82. Loss: 0.09067283570766449\n",
      "82 train 35 432\n",
      "Phase: train. Epoch: 82. Loss: 0.06212111562490463\n",
      "82 train 36 444\n",
      "Phase: train. Epoch: 82. Loss: 0.07942120730876923\n",
      "82 train 37 456\n",
      "Phase: train. Epoch: 82. Loss: 0.10704123973846436\n",
      "82 train 38 468\n",
      "Phase: train. Epoch: 82. Loss: 0.06576386839151382\n",
      "82 train 39 480\n",
      "Phase: train. Epoch: 82. Loss: 0.08693382143974304\n",
      "82 train 40 492\n",
      "Phase: train. Epoch: 82. Loss: 0.07870606333017349\n",
      "82 train 41 504\n",
      "Phase: train. Epoch: 82. Loss: 0.07060094177722931\n",
      "82 train 42 516\n",
      "Phase: train. Epoch: 82. Loss: 0.10412377119064331\n",
      "82 train 43 528\n",
      "Phase: train. Epoch: 82. Loss: 0.084847591817379\n",
      "82 train 44 540\n",
      "Phase: train. Epoch: 82. Loss: 0.0590222105383873\n",
      "82 train 45 552\n",
      "Phase: train. Epoch: 82. Loss: 0.06950674951076508\n",
      "82 train 46 564\n",
      "Phase: train. Epoch: 82. Loss: 0.06475183367729187\n",
      "82 train 47 576\n",
      "Phase: train. Epoch: 82. Loss: 0.11040859669446945\n",
      "82 train 48 588\n",
      "Phase: train. Epoch: 82. Loss: 0.07367576658725739\n",
      "82 train 49 600\n",
      "Phase: train. Epoch: 82. Loss: 0.07917087525129318\n",
      "82 train 50 612\n",
      "Phase: train. Epoch: 82. Loss: 0.08145876228809357\n",
      "82 train 51 624\n",
      "Phase: train. Epoch: 82. Loss: 0.09842853248119354\n",
      "82 train 52 636\n",
      "Phase: train. Epoch: 82. Loss: 0.07964218407869339\n",
      "82 train 53 648\n",
      "Phase: train. Epoch: 82. Loss: 0.08424835652112961\n",
      "82 train 54 660\n",
      "Phase: train. Epoch: 82. Loss: 0.0961386114358902\n",
      "82 train 55 672\n",
      "Phase: train. Epoch: 82. Loss: 0.0668758749961853\n",
      "82 train 56 684\n",
      "Phase: train. Epoch: 82. Loss: 0.08334460109472275\n",
      "82 train 57 696\n",
      "Phase: train. Epoch: 82. Loss: 0.08295837044715881\n",
      "82 train 58 708\n",
      "Phase: train. Epoch: 82. Loss: 0.07844462245702744\n",
      "82 train 59 720\n",
      "Phase: train. Epoch: 82. Loss: 0.06880128383636475\n",
      "82 train 60 732\n",
      "Phase: train. Epoch: 82. Loss: 0.07311524450778961\n",
      "82 train 61 744\n",
      "Phase: train. Epoch: 82. Loss: 0.0687587633728981\n",
      "82 train 62 751\n",
      "Phase: train. Epoch: 82. Loss: 0.0692189559340477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 val 0 763\n",
      "Phase: val. Epoch: 82. Loss: 0.07252377271652222\n",
      "82 val 1 775\n",
      "Phase: val. Epoch: 82. Loss: 0.07475762069225311\n",
      "82 val 2 787\n",
      "Phase: val. Epoch: 82. Loss: 0.07686887681484222\n",
      "82 val 3 799\n",
      "Phase: val. Epoch: 82. Loss: 0.08296013623476028\n",
      "82 val 4 811\n",
      "Phase: val. Epoch: 82. Loss: 0.08693437278270721\n",
      "82 val 5 823\n",
      "Phase: val. Epoch: 82. Loss: 0.0736011415719986\n",
      "82 val 6 835\n",
      "Phase: val. Epoch: 82. Loss: 0.10086750984191895\n",
      "82 val 7 847\n",
      "Phase: val. Epoch: 82. Loss: 0.10013987123966217\n",
      "82 val 8 859\n",
      "Phase: val. Epoch: 82. Loss: 0.07674156874418259\n",
      "82 val 9 871\n",
      "Phase: val. Epoch: 82. Loss: 0.07083623856306076\n",
      "82 val 10 883\n",
      "Phase: val. Epoch: 82. Loss: 0.08287796378135681\n",
      "82 val 11 884\n",
      "Phase: val. Epoch: 82. Loss: 0.026573901996016502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 train 0 12\n",
      "Phase: train. Epoch: 83. Loss: 0.07984138280153275\n",
      "83 train 1 24\n",
      "Phase: train. Epoch: 83. Loss: 0.07606083154678345\n",
      "83 train 2 36\n",
      "Phase: train. Epoch: 83. Loss: 0.07682435214519501\n",
      "83 train 3 48\n",
      "Phase: train. Epoch: 83. Loss: 0.0755906030535698\n",
      "83 train 4 60\n",
      "Phase: train. Epoch: 83. Loss: 0.07633879780769348\n",
      "83 train 5 72\n",
      "Phase: train. Epoch: 83. Loss: 0.11922498792409897\n",
      "83 train 6 84\n",
      "Phase: train. Epoch: 83. Loss: 0.11312288045883179\n",
      "83 train 7 96\n",
      "Phase: train. Epoch: 83. Loss: 0.08628340810537338\n",
      "83 train 8 108\n",
      "Phase: train. Epoch: 83. Loss: 0.07616492360830307\n",
      "83 train 9 120\n",
      "Phase: train. Epoch: 83. Loss: 0.06143128499388695\n",
      "83 train 10 132\n",
      "Phase: train. Epoch: 83. Loss: 0.13534002006053925\n",
      "83 train 11 144\n",
      "Phase: train. Epoch: 83. Loss: 0.06802543252706528\n",
      "83 train 12 156\n",
      "Phase: train. Epoch: 83. Loss: 0.07008330523967743\n",
      "83 train 13 168\n",
      "Phase: train. Epoch: 83. Loss: 0.07416827231645584\n",
      "83 train 14 180\n",
      "Phase: train. Epoch: 83. Loss: 0.08747761696577072\n",
      "83 train 15 192\n",
      "Phase: train. Epoch: 83. Loss: 0.07015927135944366\n",
      "83 train 16 204\n",
      "Phase: train. Epoch: 83. Loss: 0.08112893998622894\n",
      "83 train 17 216\n",
      "Phase: train. Epoch: 83. Loss: 0.062297336757183075\n",
      "83 train 18 228\n",
      "Phase: train. Epoch: 83. Loss: 0.09167474508285522\n",
      "83 train 19 240\n",
      "Phase: train. Epoch: 83. Loss: 0.10008296370506287\n",
      "83 train 20 252\n",
      "Phase: train. Epoch: 83. Loss: 0.07287763059139252\n",
      "83 train 21 264\n",
      "Phase: train. Epoch: 83. Loss: 0.07611516863107681\n",
      "83 train 22 276\n",
      "Phase: train. Epoch: 83. Loss: 0.10176700353622437\n",
      "83 train 23 288\n",
      "Phase: train. Epoch: 83. Loss: 0.07364806532859802\n",
      "83 train 24 300\n",
      "Phase: train. Epoch: 83. Loss: 0.06723175942897797\n",
      "83 train 25 312\n",
      "Phase: train. Epoch: 83. Loss: 0.10040359944105148\n",
      "83 train 26 324\n",
      "Phase: train. Epoch: 83. Loss: 0.08917918056249619\n",
      "83 train 27 336\n",
      "Phase: train. Epoch: 83. Loss: 0.07322707027196884\n",
      "83 train 28 348\n",
      "Phase: train. Epoch: 83. Loss: 0.09503313899040222\n",
      "83 train 29 360\n",
      "Phase: train. Epoch: 83. Loss: 0.10859163850545883\n",
      "83 train 30 372\n",
      "Phase: train. Epoch: 83. Loss: 0.0808175653219223\n",
      "83 train 31 384\n",
      "Phase: train. Epoch: 83. Loss: 0.06555414199829102\n",
      "83 train 32 396\n",
      "Phase: train. Epoch: 83. Loss: 0.11540019512176514\n",
      "83 train 33 408\n",
      "Phase: train. Epoch: 83. Loss: 0.13099610805511475\n",
      "83 train 34 420\n",
      "Phase: train. Epoch: 83. Loss: 0.07636861503124237\n",
      "83 train 35 432\n",
      "Phase: train. Epoch: 83. Loss: 0.08697005361318588\n",
      "83 train 36 444\n",
      "Phase: train. Epoch: 83. Loss: 0.07942352443933487\n",
      "83 train 37 456\n",
      "Phase: train. Epoch: 83. Loss: 0.07401339709758759\n",
      "83 train 38 468\n",
      "Phase: train. Epoch: 83. Loss: 0.1062418520450592\n",
      "83 train 39 480\n",
      "Phase: train. Epoch: 83. Loss: 0.08183890581130981\n",
      "83 train 40 492\n",
      "Phase: train. Epoch: 83. Loss: 0.09107668697834015\n",
      "83 train 41 504\n",
      "Phase: train. Epoch: 83. Loss: 0.07882276177406311\n",
      "83 train 42 516\n",
      "Phase: train. Epoch: 83. Loss: 0.09385356307029724\n",
      "83 train 43 528\n",
      "Phase: train. Epoch: 83. Loss: 0.06186772510409355\n",
      "83 train 44 540\n",
      "Phase: train. Epoch: 83. Loss: 0.08678267896175385\n",
      "83 train 45 552\n",
      "Phase: train. Epoch: 83. Loss: 0.0818951427936554\n",
      "83 train 46 564\n",
      "Phase: train. Epoch: 83. Loss: 0.06933248043060303\n",
      "83 train 47 576\n",
      "Phase: train. Epoch: 83. Loss: 0.07715393602848053\n",
      "83 train 48 588\n",
      "Phase: train. Epoch: 83. Loss: 0.06807467341423035\n",
      "83 train 49 600\n",
      "Phase: train. Epoch: 83. Loss: 0.08988630771636963\n",
      "83 train 50 612\n",
      "Phase: train. Epoch: 83. Loss: 0.09729407727718353\n",
      "83 train 51 624\n",
      "Phase: train. Epoch: 83. Loss: 0.07405750453472137\n",
      "83 train 52 636\n",
      "Phase: train. Epoch: 83. Loss: 0.09254994243383408\n",
      "83 train 53 648\n",
      "Phase: train. Epoch: 83. Loss: 0.06406490504741669\n",
      "83 train 54 660\n",
      "Phase: train. Epoch: 83. Loss: 0.06410757452249527\n",
      "83 train 55 672\n",
      "Phase: train. Epoch: 83. Loss: 0.08568897843360901\n",
      "83 train 56 684\n",
      "Phase: train. Epoch: 83. Loss: 0.09897683560848236\n",
      "83 train 57 696\n",
      "Phase: train. Epoch: 83. Loss: 0.08265748620033264\n",
      "83 train 58 708\n",
      "Phase: train. Epoch: 83. Loss: 0.11360960453748703\n",
      "83 train 59 720\n",
      "Phase: train. Epoch: 83. Loss: 0.08091351389884949\n",
      "83 train 60 732\n",
      "Phase: train. Epoch: 83. Loss: 0.09546836465597153\n",
      "83 train 61 744\n",
      "Phase: train. Epoch: 83. Loss: 0.08274241536855698\n",
      "83 train 62 751\n",
      "Phase: train. Epoch: 83. Loss: 0.05802417919039726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 val 0 763\n",
      "Phase: val. Epoch: 83. Loss: 0.07058894634246826\n",
      "83 val 1 775\n",
      "Phase: val. Epoch: 83. Loss: 0.08550289273262024\n",
      "83 val 2 787\n",
      "Phase: val. Epoch: 83. Loss: 0.07426947355270386\n",
      "83 val 3 799\n",
      "Phase: val. Epoch: 83. Loss: 0.08188217133283615\n",
      "83 val 4 811\n",
      "Phase: val. Epoch: 83. Loss: 0.06315599381923676\n",
      "83 val 5 823\n",
      "Phase: val. Epoch: 83. Loss: 0.08818523585796356\n",
      "83 val 6 835\n",
      "Phase: val. Epoch: 83. Loss: 0.09902617335319519\n",
      "83 val 7 847\n",
      "Phase: val. Epoch: 83. Loss: 0.08757653832435608\n",
      "83 val 8 859\n",
      "Phase: val. Epoch: 83. Loss: 0.07423673570156097\n",
      "83 val 9 871\n",
      "Phase: val. Epoch: 83. Loss: 0.06892053782939911\n",
      "83 val 10 883\n",
      "Phase: val. Epoch: 83. Loss: 0.07800984382629395\n",
      "83 val 11 884\n",
      "Phase: val. Epoch: 83. Loss: 0.0262282844632864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 train 0 12\n",
      "Phase: train. Epoch: 84. Loss: 0.11369543522596359\n",
      "84 train 1 24\n",
      "Phase: train. Epoch: 84. Loss: 0.06356971710920334\n",
      "84 train 2 36\n",
      "Phase: train. Epoch: 84. Loss: 0.09990416467189789\n",
      "84 train 3 48\n",
      "Phase: train. Epoch: 84. Loss: 0.06662799417972565\n",
      "84 train 4 60\n",
      "Phase: train. Epoch: 84. Loss: 0.10789286345243454\n",
      "84 train 5 72\n",
      "Phase: train. Epoch: 84. Loss: 0.07812763750553131\n",
      "84 train 6 84\n",
      "Phase: train. Epoch: 84. Loss: 0.08833883702754974\n",
      "84 train 7 96\n",
      "Phase: train. Epoch: 84. Loss: 0.0877036303281784\n",
      "84 train 8 108\n",
      "Phase: train. Epoch: 84. Loss: 0.07799562066793442\n",
      "84 train 9 120\n",
      "Phase: train. Epoch: 84. Loss: 0.07520665228366852\n",
      "84 train 10 132\n",
      "Phase: train. Epoch: 84. Loss: 0.0944092720746994\n",
      "84 train 11 144\n",
      "Phase: train. Epoch: 84. Loss: 0.10065280646085739\n",
      "84 train 12 156\n",
      "Phase: train. Epoch: 84. Loss: 0.07212170958518982\n",
      "84 train 13 168\n",
      "Phase: train. Epoch: 84. Loss: 0.06667295098304749\n",
      "84 train 14 180\n",
      "Phase: train. Epoch: 84. Loss: 0.06403689831495285\n",
      "84 train 15 192\n",
      "Phase: train. Epoch: 84. Loss: 0.06340362876653671\n",
      "84 train 16 204\n",
      "Phase: train. Epoch: 84. Loss: 0.07892270386219025\n",
      "84 train 17 216\n",
      "Phase: train. Epoch: 84. Loss: 0.06154199689626694\n",
      "84 train 18 228\n",
      "Phase: train. Epoch: 84. Loss: 0.06858935207128525\n",
      "84 train 19 240\n",
      "Phase: train. Epoch: 84. Loss: 0.09124934673309326\n",
      "84 train 20 252\n",
      "Phase: train. Epoch: 84. Loss: 0.07782752811908722\n",
      "84 train 21 264\n",
      "Phase: train. Epoch: 84. Loss: 0.11484180390834808\n",
      "84 train 22 276\n",
      "Phase: train. Epoch: 84. Loss: 0.09550954401493073\n",
      "84 train 23 288\n",
      "Phase: train. Epoch: 84. Loss: 0.08242198824882507\n",
      "84 train 24 300\n",
      "Phase: train. Epoch: 84. Loss: 0.10019432008266449\n",
      "84 train 25 312\n",
      "Phase: train. Epoch: 84. Loss: 0.09073122590780258\n",
      "84 train 26 324\n",
      "Phase: train. Epoch: 84. Loss: 0.06732168048620224\n",
      "84 train 27 336\n",
      "Phase: train. Epoch: 84. Loss: 0.07029526680707932\n",
      "84 train 28 348\n",
      "Phase: train. Epoch: 84. Loss: 0.07619617879390717\n",
      "84 train 29 360\n",
      "Phase: train. Epoch: 84. Loss: 0.09557324647903442\n",
      "84 train 30 372\n",
      "Phase: train. Epoch: 84. Loss: 0.09211905300617218\n",
      "84 train 31 384\n",
      "Phase: train. Epoch: 84. Loss: 0.09123289585113525\n",
      "84 train 32 396\n",
      "Phase: train. Epoch: 84. Loss: 0.06594105064868927\n",
      "84 train 33 408\n",
      "Phase: train. Epoch: 84. Loss: 0.08904578536748886\n",
      "84 train 34 420\n",
      "Phase: train. Epoch: 84. Loss: 0.0830405205488205\n",
      "84 train 35 432\n",
      "Phase: train. Epoch: 84. Loss: 0.10693243145942688\n",
      "84 train 36 444\n",
      "Phase: train. Epoch: 84. Loss: 0.07855438441038132\n",
      "84 train 37 456\n",
      "Phase: train. Epoch: 84. Loss: 0.08395694941282272\n",
      "84 train 38 468\n",
      "Phase: train. Epoch: 84. Loss: 0.06118007004261017\n",
      "84 train 39 480\n",
      "Phase: train. Epoch: 84. Loss: 0.07294174283742905\n",
      "84 train 40 492\n",
      "Phase: train. Epoch: 84. Loss: 0.09849832952022552\n",
      "84 train 41 504\n",
      "Phase: train. Epoch: 84. Loss: 0.07534807175397873\n",
      "84 train 42 516\n",
      "Phase: train. Epoch: 84. Loss: 0.05252201482653618\n",
      "84 train 43 528\n",
      "Phase: train. Epoch: 84. Loss: 0.07270593196153641\n",
      "84 train 44 540\n",
      "Phase: train. Epoch: 84. Loss: 0.0769648626446724\n",
      "84 train 45 552\n",
      "Phase: train. Epoch: 84. Loss: 0.08477817475795746\n",
      "84 train 46 564\n",
      "Phase: train. Epoch: 84. Loss: 0.06320402026176453\n",
      "84 train 47 576\n",
      "Phase: train. Epoch: 84. Loss: 0.07884864509105682\n",
      "84 train 48 588\n",
      "Phase: train. Epoch: 84. Loss: 0.09951367229223251\n",
      "84 train 49 600\n",
      "Phase: train. Epoch: 84. Loss: 0.06715089827775955\n",
      "84 train 50 612\n",
      "Phase: train. Epoch: 84. Loss: 0.09587721526622772\n",
      "84 train 51 624\n",
      "Phase: train. Epoch: 84. Loss: 0.08662866055965424\n",
      "84 train 52 636\n",
      "Phase: train. Epoch: 84. Loss: 0.08785389363765717\n",
      "84 train 53 648\n",
      "Phase: train. Epoch: 84. Loss: 0.06979963183403015\n",
      "84 train 54 660\n",
      "Phase: train. Epoch: 84. Loss: 0.11114171147346497\n",
      "84 train 55 672\n",
      "Phase: train. Epoch: 84. Loss: 0.0929204672574997\n",
      "84 train 56 684\n",
      "Phase: train. Epoch: 84. Loss: 0.12357304990291595\n",
      "84 train 57 696\n",
      "Phase: train. Epoch: 84. Loss: 0.07082685828208923\n",
      "84 train 58 708\n",
      "Phase: train. Epoch: 84. Loss: 0.10819736123085022\n",
      "84 train 59 720\n",
      "Phase: train. Epoch: 84. Loss: 0.08552958816289902\n",
      "84 train 60 732\n",
      "Phase: train. Epoch: 84. Loss: 0.07304202020168304\n",
      "84 train 61 744\n",
      "Phase: train. Epoch: 84. Loss: 0.07852411270141602\n",
      "84 train 62 751\n",
      "Phase: train. Epoch: 84. Loss: 0.07088787853717804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 val 0 763\n",
      "Phase: val. Epoch: 84. Loss: 0.06924378126859665\n",
      "84 val 1 775\n",
      "Phase: val. Epoch: 84. Loss: 0.07642671465873718\n",
      "84 val 2 787\n",
      "Phase: val. Epoch: 84. Loss: 0.08739620447158813\n",
      "84 val 3 799\n",
      "Phase: val. Epoch: 84. Loss: 0.0744594931602478\n",
      "84 val 4 811\n",
      "Phase: val. Epoch: 84. Loss: 0.1012822613120079\n",
      "84 val 5 823\n",
      "Phase: val. Epoch: 84. Loss: 0.0749274492263794\n",
      "84 val 6 835\n",
      "Phase: val. Epoch: 84. Loss: 0.07337911427021027\n",
      "84 val 7 847\n",
      "Phase: val. Epoch: 84. Loss: 0.08251632750034332\n",
      "84 val 8 859\n",
      "Phase: val. Epoch: 84. Loss: 0.0723908469080925\n",
      "84 val 9 871\n",
      "Phase: val. Epoch: 84. Loss: 0.06718272715806961\n",
      "84 val 10 883\n",
      "Phase: val. Epoch: 84. Loss: 0.07466447353363037\n",
      "84 val 11 884\n",
      "Phase: val. Epoch: 84. Loss: 0.11887386441230774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 train 0 12\n",
      "Phase: train. Epoch: 85. Loss: 0.08398868888616562\n",
      "85 train 1 24\n",
      "Phase: train. Epoch: 85. Loss: 0.07533326745033264\n",
      "85 train 2 36\n",
      "Phase: train. Epoch: 85. Loss: 0.09880455583333969\n",
      "85 train 3 48\n",
      "Phase: train. Epoch: 85. Loss: 0.07943707704544067\n",
      "85 train 4 60\n",
      "Phase: train. Epoch: 85. Loss: 0.0739995688199997\n",
      "85 train 5 72\n",
      "Phase: train. Epoch: 85. Loss: 0.09679484367370605\n",
      "85 train 6 84\n",
      "Phase: train. Epoch: 85. Loss: 0.14210769534111023\n",
      "85 train 7 96\n",
      "Phase: train. Epoch: 85. Loss: 0.1256236881017685\n",
      "85 train 8 108\n",
      "Phase: train. Epoch: 85. Loss: 0.06183789297938347\n",
      "85 train 9 120\n",
      "Phase: train. Epoch: 85. Loss: 0.06585624814033508\n",
      "85 train 10 132\n",
      "Phase: train. Epoch: 85. Loss: 0.10199719667434692\n",
      "85 train 11 144\n",
      "Phase: train. Epoch: 85. Loss: 0.09166812896728516\n",
      "85 train 12 156\n",
      "Phase: train. Epoch: 85. Loss: 0.10385274142026901\n",
      "85 train 13 168\n",
      "Phase: train. Epoch: 85. Loss: 0.10840658843517303\n",
      "85 train 14 180\n",
      "Phase: train. Epoch: 85. Loss: 0.07835349440574646\n",
      "85 train 15 192\n",
      "Phase: train. Epoch: 85. Loss: 0.07033233344554901\n",
      "85 train 16 204\n",
      "Phase: train. Epoch: 85. Loss: 0.07454732060432434\n",
      "85 train 17 216\n",
      "Phase: train. Epoch: 85. Loss: 0.07058481872081757\n",
      "85 train 18 228\n",
      "Phase: train. Epoch: 85. Loss: 0.1229020208120346\n",
      "85 train 19 240\n",
      "Phase: train. Epoch: 85. Loss: 0.07157278060913086\n",
      "85 train 20 252\n",
      "Phase: train. Epoch: 85. Loss: 0.06642022728919983\n",
      "85 train 21 264\n",
      "Phase: train. Epoch: 85. Loss: 0.09726440906524658\n",
      "85 train 22 276\n",
      "Phase: train. Epoch: 85. Loss: 0.0814158171415329\n",
      "85 train 23 288\n",
      "Phase: train. Epoch: 85. Loss: 0.08128675073385239\n",
      "85 train 24 300\n",
      "Phase: train. Epoch: 85. Loss: 0.06845878809690475\n",
      "85 train 25 312\n",
      "Phase: train. Epoch: 85. Loss: 0.07166986167430878\n",
      "85 train 26 324\n",
      "Phase: train. Epoch: 85. Loss: 0.07236538082361221\n",
      "85 train 27 336\n",
      "Phase: train. Epoch: 85. Loss: 0.09074138104915619\n",
      "85 train 28 348\n",
      "Phase: train. Epoch: 85. Loss: 0.07358982414007187\n",
      "85 train 29 360\n",
      "Phase: train. Epoch: 85. Loss: 0.08845004439353943\n",
      "85 train 30 372\n",
      "Phase: train. Epoch: 85. Loss: 0.06781169027090073\n",
      "85 train 31 384\n",
      "Phase: train. Epoch: 85. Loss: 0.09608066082000732\n",
      "85 train 32 396\n",
      "Phase: train. Epoch: 85. Loss: 0.08083198964595795\n",
      "85 train 33 408\n",
      "Phase: train. Epoch: 85. Loss: 0.08112166821956635\n",
      "85 train 34 420\n",
      "Phase: train. Epoch: 85. Loss: 0.089807890355587\n",
      "85 train 35 432\n",
      "Phase: train. Epoch: 85. Loss: 0.09778144955635071\n",
      "85 train 36 444\n",
      "Phase: train. Epoch: 85. Loss: 0.05668795108795166\n",
      "85 train 37 456\n",
      "Phase: train. Epoch: 85. Loss: 0.0842677652835846\n",
      "85 train 38 468\n",
      "Phase: train. Epoch: 85. Loss: 0.08734907954931259\n",
      "85 train 39 480\n",
      "Phase: train. Epoch: 85. Loss: 0.08246362209320068\n",
      "85 train 40 492\n",
      "Phase: train. Epoch: 85. Loss: 0.09771513193845749\n",
      "85 train 41 504\n",
      "Phase: train. Epoch: 85. Loss: 0.08633502572774887\n",
      "85 train 42 516\n",
      "Phase: train. Epoch: 85. Loss: 0.06587835401296616\n",
      "85 train 43 528\n",
      "Phase: train. Epoch: 85. Loss: 0.064084991812706\n",
      "85 train 44 540\n",
      "Phase: train. Epoch: 85. Loss: 0.06732489168643951\n",
      "85 train 45 552\n",
      "Phase: train. Epoch: 85. Loss: 0.09636218845844269\n",
      "85 train 46 564\n",
      "Phase: train. Epoch: 85. Loss: 0.0650479644536972\n",
      "85 train 47 576\n",
      "Phase: train. Epoch: 85. Loss: 0.06283069401979446\n",
      "85 train 48 588\n",
      "Phase: train. Epoch: 85. Loss: 0.1065078005194664\n",
      "85 train 49 600\n",
      "Phase: train. Epoch: 85. Loss: 0.08445825427770615\n",
      "85 train 50 612\n",
      "Phase: train. Epoch: 85. Loss: 0.08258846402168274\n",
      "85 train 51 624\n",
      "Phase: train. Epoch: 85. Loss: 0.08027305454015732\n",
      "85 train 52 636\n",
      "Phase: train. Epoch: 85. Loss: 0.07497517764568329\n",
      "85 train 53 648\n",
      "Phase: train. Epoch: 85. Loss: 0.08759906888008118\n",
      "85 train 54 660\n",
      "Phase: train. Epoch: 85. Loss: 0.09008744359016418\n",
      "85 train 55 672\n",
      "Phase: train. Epoch: 85. Loss: 0.0977783203125\n",
      "85 train 56 684\n",
      "Phase: train. Epoch: 85. Loss: 0.085621178150177\n",
      "85 train 57 696\n",
      "Phase: train. Epoch: 85. Loss: 0.08534397929906845\n",
      "85 train 58 708\n",
      "Phase: train. Epoch: 85. Loss: 0.06679213047027588\n",
      "85 train 59 720\n",
      "Phase: train. Epoch: 85. Loss: 0.08150404691696167\n",
      "85 train 60 732\n",
      "Phase: train. Epoch: 85. Loss: 0.09144532680511475\n",
      "85 train 61 744\n",
      "Phase: train. Epoch: 85. Loss: 0.09676956385374069\n",
      "85 train 62 751\n",
      "Phase: train. Epoch: 85. Loss: 0.0627482458949089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 val 0 763\n",
      "Phase: val. Epoch: 85. Loss: 0.09042277187108994\n",
      "85 val 1 775\n",
      "Phase: val. Epoch: 85. Loss: 0.09169590473175049\n",
      "85 val 2 787\n",
      "Phase: val. Epoch: 85. Loss: 0.07068552821874619\n",
      "85 val 3 799\n",
      "Phase: val. Epoch: 85. Loss: 0.0649317279458046\n",
      "85 val 4 811\n",
      "Phase: val. Epoch: 85. Loss: 0.09818043559789658\n",
      "85 val 5 823\n",
      "Phase: val. Epoch: 85. Loss: 0.062006913125514984\n",
      "85 val 6 835\n",
      "Phase: val. Epoch: 85. Loss: 0.07946152985095978\n",
      "85 val 7 847\n",
      "Phase: val. Epoch: 85. Loss: 0.07085128128528595\n",
      "85 val 8 859\n",
      "Phase: val. Epoch: 85. Loss: 0.07301174849271774\n",
      "85 val 9 871\n",
      "Phase: val. Epoch: 85. Loss: 0.09432071447372437\n",
      "85 val 10 883\n",
      "Phase: val. Epoch: 85. Loss: 0.07663191854953766\n",
      "85 val 11 884\n",
      "Phase: val. Epoch: 85. Loss: 0.09528424590826035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 train 0 12\n",
      "Phase: train. Epoch: 86. Loss: 0.06331606209278107\n",
      "86 train 1 24\n",
      "Phase: train. Epoch: 86. Loss: 0.07464507222175598\n",
      "86 train 2 36\n",
      "Phase: train. Epoch: 86. Loss: 0.07463348656892776\n",
      "86 train 3 48\n",
      "Phase: train. Epoch: 86. Loss: 0.0804872214794159\n",
      "86 train 4 60\n",
      "Phase: train. Epoch: 86. Loss: 0.10483048856258392\n",
      "86 train 5 72\n",
      "Phase: train. Epoch: 86. Loss: 0.08160491287708282\n",
      "86 train 6 84\n",
      "Phase: train. Epoch: 86. Loss: 0.10095574706792831\n",
      "86 train 7 96\n",
      "Phase: train. Epoch: 86. Loss: 0.10564340651035309\n",
      "86 train 8 108\n",
      "Phase: train. Epoch: 86. Loss: 0.0625171959400177\n",
      "86 train 9 120\n",
      "Phase: train. Epoch: 86. Loss: 0.07100696116685867\n",
      "86 train 10 132\n",
      "Phase: train. Epoch: 86. Loss: 0.07058283686637878\n",
      "86 train 11 144\n",
      "Phase: train. Epoch: 86. Loss: 0.06695297360420227\n",
      "86 train 12 156\n",
      "Phase: train. Epoch: 86. Loss: 0.08700882643461227\n",
      "86 train 13 168\n",
      "Phase: train. Epoch: 86. Loss: 0.10674373805522919\n",
      "86 train 14 180\n",
      "Phase: train. Epoch: 86. Loss: 0.06991904228925705\n",
      "86 train 15 192\n",
      "Phase: train. Epoch: 86. Loss: 0.08904992043972015\n",
      "86 train 16 204\n",
      "Phase: train. Epoch: 86. Loss: 0.07827048003673553\n",
      "86 train 17 216\n",
      "Phase: train. Epoch: 86. Loss: 0.08542466163635254\n",
      "86 train 18 228\n",
      "Phase: train. Epoch: 86. Loss: 0.0839049443602562\n",
      "86 train 19 240\n",
      "Phase: train. Epoch: 86. Loss: 0.07009466737508774\n",
      "86 train 20 252\n",
      "Phase: train. Epoch: 86. Loss: 0.08962476253509521\n",
      "86 train 21 264\n",
      "Phase: train. Epoch: 86. Loss: 0.10845150798559189\n",
      "86 train 22 276\n",
      "Phase: train. Epoch: 86. Loss: 0.09261789172887802\n",
      "86 train 23 288\n",
      "Phase: train. Epoch: 86. Loss: 0.06698010861873627\n",
      "86 train 24 300\n",
      "Phase: train. Epoch: 86. Loss: 0.07271569967269897\n",
      "86 train 25 312\n",
      "Phase: train. Epoch: 86. Loss: 0.05837828665971756\n",
      "86 train 26 324\n",
      "Phase: train. Epoch: 86. Loss: 0.06823863834142685\n",
      "86 train 27 336\n",
      "Phase: train. Epoch: 86. Loss: 0.09890109300613403\n",
      "86 train 28 348\n",
      "Phase: train. Epoch: 86. Loss: 0.08603210747241974\n",
      "86 train 29 360\n",
      "Phase: train. Epoch: 86. Loss: 0.07498393952846527\n",
      "86 train 30 372\n",
      "Phase: train. Epoch: 86. Loss: 0.07861737161874771\n",
      "86 train 31 384\n",
      "Phase: train. Epoch: 86. Loss: 0.08595313876867294\n",
      "86 train 32 396\n",
      "Phase: train. Epoch: 86. Loss: 0.08292336016893387\n",
      "86 train 33 408\n",
      "Phase: train. Epoch: 86. Loss: 0.0839528888463974\n",
      "86 train 34 420\n",
      "Phase: train. Epoch: 86. Loss: 0.07796882092952728\n",
      "86 train 35 432\n",
      "Phase: train. Epoch: 86. Loss: 0.06605418026447296\n",
      "86 train 36 444\n",
      "Phase: train. Epoch: 86. Loss: 0.08556193113327026\n",
      "86 train 37 456\n",
      "Phase: train. Epoch: 86. Loss: 0.08640065044164658\n",
      "86 train 38 468\n",
      "Phase: train. Epoch: 86. Loss: 0.11451464891433716\n",
      "86 train 39 480\n",
      "Phase: train. Epoch: 86. Loss: 0.08541978895664215\n",
      "86 train 40 492\n",
      "Phase: train. Epoch: 86. Loss: 0.08666802942752838\n",
      "86 train 41 504\n",
      "Phase: train. Epoch: 86. Loss: 0.08255580067634583\n",
      "86 train 42 516\n",
      "Phase: train. Epoch: 86. Loss: 0.05949399992823601\n",
      "86 train 43 528\n",
      "Phase: train. Epoch: 86. Loss: 0.09355141967535019\n",
      "86 train 44 540\n",
      "Phase: train. Epoch: 86. Loss: 0.09461933374404907\n",
      "86 train 45 552\n",
      "Phase: train. Epoch: 86. Loss: 0.08413925766944885\n",
      "86 train 46 564\n",
      "Phase: train. Epoch: 86. Loss: 0.08618365973234177\n",
      "86 train 47 576\n",
      "Phase: train. Epoch: 86. Loss: 0.08175571262836456\n",
      "86 train 48 588\n",
      "Phase: train. Epoch: 86. Loss: 0.0668335109949112\n",
      "86 train 49 600\n",
      "Phase: train. Epoch: 86. Loss: 0.09005759656429291\n",
      "86 train 50 612\n",
      "Phase: train. Epoch: 86. Loss: 0.08583756536245346\n",
      "86 train 51 624\n",
      "Phase: train. Epoch: 86. Loss: 0.07546718418598175\n",
      "86 train 52 636\n",
      "Phase: train. Epoch: 86. Loss: 0.07786838710308075\n",
      "86 train 53 648\n",
      "Phase: train. Epoch: 86. Loss: 0.09802208840847015\n",
      "86 train 54 660\n",
      "Phase: train. Epoch: 86. Loss: 0.1417417824268341\n",
      "86 train 55 672\n",
      "Phase: train. Epoch: 86. Loss: 0.09551846981048584\n",
      "86 train 56 684\n",
      "Phase: train. Epoch: 86. Loss: 0.07802131772041321\n",
      "86 train 57 696\n",
      "Phase: train. Epoch: 86. Loss: 0.04958299547433853\n",
      "86 train 58 708\n",
      "Phase: train. Epoch: 86. Loss: 0.10402165353298187\n",
      "86 train 59 720\n",
      "Phase: train. Epoch: 86. Loss: 0.08363791555166245\n",
      "86 train 60 732\n",
      "Phase: train. Epoch: 86. Loss: 0.07344645261764526\n",
      "86 train 61 744\n",
      "Phase: train. Epoch: 86. Loss: 0.08206453174352646\n",
      "86 train 62 751\n",
      "Phase: train. Epoch: 86. Loss: 0.07687714695930481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 val 0 763\n",
      "Phase: val. Epoch: 86. Loss: 0.08106409013271332\n",
      "86 val 1 775\n",
      "Phase: val. Epoch: 86. Loss: 0.09731756895780563\n",
      "86 val 2 787\n",
      "Phase: val. Epoch: 86. Loss: 0.0661560595035553\n",
      "86 val 3 799\n",
      "Phase: val. Epoch: 86. Loss: 0.0736498236656189\n",
      "86 val 4 811\n",
      "Phase: val. Epoch: 86. Loss: 0.06445970386266708\n",
      "86 val 5 823\n",
      "Phase: val. Epoch: 86. Loss: 0.08396746963262558\n",
      "86 val 6 835\n",
      "Phase: val. Epoch: 86. Loss: 0.09000762552022934\n",
      "86 val 7 847\n",
      "Phase: val. Epoch: 86. Loss: 0.08192022889852524\n",
      "86 val 8 859\n",
      "Phase: val. Epoch: 86. Loss: 0.08495321124792099\n",
      "86 val 9 871\n",
      "Phase: val. Epoch: 86. Loss: 0.07597825676202774\n",
      "86 val 10 883\n",
      "Phase: val. Epoch: 86. Loss: 0.0955396518111229\n",
      "86 val 11 884\n",
      "Phase: val. Epoch: 86. Loss: 0.0251679178327322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 train 0 12\n",
      "Phase: train. Epoch: 87. Loss: 0.07508818805217743\n",
      "87 train 1 24\n",
      "Phase: train. Epoch: 87. Loss: 0.0795854926109314\n",
      "87 train 2 36\n",
      "Phase: train. Epoch: 87. Loss: 0.08343708515167236\n",
      "87 train 3 48\n",
      "Phase: train. Epoch: 87. Loss: 0.11340183019638062\n",
      "87 train 4 60\n",
      "Phase: train. Epoch: 87. Loss: 0.1105814203619957\n",
      "87 train 5 72\n",
      "Phase: train. Epoch: 87. Loss: 0.07588176429271698\n",
      "87 train 6 84\n",
      "Phase: train. Epoch: 87. Loss: 0.09776517003774643\n",
      "87 train 7 96\n",
      "Phase: train. Epoch: 87. Loss: 0.07481835782527924\n",
      "87 train 8 108\n",
      "Phase: train. Epoch: 87. Loss: 0.11767563223838806\n",
      "87 train 9 120\n",
      "Phase: train. Epoch: 87. Loss: 0.0950215756893158\n",
      "87 train 10 132\n",
      "Phase: train. Epoch: 87. Loss: 0.07127245515584946\n",
      "87 train 11 144\n",
      "Phase: train. Epoch: 87. Loss: 0.09063472598791122\n",
      "87 train 12 156\n",
      "Phase: train. Epoch: 87. Loss: 0.061780672520399094\n",
      "87 train 13 168\n",
      "Phase: train. Epoch: 87. Loss: 0.06203383952379227\n",
      "87 train 14 180\n",
      "Phase: train. Epoch: 87. Loss: 0.09269043058156967\n",
      "87 train 15 192\n",
      "Phase: train. Epoch: 87. Loss: 0.07893957942724228\n",
      "87 train 16 204\n",
      "Phase: train. Epoch: 87. Loss: 0.10695739835500717\n",
      "87 train 17 216\n",
      "Phase: train. Epoch: 87. Loss: 0.06293362379074097\n",
      "87 train 18 228\n",
      "Phase: train. Epoch: 87. Loss: 0.07619360834360123\n",
      "87 train 19 240\n",
      "Phase: train. Epoch: 87. Loss: 0.10443973541259766\n",
      "87 train 20 252\n",
      "Phase: train. Epoch: 87. Loss: 0.09582468867301941\n",
      "87 train 21 264\n",
      "Phase: train. Epoch: 87. Loss: 0.10264544188976288\n",
      "87 train 22 276\n",
      "Phase: train. Epoch: 87. Loss: 0.06051415950059891\n",
      "87 train 23 288\n",
      "Phase: train. Epoch: 87. Loss: 0.08462252467870712\n",
      "87 train 24 300\n",
      "Phase: train. Epoch: 87. Loss: 0.06351032108068466\n",
      "87 train 25 312\n",
      "Phase: train. Epoch: 87. Loss: 0.08317123353481293\n",
      "87 train 26 324\n",
      "Phase: train. Epoch: 87. Loss: 0.10782146453857422\n",
      "87 train 27 336\n",
      "Phase: train. Epoch: 87. Loss: 0.07906069606542587\n",
      "87 train 28 348\n",
      "Phase: train. Epoch: 87. Loss: 0.08762243390083313\n",
      "87 train 29 360\n",
      "Phase: train. Epoch: 87. Loss: 0.11132386326789856\n",
      "87 train 30 372\n",
      "Phase: train. Epoch: 87. Loss: 0.06305109709501266\n",
      "87 train 31 384\n",
      "Phase: train. Epoch: 87. Loss: 0.0642397403717041\n",
      "87 train 32 396\n",
      "Phase: train. Epoch: 87. Loss: 0.09439943730831146\n",
      "87 train 33 408\n",
      "Phase: train. Epoch: 87. Loss: 0.0959809273481369\n",
      "87 train 34 420\n",
      "Phase: train. Epoch: 87. Loss: 0.07195930927991867\n",
      "87 train 35 432\n",
      "Phase: train. Epoch: 87. Loss: 0.08700092881917953\n",
      "87 train 36 444\n",
      "Phase: train. Epoch: 87. Loss: 0.08848355710506439\n",
      "87 train 37 456\n",
      "Phase: train. Epoch: 87. Loss: 0.0616152286529541\n",
      "87 train 38 468\n",
      "Phase: train. Epoch: 87. Loss: 0.06742015480995178\n",
      "87 train 39 480\n",
      "Phase: train. Epoch: 87. Loss: 0.07227334380149841\n",
      "87 train 40 492\n",
      "Phase: train. Epoch: 87. Loss: 0.0778908059000969\n",
      "87 train 41 504\n",
      "Phase: train. Epoch: 87. Loss: 0.07776542752981186\n",
      "87 train 42 516\n",
      "Phase: train. Epoch: 87. Loss: 0.07054639607667923\n",
      "87 train 43 528\n",
      "Phase: train. Epoch: 87. Loss: 0.10275675356388092\n",
      "87 train 44 540\n",
      "Phase: train. Epoch: 87. Loss: 0.09031673520803452\n",
      "87 train 45 552\n",
      "Phase: train. Epoch: 87. Loss: 0.09221325814723969\n",
      "87 train 46 564\n",
      "Phase: train. Epoch: 87. Loss: 0.07904869318008423\n",
      "87 train 47 576\n",
      "Phase: train. Epoch: 87. Loss: 0.12088959664106369\n",
      "87 train 48 588\n",
      "Phase: train. Epoch: 87. Loss: 0.08305270969867706\n",
      "87 train 49 600\n",
      "Phase: train. Epoch: 87. Loss: 0.10492028295993805\n",
      "87 train 50 612\n",
      "Phase: train. Epoch: 87. Loss: 0.07327032089233398\n",
      "87 train 51 624\n",
      "Phase: train. Epoch: 87. Loss: 0.05621711164712906\n",
      "87 train 52 636\n",
      "Phase: train. Epoch: 87. Loss: 0.09630756825208664\n",
      "87 train 53 648\n",
      "Phase: train. Epoch: 87. Loss: 0.09815409779548645\n",
      "87 train 54 660\n",
      "Phase: train. Epoch: 87. Loss: 0.07309968769550323\n",
      "87 train 55 672\n",
      "Phase: train. Epoch: 87. Loss: 0.06106073781847954\n",
      "87 train 56 684\n",
      "Phase: train. Epoch: 87. Loss: 0.0718136578798294\n",
      "87 train 57 696\n",
      "Phase: train. Epoch: 87. Loss: 0.08048388361930847\n",
      "87 train 58 708\n",
      "Phase: train. Epoch: 87. Loss: 0.0784597247838974\n",
      "87 train 59 720\n",
      "Phase: train. Epoch: 87. Loss: 0.12860089540481567\n",
      "87 train 60 732\n",
      "Phase: train. Epoch: 87. Loss: 0.05869090557098389\n",
      "87 train 61 744\n",
      "Phase: train. Epoch: 87. Loss: 0.09527081251144409\n",
      "87 train 62 751\n",
      "Phase: train. Epoch: 87. Loss: 0.05670197680592537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 val 0 763\n",
      "Phase: val. Epoch: 87. Loss: 0.07095775008201599\n",
      "87 val 1 775\n",
      "Phase: val. Epoch: 87. Loss: 0.06581246852874756\n",
      "87 val 2 787\n",
      "Phase: val. Epoch: 87. Loss: 0.09701549261808395\n",
      "87 val 3 799\n",
      "Phase: val. Epoch: 87. Loss: 0.07894822210073471\n",
      "87 val 4 811\n",
      "Phase: val. Epoch: 87. Loss: 0.10819091647863388\n",
      "87 val 5 823\n",
      "Phase: val. Epoch: 87. Loss: 0.05386185273528099\n",
      "87 val 6 835\n",
      "Phase: val. Epoch: 87. Loss: 0.07315363734960556\n",
      "87 val 7 847\n",
      "Phase: val. Epoch: 87. Loss: 0.08000831305980682\n",
      "87 val 8 859\n",
      "Phase: val. Epoch: 87. Loss: 0.07690626382827759\n",
      "87 val 9 871\n",
      "Phase: val. Epoch: 87. Loss: 0.06448787450790405\n",
      "87 val 10 883\n",
      "Phase: val. Epoch: 87. Loss: 0.08239315450191498\n",
      "87 val 11 884\n",
      "Phase: val. Epoch: 87. Loss: 0.184821218252182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 train 0 12\n",
      "Phase: train. Epoch: 88. Loss: 0.08375702798366547\n",
      "88 train 1 24\n",
      "Phase: train. Epoch: 88. Loss: 0.0807088315486908\n",
      "88 train 2 36\n",
      "Phase: train. Epoch: 88. Loss: 0.11226062476634979\n",
      "88 train 3 48\n",
      "Phase: train. Epoch: 88. Loss: 0.07174918055534363\n",
      "88 train 4 60\n",
      "Phase: train. Epoch: 88. Loss: 0.0912247747182846\n",
      "88 train 5 72\n",
      "Phase: train. Epoch: 88. Loss: 0.07776261866092682\n",
      "88 train 6 84\n",
      "Phase: train. Epoch: 88. Loss: 0.08205465972423553\n",
      "88 train 7 96\n",
      "Phase: train. Epoch: 88. Loss: 0.0923922061920166\n",
      "88 train 8 108\n",
      "Phase: train. Epoch: 88. Loss: 0.06872770190238953\n",
      "88 train 9 120\n",
      "Phase: train. Epoch: 88. Loss: 0.0826793983578682\n",
      "88 train 10 132\n",
      "Phase: train. Epoch: 88. Loss: 0.06717751920223236\n",
      "88 train 11 144\n",
      "Phase: train. Epoch: 88. Loss: 0.07993835210800171\n",
      "88 train 12 156\n",
      "Phase: train. Epoch: 88. Loss: 0.06618893146514893\n",
      "88 train 13 168\n",
      "Phase: train. Epoch: 88. Loss: 0.10512784123420715\n",
      "88 train 14 180\n",
      "Phase: train. Epoch: 88. Loss: 0.06654347479343414\n",
      "88 train 15 192\n",
      "Phase: train. Epoch: 88. Loss: 0.09404059499502182\n",
      "88 train 16 204\n",
      "Phase: train. Epoch: 88. Loss: 0.07779566943645477\n",
      "88 train 17 216\n",
      "Phase: train. Epoch: 88. Loss: 0.060718994587659836\n",
      "88 train 18 228\n",
      "Phase: train. Epoch: 88. Loss: 0.07561250776052475\n",
      "88 train 19 240\n",
      "Phase: train. Epoch: 88. Loss: 0.06447617709636688\n",
      "88 train 20 252\n",
      "Phase: train. Epoch: 88. Loss: 0.0731625184416771\n",
      "88 train 21 264\n",
      "Phase: train. Epoch: 88. Loss: 0.09864316880702972\n",
      "88 train 22 276\n",
      "Phase: train. Epoch: 88. Loss: 0.08308790624141693\n",
      "88 train 23 288\n",
      "Phase: train. Epoch: 88. Loss: 0.1099732369184494\n",
      "88 train 24 300\n",
      "Phase: train. Epoch: 88. Loss: 0.09124278277158737\n",
      "88 train 25 312\n",
      "Phase: train. Epoch: 88. Loss: 0.06175105273723602\n",
      "88 train 26 324\n",
      "Phase: train. Epoch: 88. Loss: 0.09465839713811874\n",
      "88 train 27 336\n",
      "Phase: train. Epoch: 88. Loss: 0.07377061247825623\n",
      "88 train 28 348\n",
      "Phase: train. Epoch: 88. Loss: 0.0748843103647232\n",
      "88 train 29 360\n",
      "Phase: train. Epoch: 88. Loss: 0.09591273963451385\n",
      "88 train 30 372\n",
      "Phase: train. Epoch: 88. Loss: 0.07175128161907196\n",
      "88 train 31 384\n",
      "Phase: train. Epoch: 88. Loss: 0.08634921163320541\n",
      "88 train 32 396\n",
      "Phase: train. Epoch: 88. Loss: 0.06425631046295166\n",
      "88 train 33 408\n",
      "Phase: train. Epoch: 88. Loss: 0.09248003363609314\n",
      "88 train 34 420\n",
      "Phase: train. Epoch: 88. Loss: 0.05301691219210625\n",
      "88 train 35 432\n",
      "Phase: train. Epoch: 88. Loss: 0.06384363770484924\n",
      "88 train 36 444\n",
      "Phase: train. Epoch: 88. Loss: 0.07958316057920456\n",
      "88 train 37 456\n",
      "Phase: train. Epoch: 88. Loss: 0.08060705661773682\n",
      "88 train 38 468\n",
      "Phase: train. Epoch: 88. Loss: 0.0798247903585434\n",
      "88 train 39 480\n",
      "Phase: train. Epoch: 88. Loss: 0.09418654441833496\n",
      "88 train 40 492\n",
      "Phase: train. Epoch: 88. Loss: 0.08382785320281982\n",
      "88 train 41 504\n",
      "Phase: train. Epoch: 88. Loss: 0.10503723472356796\n",
      "88 train 42 516\n",
      "Phase: train. Epoch: 88. Loss: 0.07304257154464722\n",
      "88 train 43 528\n",
      "Phase: train. Epoch: 88. Loss: 0.07177060842514038\n",
      "88 train 44 540\n",
      "Phase: train. Epoch: 88. Loss: 0.0900670513510704\n",
      "88 train 45 552\n",
      "Phase: train. Epoch: 88. Loss: 0.11907856166362762\n",
      "88 train 46 564\n",
      "Phase: train. Epoch: 88. Loss: 0.08798880875110626\n",
      "88 train 47 576\n",
      "Phase: train. Epoch: 88. Loss: 0.08315800875425339\n",
      "88 train 48 588\n",
      "Phase: train. Epoch: 88. Loss: 0.0825616791844368\n",
      "88 train 49 600\n",
      "Phase: train. Epoch: 88. Loss: 0.11293880641460419\n",
      "88 train 50 612\n",
      "Phase: train. Epoch: 88. Loss: 0.06606516242027283\n",
      "88 train 51 624\n",
      "Phase: train. Epoch: 88. Loss: 0.13254371285438538\n",
      "88 train 52 636\n",
      "Phase: train. Epoch: 88. Loss: 0.06330863386392593\n",
      "88 train 53 648\n",
      "Phase: train. Epoch: 88. Loss: 0.10715292394161224\n",
      "88 train 54 660\n",
      "Phase: train. Epoch: 88. Loss: 0.061951905488967896\n",
      "88 train 55 672\n",
      "Phase: train. Epoch: 88. Loss: 0.09620825946331024\n",
      "88 train 56 684\n",
      "Phase: train. Epoch: 88. Loss: 0.08344411104917526\n",
      "88 train 57 696\n",
      "Phase: train. Epoch: 88. Loss: 0.08185037225484848\n",
      "88 train 58 708\n",
      "Phase: train. Epoch: 88. Loss: 0.08391230553388596\n",
      "88 train 59 720\n",
      "Phase: train. Epoch: 88. Loss: 0.1196122020483017\n",
      "88 train 60 732\n",
      "Phase: train. Epoch: 88. Loss: 0.08680444955825806\n",
      "88 train 61 744\n",
      "Phase: train. Epoch: 88. Loss: 0.07698161900043488\n",
      "88 train 62 751\n",
      "Phase: train. Epoch: 88. Loss: 0.07798787206411362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 val 0 763\n",
      "Phase: val. Epoch: 88. Loss: 0.07951544225215912\n",
      "88 val 1 775\n",
      "Phase: val. Epoch: 88. Loss: 0.07020851224660873\n",
      "88 val 2 787\n",
      "Phase: val. Epoch: 88. Loss: 0.12177489697933197\n",
      "88 val 3 799\n",
      "Phase: val. Epoch: 88. Loss: 0.08538755029439926\n",
      "88 val 4 811\n",
      "Phase: val. Epoch: 88. Loss: 0.05004771053791046\n",
      "88 val 5 823\n",
      "Phase: val. Epoch: 88. Loss: 0.08263331651687622\n",
      "88 val 6 835\n",
      "Phase: val. Epoch: 88. Loss: 0.09444761276245117\n",
      "88 val 7 847\n",
      "Phase: val. Epoch: 88. Loss: 0.10419729351997375\n",
      "88 val 8 859\n",
      "Phase: val. Epoch: 88. Loss: 0.060448579490184784\n",
      "88 val 9 871\n",
      "Phase: val. Epoch: 88. Loss: 0.07672428339719772\n",
      "88 val 10 883\n",
      "Phase: val. Epoch: 88. Loss: 0.07263588160276413\n",
      "88 val 11 884\n",
      "Phase: val. Epoch: 88. Loss: 0.024805087596178055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 train 0 12\n",
      "Phase: train. Epoch: 89. Loss: 0.08715999126434326\n",
      "89 train 1 24\n",
      "Phase: train. Epoch: 89. Loss: 0.07584434002637863\n",
      "89 train 2 36\n",
      "Phase: train. Epoch: 89. Loss: 0.08989511430263519\n",
      "89 train 3 48\n",
      "Phase: train. Epoch: 89. Loss: 0.08979489654302597\n",
      "89 train 4 60\n",
      "Phase: train. Epoch: 89. Loss: 0.1134633868932724\n",
      "89 train 5 72\n",
      "Phase: train. Epoch: 89. Loss: 0.05656985566020012\n",
      "89 train 6 84\n",
      "Phase: train. Epoch: 89. Loss: 0.09567813575267792\n",
      "89 train 7 96\n",
      "Phase: train. Epoch: 89. Loss: 0.08747373521327972\n",
      "89 train 8 108\n",
      "Phase: train. Epoch: 89. Loss: 0.08843539655208588\n",
      "89 train 9 120\n",
      "Phase: train. Epoch: 89. Loss: 0.08863893151283264\n",
      "89 train 10 132\n",
      "Phase: train. Epoch: 89. Loss: 0.0822484940290451\n",
      "89 train 11 144\n",
      "Phase: train. Epoch: 89. Loss: 0.08700563758611679\n",
      "89 train 12 156\n",
      "Phase: train. Epoch: 89. Loss: 0.0759829431772232\n",
      "89 train 13 168\n",
      "Phase: train. Epoch: 89. Loss: 0.08539743721485138\n",
      "89 train 14 180\n",
      "Phase: train. Epoch: 89. Loss: 0.07903766632080078\n",
      "89 train 15 192\n",
      "Phase: train. Epoch: 89. Loss: 0.06487981975078583\n",
      "89 train 16 204\n",
      "Phase: train. Epoch: 89. Loss: 0.07793877273797989\n",
      "89 train 17 216\n",
      "Phase: train. Epoch: 89. Loss: 0.08122643828392029\n",
      "89 train 18 228\n",
      "Phase: train. Epoch: 89. Loss: 0.08710607141256332\n",
      "89 train 19 240\n",
      "Phase: train. Epoch: 89. Loss: 0.09120148420333862\n",
      "89 train 20 252\n",
      "Phase: train. Epoch: 89. Loss: 0.09344138205051422\n",
      "89 train 21 264\n",
      "Phase: train. Epoch: 89. Loss: 0.0692378580570221\n",
      "89 train 22 276\n",
      "Phase: train. Epoch: 89. Loss: 0.09455034136772156\n",
      "89 train 23 288\n",
      "Phase: train. Epoch: 89. Loss: 0.06705906242132187\n",
      "89 train 24 300\n",
      "Phase: train. Epoch: 89. Loss: 0.08616643399000168\n",
      "89 train 25 312\n",
      "Phase: train. Epoch: 89. Loss: 0.07808954268693924\n",
      "89 train 26 324\n",
      "Phase: train. Epoch: 89. Loss: 0.09807994961738586\n",
      "89 train 27 336\n",
      "Phase: train. Epoch: 89. Loss: 0.064712755382061\n",
      "89 train 28 348\n",
      "Phase: train. Epoch: 89. Loss: 0.08824975788593292\n",
      "89 train 29 360\n",
      "Phase: train. Epoch: 89. Loss: 0.10096346586942673\n",
      "89 train 30 372\n",
      "Phase: train. Epoch: 89. Loss: 0.07980598509311676\n",
      "89 train 31 384\n",
      "Phase: train. Epoch: 89. Loss: 0.08310937136411667\n",
      "89 train 32 396\n",
      "Phase: train. Epoch: 89. Loss: 0.08066478371620178\n",
      "89 train 33 408\n",
      "Phase: train. Epoch: 89. Loss: 0.1023220419883728\n",
      "89 train 34 420\n",
      "Phase: train. Epoch: 89. Loss: 0.07948290556669235\n",
      "89 train 35 432\n",
      "Phase: train. Epoch: 89. Loss: 0.09752538055181503\n",
      "89 train 36 444\n",
      "Phase: train. Epoch: 89. Loss: 0.086347296833992\n",
      "89 train 37 456\n",
      "Phase: train. Epoch: 89. Loss: 0.07007992267608643\n",
      "89 train 38 468\n",
      "Phase: train. Epoch: 89. Loss: 0.08310636132955551\n",
      "89 train 39 480\n",
      "Phase: train. Epoch: 89. Loss: 0.07280424237251282\n",
      "89 train 40 492\n",
      "Phase: train. Epoch: 89. Loss: 0.11081725358963013\n",
      "89 train 41 504\n",
      "Phase: train. Epoch: 89. Loss: 0.06937878578901291\n",
      "89 train 42 516\n",
      "Phase: train. Epoch: 89. Loss: 0.07353523373603821\n",
      "89 train 43 528\n",
      "Phase: train. Epoch: 89. Loss: 0.08252857625484467\n",
      "89 train 44 540\n",
      "Phase: train. Epoch: 89. Loss: 0.08431898057460785\n",
      "89 train 45 552\n",
      "Phase: train. Epoch: 89. Loss: 0.07655800879001617\n",
      "89 train 46 564\n",
      "Phase: train. Epoch: 89. Loss: 0.07032700628042221\n",
      "89 train 47 576\n",
      "Phase: train. Epoch: 89. Loss: 0.09748472273349762\n",
      "89 train 48 588\n",
      "Phase: train. Epoch: 89. Loss: 0.09675990045070648\n",
      "89 train 49 600\n",
      "Phase: train. Epoch: 89. Loss: 0.08591627329587936\n",
      "89 train 50 612\n",
      "Phase: train. Epoch: 89. Loss: 0.07565716654062271\n",
      "89 train 51 624\n",
      "Phase: train. Epoch: 89. Loss: 0.09899204224348068\n",
      "89 train 52 636\n",
      "Phase: train. Epoch: 89. Loss: 0.08338841795921326\n",
      "89 train 53 648\n",
      "Phase: train. Epoch: 89. Loss: 0.10569065064191818\n",
      "89 train 54 660\n",
      "Phase: train. Epoch: 89. Loss: 0.0642789974808693\n",
      "89 train 55 672\n",
      "Phase: train. Epoch: 89. Loss: 0.0855945572257042\n",
      "89 train 56 684\n",
      "Phase: train. Epoch: 89. Loss: 0.08260827511548996\n",
      "89 train 57 696\n",
      "Phase: train. Epoch: 89. Loss: 0.08886884152889252\n",
      "89 train 58 708\n",
      "Phase: train. Epoch: 89. Loss: 0.06989048421382904\n",
      "89 train 59 720\n",
      "Phase: train. Epoch: 89. Loss: 0.08525978028774261\n",
      "89 train 60 732\n",
      "Phase: train. Epoch: 89. Loss: 0.0546918660402298\n",
      "89 train 61 744\n",
      "Phase: train. Epoch: 89. Loss: 0.06536545604467392\n",
      "89 train 62 751\n",
      "Phase: train. Epoch: 89. Loss: 0.07530589401721954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 val 0 763\n",
      "Phase: val. Epoch: 89. Loss: 0.08395349979400635\n",
      "89 val 1 775\n",
      "Phase: val. Epoch: 89. Loss: 0.04324101656675339\n",
      "89 val 2 787\n",
      "Phase: val. Epoch: 89. Loss: 0.06505867838859558\n",
      "89 val 3 799\n",
      "Phase: val. Epoch: 89. Loss: 0.09664996713399887\n",
      "89 val 4 811\n",
      "Phase: val. Epoch: 89. Loss: 0.08770527690649033\n",
      "89 val 5 823\n",
      "Phase: val. Epoch: 89. Loss: 0.0925646647810936\n",
      "89 val 6 835\n",
      "Phase: val. Epoch: 89. Loss: 0.08185333013534546\n",
      "89 val 7 847\n",
      "Phase: val. Epoch: 89. Loss: 0.0893932431936264\n",
      "89 val 8 859\n",
      "Phase: val. Epoch: 89. Loss: 0.09399612247943878\n",
      "89 val 9 871\n",
      "Phase: val. Epoch: 89. Loss: 0.040973152965307236\n",
      "89 val 10 883\n",
      "Phase: val. Epoch: 89. Loss: 0.0916692465543747\n",
      "89 val 11 884\n",
      "Phase: val. Epoch: 89. Loss: 0.05922172963619232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 train 0 12\n",
      "Phase: train. Epoch: 90. Loss: 0.08402590453624725\n",
      "90 train 1 24\n",
      "Phase: train. Epoch: 90. Loss: 0.06993915140628815\n",
      "90 train 2 36\n",
      "Phase: train. Epoch: 90. Loss: 0.10865041613578796\n",
      "90 train 3 48\n",
      "Phase: train. Epoch: 90. Loss: 0.06682638078927994\n",
      "90 train 4 60\n",
      "Phase: train. Epoch: 90. Loss: 0.07192793488502502\n",
      "90 train 5 72\n",
      "Phase: train. Epoch: 90. Loss: 0.07116265594959259\n",
      "90 train 6 84\n",
      "Phase: train. Epoch: 90. Loss: 0.08337279409170151\n",
      "90 train 7 96\n",
      "Phase: train. Epoch: 90. Loss: 0.08602532744407654\n",
      "90 train 8 108\n",
      "Phase: train. Epoch: 90. Loss: 0.10035749524831772\n",
      "90 train 9 120\n",
      "Phase: train. Epoch: 90. Loss: 0.07195857912302017\n",
      "90 train 10 132\n",
      "Phase: train. Epoch: 90. Loss: 0.07544471323490143\n",
      "90 train 11 144\n",
      "Phase: train. Epoch: 90. Loss: 0.08772256225347519\n",
      "90 train 12 156\n",
      "Phase: train. Epoch: 90. Loss: 0.07340210676193237\n",
      "90 train 13 168\n",
      "Phase: train. Epoch: 90. Loss: 0.07990722358226776\n",
      "90 train 14 180\n",
      "Phase: train. Epoch: 90. Loss: 0.059970952570438385\n",
      "90 train 15 192\n",
      "Phase: train. Epoch: 90. Loss: 0.08725988864898682\n",
      "90 train 16 204\n",
      "Phase: train. Epoch: 90. Loss: 0.07907998561859131\n",
      "90 train 17 216\n",
      "Phase: train. Epoch: 90. Loss: 0.06549273431301117\n",
      "90 train 18 228\n",
      "Phase: train. Epoch: 90. Loss: 0.09453938901424408\n",
      "90 train 19 240\n",
      "Phase: train. Epoch: 90. Loss: 0.10357113182544708\n",
      "90 train 20 252\n",
      "Phase: train. Epoch: 90. Loss: 0.05715484544634819\n",
      "90 train 21 264\n",
      "Phase: train. Epoch: 90. Loss: 0.08980187028646469\n",
      "90 train 22 276\n",
      "Phase: train. Epoch: 90. Loss: 0.08729293942451477\n",
      "90 train 23 288\n",
      "Phase: train. Epoch: 90. Loss: 0.10218873620033264\n",
      "90 train 24 300\n",
      "Phase: train. Epoch: 90. Loss: 0.08264490962028503\n",
      "90 train 25 312\n",
      "Phase: train. Epoch: 90. Loss: 0.06269341707229614\n",
      "90 train 26 324\n",
      "Phase: train. Epoch: 90. Loss: 0.10836891829967499\n",
      "90 train 27 336\n",
      "Phase: train. Epoch: 90. Loss: 0.07433576881885529\n",
      "90 train 28 348\n",
      "Phase: train. Epoch: 90. Loss: 0.0800064206123352\n",
      "90 train 29 360\n",
      "Phase: train. Epoch: 90. Loss: 0.06213899701833725\n",
      "90 train 30 372\n",
      "Phase: train. Epoch: 90. Loss: 0.09357961267232895\n",
      "90 train 31 384\n",
      "Phase: train. Epoch: 90. Loss: 0.06583929806947708\n",
      "90 train 32 396\n",
      "Phase: train. Epoch: 90. Loss: 0.06099507957696915\n",
      "90 train 33 408\n",
      "Phase: train. Epoch: 90. Loss: 0.08088561147451401\n",
      "90 train 34 420\n",
      "Phase: train. Epoch: 90. Loss: 0.06879055500030518\n",
      "90 train 35 432\n",
      "Phase: train. Epoch: 90. Loss: 0.07762506604194641\n",
      "90 train 36 444\n",
      "Phase: train. Epoch: 90. Loss: 0.06292945146560669\n",
      "90 train 37 456\n",
      "Phase: train. Epoch: 90. Loss: 0.06611593812704086\n",
      "90 train 38 468\n",
      "Phase: train. Epoch: 90. Loss: 0.10165385901927948\n",
      "90 train 39 480\n",
      "Phase: train. Epoch: 90. Loss: 0.0762825533747673\n",
      "90 train 40 492\n",
      "Phase: train. Epoch: 90. Loss: 0.0868777260184288\n",
      "90 train 41 504\n",
      "Phase: train. Epoch: 90. Loss: 0.09222322702407837\n",
      "90 train 42 516\n",
      "Phase: train. Epoch: 90. Loss: 0.07973378151655197\n",
      "90 train 43 528\n",
      "Phase: train. Epoch: 90. Loss: 0.09199174493551254\n",
      "90 train 44 540\n",
      "Phase: train. Epoch: 90. Loss: 0.09477347135543823\n",
      "90 train 45 552\n",
      "Phase: train. Epoch: 90. Loss: 0.09319566190242767\n",
      "90 train 46 564\n",
      "Phase: train. Epoch: 90. Loss: 0.08413004130125046\n",
      "90 train 47 576\n",
      "Phase: train. Epoch: 90. Loss: 0.0880206823348999\n",
      "90 train 48 588\n",
      "Phase: train. Epoch: 90. Loss: 0.101114422082901\n",
      "90 train 49 600\n",
      "Phase: train. Epoch: 90. Loss: 0.0950063094496727\n",
      "90 train 50 612\n",
      "Phase: train. Epoch: 90. Loss: 0.08097008615732193\n",
      "90 train 51 624\n",
      "Phase: train. Epoch: 90. Loss: 0.07679976522922516\n",
      "90 train 52 636\n",
      "Phase: train. Epoch: 90. Loss: 0.0780300572514534\n",
      "90 train 53 648\n",
      "Phase: train. Epoch: 90. Loss: 0.08640943467617035\n",
      "90 train 54 660\n",
      "Phase: train. Epoch: 90. Loss: 0.08064568042755127\n",
      "90 train 55 672\n",
      "Phase: train. Epoch: 90. Loss: 0.09773524850606918\n",
      "90 train 56 684\n",
      "Phase: train. Epoch: 90. Loss: 0.09489592164754868\n",
      "90 train 57 696\n",
      "Phase: train. Epoch: 90. Loss: 0.07449167966842651\n",
      "90 train 58 708\n",
      "Phase: train. Epoch: 90. Loss: 0.08578142523765564\n",
      "90 train 59 720\n",
      "Phase: train. Epoch: 90. Loss: 0.08720755577087402\n",
      "90 train 60 732\n",
      "Phase: train. Epoch: 90. Loss: 0.07232282310724258\n",
      "90 train 61 744\n",
      "Phase: train. Epoch: 90. Loss: 0.07371698319911957\n",
      "90 train 62 751\n",
      "Phase: train. Epoch: 90. Loss: 0.06664776057004929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 val 0 763\n",
      "Phase: val. Epoch: 90. Loss: 0.08158992975950241\n",
      "90 val 1 775\n",
      "Phase: val. Epoch: 90. Loss: 0.08081957697868347\n",
      "90 val 2 787\n",
      "Phase: val. Epoch: 90. Loss: 0.07993274182081223\n",
      "90 val 3 799\n",
      "Phase: val. Epoch: 90. Loss: 0.08225103467702866\n",
      "90 val 4 811\n",
      "Phase: val. Epoch: 90. Loss: 0.06343202292919159\n",
      "90 val 5 823\n",
      "Phase: val. Epoch: 90. Loss: 0.06700875610113144\n",
      "90 val 6 835\n",
      "Phase: val. Epoch: 90. Loss: 0.10278766602277756\n",
      "90 val 7 847\n",
      "Phase: val. Epoch: 90. Loss: 0.07429549843072891\n",
      "90 val 8 859\n",
      "Phase: val. Epoch: 90. Loss: 0.06847026199102402\n",
      "90 val 9 871\n",
      "Phase: val. Epoch: 90. Loss: 0.07194250822067261\n",
      "90 val 10 883\n",
      "Phase: val. Epoch: 90. Loss: 0.10479384660720825\n",
      "90 val 11 884\n",
      "Phase: val. Epoch: 90. Loss: 0.024555522948503494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 train 0 12\n",
      "Phase: train. Epoch: 91. Loss: 0.07706849277019501\n",
      "91 train 1 24\n",
      "Phase: train. Epoch: 91. Loss: 0.07965859025716782\n",
      "91 train 2 36\n",
      "Phase: train. Epoch: 91. Loss: 0.07979804277420044\n",
      "91 train 3 48\n",
      "Phase: train. Epoch: 91. Loss: 0.09172069281339645\n",
      "91 train 4 60\n",
      "Phase: train. Epoch: 91. Loss: 0.07247081398963928\n",
      "91 train 5 72\n",
      "Phase: train. Epoch: 91. Loss: 0.05460193008184433\n",
      "91 train 6 84\n",
      "Phase: train. Epoch: 91. Loss: 0.09687343239784241\n",
      "91 train 7 96\n",
      "Phase: train. Epoch: 91. Loss: 0.07730414718389511\n",
      "91 train 8 108\n",
      "Phase: train. Epoch: 91. Loss: 0.08095308393239975\n",
      "91 train 9 120\n",
      "Phase: train. Epoch: 91. Loss: 0.10854452848434448\n",
      "91 train 10 132\n",
      "Phase: train. Epoch: 91. Loss: 0.10080644488334656\n",
      "91 train 11 144\n",
      "Phase: train. Epoch: 91. Loss: 0.07929745316505432\n",
      "91 train 12 156\n",
      "Phase: train. Epoch: 91. Loss: 0.07540594041347504\n",
      "91 train 13 168\n",
      "Phase: train. Epoch: 91. Loss: 0.056842368096113205\n",
      "91 train 14 180\n",
      "Phase: train. Epoch: 91. Loss: 0.09589791297912598\n",
      "91 train 15 192\n",
      "Phase: train. Epoch: 91. Loss: 0.07427399605512619\n",
      "91 train 16 204\n",
      "Phase: train. Epoch: 91. Loss: 0.10030420869588852\n",
      "91 train 17 216\n",
      "Phase: train. Epoch: 91. Loss: 0.10667383670806885\n",
      "91 train 18 228\n",
      "Phase: train. Epoch: 91. Loss: 0.08166705816984177\n",
      "91 train 19 240\n",
      "Phase: train. Epoch: 91. Loss: 0.09027337282896042\n",
      "91 train 20 252\n",
      "Phase: train. Epoch: 91. Loss: 0.09222280979156494\n",
      "91 train 21 264\n",
      "Phase: train. Epoch: 91. Loss: 0.08286990225315094\n",
      "91 train 22 276\n",
      "Phase: train. Epoch: 91. Loss: 0.11477100849151611\n",
      "91 train 23 288\n",
      "Phase: train. Epoch: 91. Loss: 0.07598098367452621\n",
      "91 train 24 300\n",
      "Phase: train. Epoch: 91. Loss: 0.07521866261959076\n",
      "91 train 25 312\n",
      "Phase: train. Epoch: 91. Loss: 0.08600004762411118\n",
      "91 train 26 324\n",
      "Phase: train. Epoch: 91. Loss: 0.09151910245418549\n",
      "91 train 27 336\n",
      "Phase: train. Epoch: 91. Loss: 0.06887095421552658\n",
      "91 train 28 348\n",
      "Phase: train. Epoch: 91. Loss: 0.07888119667768478\n",
      "91 train 29 360\n",
      "Phase: train. Epoch: 91. Loss: 0.08151530474424362\n",
      "91 train 30 372\n",
      "Phase: train. Epoch: 91. Loss: 0.08943041414022446\n",
      "91 train 31 384\n",
      "Phase: train. Epoch: 91. Loss: 0.08739378303289413\n",
      "91 train 32 396\n",
      "Phase: train. Epoch: 91. Loss: 0.08081172406673431\n",
      "91 train 33 408\n",
      "Phase: train. Epoch: 91. Loss: 0.08274281024932861\n",
      "91 train 34 420\n",
      "Phase: train. Epoch: 91. Loss: 0.10049586743116379\n",
      "91 train 35 432\n",
      "Phase: train. Epoch: 91. Loss: 0.08296354115009308\n",
      "91 train 36 444\n",
      "Phase: train. Epoch: 91. Loss: 0.08576017618179321\n",
      "91 train 37 456\n",
      "Phase: train. Epoch: 91. Loss: 0.09154155105352402\n",
      "91 train 38 468\n",
      "Phase: train. Epoch: 91. Loss: 0.07022704184055328\n",
      "91 train 39 480\n",
      "Phase: train. Epoch: 91. Loss: 0.09360352158546448\n",
      "91 train 40 492\n",
      "Phase: train. Epoch: 91. Loss: 0.0737476572394371\n",
      "91 train 41 504\n",
      "Phase: train. Epoch: 91. Loss: 0.06950600445270538\n",
      "91 train 42 516\n",
      "Phase: train. Epoch: 91. Loss: 0.07946532964706421\n",
      "91 train 43 528\n",
      "Phase: train. Epoch: 91. Loss: 0.10166376829147339\n",
      "91 train 44 540\n",
      "Phase: train. Epoch: 91. Loss: 0.11352322995662689\n",
      "91 train 45 552\n",
      "Phase: train. Epoch: 91. Loss: 0.06980542838573456\n",
      "91 train 46 564\n",
      "Phase: train. Epoch: 91. Loss: 0.0725463479757309\n",
      "91 train 47 576\n",
      "Phase: train. Epoch: 91. Loss: 0.09397894144058228\n",
      "91 train 48 588\n",
      "Phase: train. Epoch: 91. Loss: 0.08677040040493011\n",
      "91 train 49 600\n",
      "Phase: train. Epoch: 91. Loss: 0.08103227615356445\n",
      "91 train 50 612\n",
      "Phase: train. Epoch: 91. Loss: 0.08071761578321457\n",
      "91 train 51 624\n",
      "Phase: train. Epoch: 91. Loss: 0.08805423229932785\n",
      "91 train 52 636\n",
      "Phase: train. Epoch: 91. Loss: 0.08125223219394684\n",
      "91 train 53 648\n",
      "Phase: train. Epoch: 91. Loss: 0.07895004004240036\n",
      "91 train 54 660\n",
      "Phase: train. Epoch: 91. Loss: 0.0850561261177063\n",
      "91 train 55 672\n",
      "Phase: train. Epoch: 91. Loss: 0.06424872577190399\n",
      "91 train 56 684\n",
      "Phase: train. Epoch: 91. Loss: 0.08452874422073364\n",
      "91 train 57 696\n",
      "Phase: train. Epoch: 91. Loss: 0.09481070935726166\n",
      "91 train 58 708\n",
      "Phase: train. Epoch: 91. Loss: 0.09510090202093124\n",
      "91 train 59 720\n",
      "Phase: train. Epoch: 91. Loss: 0.09190116822719574\n",
      "91 train 60 732\n",
      "Phase: train. Epoch: 91. Loss: 0.07092040777206421\n",
      "91 train 61 744\n",
      "Phase: train. Epoch: 91. Loss: 0.08164050430059433\n",
      "91 train 62 751\n",
      "Phase: train. Epoch: 91. Loss: 0.07249893993139267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 val 0 763\n",
      "Phase: val. Epoch: 91. Loss: 0.05994432419538498\n",
      "91 val 1 775\n",
      "Phase: val. Epoch: 91. Loss: 0.08790569007396698\n",
      "91 val 2 787\n",
      "Phase: val. Epoch: 91. Loss: 0.07177136838436127\n",
      "91 val 3 799\n",
      "Phase: val. Epoch: 91. Loss: 0.08828191459178925\n",
      "91 val 4 811\n",
      "Phase: val. Epoch: 91. Loss: 0.09293434023857117\n",
      "91 val 5 823\n",
      "Phase: val. Epoch: 91. Loss: 0.05496000498533249\n",
      "91 val 6 835\n",
      "Phase: val. Epoch: 91. Loss: 0.09536817669868469\n",
      "91 val 7 847\n",
      "Phase: val. Epoch: 91. Loss: 0.08511557430028915\n",
      "91 val 8 859\n",
      "Phase: val. Epoch: 91. Loss: 0.07375001907348633\n",
      "91 val 9 871\n",
      "Phase: val. Epoch: 91. Loss: 0.0811600610613823\n",
      "91 val 10 883\n",
      "Phase: val. Epoch: 91. Loss: 0.09238307178020477\n",
      "91 val 11 884\n",
      "Phase: val. Epoch: 91. Loss: 0.02485240064561367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 train 0 12\n",
      "Phase: train. Epoch: 92. Loss: 0.0907697081565857\n",
      "92 train 1 24\n",
      "Phase: train. Epoch: 92. Loss: 0.10242666304111481\n",
      "92 train 2 36\n",
      "Phase: train. Epoch: 92. Loss: 0.0899096131324768\n",
      "92 train 3 48\n",
      "Phase: train. Epoch: 92. Loss: 0.06374789774417877\n",
      "92 train 4 60\n",
      "Phase: train. Epoch: 92. Loss: 0.05877895653247833\n",
      "92 train 5 72\n",
      "Phase: train. Epoch: 92. Loss: 0.087002694606781\n",
      "92 train 6 84\n",
      "Phase: train. Epoch: 92. Loss: 0.08750590682029724\n",
      "92 train 7 96\n",
      "Phase: train. Epoch: 92. Loss: 0.06713268160820007\n",
      "92 train 8 108\n",
      "Phase: train. Epoch: 92. Loss: 0.12953126430511475\n",
      "92 train 9 120\n",
      "Phase: train. Epoch: 92. Loss: 0.06437358260154724\n",
      "92 train 10 132\n",
      "Phase: train. Epoch: 92. Loss: 0.08652015775442123\n",
      "92 train 11 144\n",
      "Phase: train. Epoch: 92. Loss: 0.08182813972234726\n",
      "92 train 12 156\n",
      "Phase: train. Epoch: 92. Loss: 0.08171804994344711\n",
      "92 train 13 168\n",
      "Phase: train. Epoch: 92. Loss: 0.11364440619945526\n",
      "92 train 14 180\n",
      "Phase: train. Epoch: 92. Loss: 0.08432964980602264\n",
      "92 train 15 192\n",
      "Phase: train. Epoch: 92. Loss: 0.05714751034975052\n",
      "92 train 16 204\n",
      "Phase: train. Epoch: 92. Loss: 0.06195467337965965\n",
      "92 train 17 216\n",
      "Phase: train. Epoch: 92. Loss: 0.07636375725269318\n",
      "92 train 18 228\n",
      "Phase: train. Epoch: 92. Loss: 0.07557395100593567\n",
      "92 train 19 240\n",
      "Phase: train. Epoch: 92. Loss: 0.097992442548275\n",
      "92 train 20 252\n",
      "Phase: train. Epoch: 92. Loss: 0.07084889709949493\n",
      "92 train 21 264\n",
      "Phase: train. Epoch: 92. Loss: 0.08077776432037354\n",
      "92 train 22 276\n",
      "Phase: train. Epoch: 92. Loss: 0.06542234867811203\n",
      "92 train 23 288\n",
      "Phase: train. Epoch: 92. Loss: 0.057666581124067307\n",
      "92 train 24 300\n",
      "Phase: train. Epoch: 92. Loss: 0.06880375742912292\n",
      "92 train 25 312\n",
      "Phase: train. Epoch: 92. Loss: 0.0752582773566246\n",
      "92 train 26 324\n",
      "Phase: train. Epoch: 92. Loss: 0.09486909955739975\n",
      "92 train 27 336\n",
      "Phase: train. Epoch: 92. Loss: 0.08439677953720093\n",
      "92 train 28 348\n",
      "Phase: train. Epoch: 92. Loss: 0.10700307786464691\n",
      "92 train 29 360\n",
      "Phase: train. Epoch: 92. Loss: 0.1235431432723999\n",
      "92 train 30 372\n",
      "Phase: train. Epoch: 92. Loss: 0.07156737148761749\n",
      "92 train 31 384\n",
      "Phase: train. Epoch: 92. Loss: 0.08513610064983368\n",
      "92 train 32 396\n",
      "Phase: train. Epoch: 92. Loss: 0.1108393669128418\n",
      "92 train 33 408\n",
      "Phase: train. Epoch: 92. Loss: 0.07549898326396942\n",
      "92 train 34 420\n",
      "Phase: train. Epoch: 92. Loss: 0.10913386195898056\n",
      "92 train 35 432\n",
      "Phase: train. Epoch: 92. Loss: 0.08765824884176254\n",
      "92 train 36 444\n",
      "Phase: train. Epoch: 92. Loss: 0.07120655477046967\n",
      "92 train 37 456\n",
      "Phase: train. Epoch: 92. Loss: 0.07748570293188095\n",
      "92 train 38 468\n",
      "Phase: train. Epoch: 92. Loss: 0.08418349921703339\n",
      "92 train 39 480\n",
      "Phase: train. Epoch: 92. Loss: 0.10619314014911652\n",
      "92 train 40 492\n",
      "Phase: train. Epoch: 92. Loss: 0.07052453607320786\n",
      "92 train 41 504\n",
      "Phase: train. Epoch: 92. Loss: 0.07113407552242279\n",
      "92 train 42 516\n",
      "Phase: train. Epoch: 92. Loss: 0.10483662784099579\n",
      "92 train 43 528\n",
      "Phase: train. Epoch: 92. Loss: 0.06802747398614883\n",
      "92 train 44 540\n",
      "Phase: train. Epoch: 92. Loss: 0.08904872834682465\n",
      "92 train 45 552\n",
      "Phase: train. Epoch: 92. Loss: 0.07309970259666443\n",
      "92 train 46 564\n",
      "Phase: train. Epoch: 92. Loss: 0.07781265676021576\n",
      "92 train 47 576\n",
      "Phase: train. Epoch: 92. Loss: 0.08221571147441864\n",
      "92 train 48 588\n",
      "Phase: train. Epoch: 92. Loss: 0.07070625573396683\n",
      "92 train 49 600\n",
      "Phase: train. Epoch: 92. Loss: 0.06419040262699127\n",
      "92 train 50 612\n",
      "Phase: train. Epoch: 92. Loss: 0.09182693064212799\n",
      "92 train 51 624\n",
      "Phase: train. Epoch: 92. Loss: 0.07052376866340637\n",
      "92 train 52 636\n",
      "Phase: train. Epoch: 92. Loss: 0.08951354026794434\n",
      "92 train 53 648\n",
      "Phase: train. Epoch: 92. Loss: 0.0794735699892044\n",
      "92 train 54 660\n",
      "Phase: train. Epoch: 92. Loss: 0.07495773583650589\n",
      "92 train 55 672\n",
      "Phase: train. Epoch: 92. Loss: 0.07658353447914124\n",
      "92 train 56 684\n",
      "Phase: train. Epoch: 92. Loss: 0.073452889919281\n",
      "92 train 57 696\n",
      "Phase: train. Epoch: 92. Loss: 0.10133616626262665\n",
      "92 train 58 708\n",
      "Phase: train. Epoch: 92. Loss: 0.08996053785085678\n",
      "92 train 59 720\n",
      "Phase: train. Epoch: 92. Loss: 0.0859537422657013\n",
      "92 train 60 732\n",
      "Phase: train. Epoch: 92. Loss: 0.06694608181715012\n",
      "92 train 61 744\n",
      "Phase: train. Epoch: 92. Loss: 0.07498236000537872\n",
      "92 train 62 751\n",
      "Phase: train. Epoch: 92. Loss: 0.0901118814945221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 val 0 763\n",
      "Phase: val. Epoch: 92. Loss: 0.05516266077756882\n",
      "92 val 1 775\n",
      "Phase: val. Epoch: 92. Loss: 0.07154792547225952\n",
      "92 val 2 787\n",
      "Phase: val. Epoch: 92. Loss: 0.08746296167373657\n",
      "92 val 3 799\n",
      "Phase: val. Epoch: 92. Loss: 0.11251088231801987\n",
      "92 val 4 811\n",
      "Phase: val. Epoch: 92. Loss: 0.11124836653470993\n",
      "92 val 5 823\n",
      "Phase: val. Epoch: 92. Loss: 0.07531517744064331\n",
      "92 val 6 835\n",
      "Phase: val. Epoch: 92. Loss: 0.1062055453658104\n",
      "92 val 7 847\n",
      "Phase: val. Epoch: 92. Loss: 0.08447691053152084\n",
      "92 val 8 859\n",
      "Phase: val. Epoch: 92. Loss: 0.06234992668032646\n",
      "92 val 9 871\n",
      "Phase: val. Epoch: 92. Loss: 0.06232831999659538\n",
      "92 val 10 883\n",
      "Phase: val. Epoch: 92. Loss: 0.06658300757408142\n",
      "92 val 11 884\n",
      "Phase: val. Epoch: 92. Loss: 0.024337653070688248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 train 0 12\n",
      "Phase: train. Epoch: 93. Loss: 0.07593537867069244\n",
      "93 train 1 24\n",
      "Phase: train. Epoch: 93. Loss: 0.09540784358978271\n",
      "93 train 2 36\n",
      "Phase: train. Epoch: 93. Loss: 0.07971425354480743\n",
      "93 train 3 48\n",
      "Phase: train. Epoch: 93. Loss: 0.07880671322345734\n",
      "93 train 4 60\n",
      "Phase: train. Epoch: 93. Loss: 0.06408332288265228\n",
      "93 train 5 72\n",
      "Phase: train. Epoch: 93. Loss: 0.0731470137834549\n",
      "93 train 6 84\n",
      "Phase: train. Epoch: 93. Loss: 0.11481853574514389\n",
      "93 train 7 96\n",
      "Phase: train. Epoch: 93. Loss: 0.0992935448884964\n",
      "93 train 8 108\n",
      "Phase: train. Epoch: 93. Loss: 0.06855648756027222\n",
      "93 train 9 120\n",
      "Phase: train. Epoch: 93. Loss: 0.0922425240278244\n",
      "93 train 10 132\n",
      "Phase: train. Epoch: 93. Loss: 0.08413319289684296\n",
      "93 train 11 144\n",
      "Phase: train. Epoch: 93. Loss: 0.08398638665676117\n",
      "93 train 12 156\n",
      "Phase: train. Epoch: 93. Loss: 0.08288854360580444\n",
      "93 train 13 168\n",
      "Phase: train. Epoch: 93. Loss: 0.08250311017036438\n",
      "93 train 14 180\n",
      "Phase: train. Epoch: 93. Loss: 0.09524363279342651\n",
      "93 train 15 192\n",
      "Phase: train. Epoch: 93. Loss: 0.07666175067424774\n",
      "93 train 16 204\n",
      "Phase: train. Epoch: 93. Loss: 0.07239298522472382\n",
      "93 train 17 216\n",
      "Phase: train. Epoch: 93. Loss: 0.08518480509519577\n",
      "93 train 18 228\n",
      "Phase: train. Epoch: 93. Loss: 0.08003415167331696\n",
      "93 train 19 240\n",
      "Phase: train. Epoch: 93. Loss: 0.08436616510152817\n",
      "93 train 20 252\n",
      "Phase: train. Epoch: 93. Loss: 0.056924816220998764\n",
      "93 train 21 264\n",
      "Phase: train. Epoch: 93. Loss: 0.08693802356719971\n",
      "93 train 22 276\n",
      "Phase: train. Epoch: 93. Loss: 0.08788901567459106\n",
      "93 train 23 288\n",
      "Phase: train. Epoch: 93. Loss: 0.06971829384565353\n",
      "93 train 24 300\n",
      "Phase: train. Epoch: 93. Loss: 0.0854877382516861\n",
      "93 train 25 312\n",
      "Phase: train. Epoch: 93. Loss: 0.07372778654098511\n",
      "93 train 26 324\n",
      "Phase: train. Epoch: 93. Loss: 0.08692606538534164\n",
      "93 train 27 336\n",
      "Phase: train. Epoch: 93. Loss: 0.09207205474376678\n",
      "93 train 28 348\n",
      "Phase: train. Epoch: 93. Loss: 0.05991451069712639\n",
      "93 train 29 360\n",
      "Phase: train. Epoch: 93. Loss: 0.07091253250837326\n",
      "93 train 30 372\n",
      "Phase: train. Epoch: 93. Loss: 0.12086423486471176\n",
      "93 train 31 384\n",
      "Phase: train. Epoch: 93. Loss: 0.06747908890247345\n",
      "93 train 32 396\n",
      "Phase: train. Epoch: 93. Loss: 0.07717011868953705\n",
      "93 train 33 408\n",
      "Phase: train. Epoch: 93. Loss: 0.08211266994476318\n",
      "93 train 34 420\n",
      "Phase: train. Epoch: 93. Loss: 0.07621535658836365\n",
      "93 train 35 432\n",
      "Phase: train. Epoch: 93. Loss: 0.08806197345256805\n",
      "93 train 36 444\n",
      "Phase: train. Epoch: 93. Loss: 0.07774510979652405\n",
      "93 train 37 456\n",
      "Phase: train. Epoch: 93. Loss: 0.09142343699932098\n",
      "93 train 38 468\n",
      "Phase: train. Epoch: 93. Loss: 0.07923402637243271\n",
      "93 train 39 480\n",
      "Phase: train. Epoch: 93. Loss: 0.06640136241912842\n",
      "93 train 40 492\n",
      "Phase: train. Epoch: 93. Loss: 0.06278300285339355\n",
      "93 train 41 504\n",
      "Phase: train. Epoch: 93. Loss: 0.06591160595417023\n",
      "93 train 42 516\n",
      "Phase: train. Epoch: 93. Loss: 0.09794556349515915\n",
      "93 train 43 528\n",
      "Phase: train. Epoch: 93. Loss: 0.08255364000797272\n",
      "93 train 44 540\n",
      "Phase: train. Epoch: 93. Loss: 0.08371584862470627\n",
      "93 train 45 552\n",
      "Phase: train. Epoch: 93. Loss: 0.08373501151800156\n",
      "93 train 46 564\n",
      "Phase: train. Epoch: 93. Loss: 0.08857621252536774\n",
      "93 train 47 576\n",
      "Phase: train. Epoch: 93. Loss: 0.08365646004676819\n",
      "93 train 48 588\n",
      "Phase: train. Epoch: 93. Loss: 0.07631538808345795\n",
      "93 train 49 600\n",
      "Phase: train. Epoch: 93. Loss: 0.0728239119052887\n",
      "93 train 50 612\n",
      "Phase: train. Epoch: 93. Loss: 0.10250474512577057\n",
      "93 train 51 624\n",
      "Phase: train. Epoch: 93. Loss: 0.11691944301128387\n",
      "93 train 52 636\n",
      "Phase: train. Epoch: 93. Loss: 0.08851679414510727\n",
      "93 train 53 648\n",
      "Phase: train. Epoch: 93. Loss: 0.08320224285125732\n",
      "93 train 54 660\n",
      "Phase: train. Epoch: 93. Loss: 0.11386235803365707\n",
      "93 train 55 672\n",
      "Phase: train. Epoch: 93. Loss: 0.08030171692371368\n",
      "93 train 56 684\n",
      "Phase: train. Epoch: 93. Loss: 0.10064816474914551\n",
      "93 train 57 696\n",
      "Phase: train. Epoch: 93. Loss: 0.07610964775085449\n",
      "93 train 58 708\n",
      "Phase: train. Epoch: 93. Loss: 0.05706029385328293\n",
      "93 train 59 720\n",
      "Phase: train. Epoch: 93. Loss: 0.07765381783246994\n",
      "93 train 60 732\n",
      "Phase: train. Epoch: 93. Loss: 0.0720471665263176\n",
      "93 train 61 744\n",
      "Phase: train. Epoch: 93. Loss: 0.10481002181768417\n",
      "93 train 62 751\n",
      "Phase: train. Epoch: 93. Loss: 0.10809066146612167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 val 0 763\n",
      "Phase: val. Epoch: 93. Loss: 0.07526379078626633\n",
      "93 val 1 775\n",
      "Phase: val. Epoch: 93. Loss: 0.08959180116653442\n",
      "93 val 2 787\n",
      "Phase: val. Epoch: 93. Loss: 0.06920330226421356\n",
      "93 val 3 799\n",
      "Phase: val. Epoch: 93. Loss: 0.08199283480644226\n",
      "93 val 4 811\n",
      "Phase: val. Epoch: 93. Loss: 0.09344448894262314\n",
      "93 val 5 823\n",
      "Phase: val. Epoch: 93. Loss: 0.05105718970298767\n",
      "93 val 6 835\n",
      "Phase: val. Epoch: 93. Loss: 0.05292656272649765\n",
      "93 val 7 847\n",
      "Phase: val. Epoch: 93. Loss: 0.08483250439167023\n",
      "93 val 8 859\n",
      "Phase: val. Epoch: 93. Loss: 0.11002536118030548\n",
      "93 val 9 871\n",
      "Phase: val. Epoch: 93. Loss: 0.10425238311290741\n",
      "93 val 10 883\n",
      "Phase: val. Epoch: 93. Loss: 0.06154904514551163\n",
      "93 val 11 884\n",
      "Phase: val. Epoch: 93. Loss: 0.14524677395820618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 train 0 12\n",
      "Phase: train. Epoch: 94. Loss: 0.1066102534532547\n",
      "94 train 1 24\n",
      "Phase: train. Epoch: 94. Loss: 0.06656869500875473\n",
      "94 train 2 36\n",
      "Phase: train. Epoch: 94. Loss: 0.06346411257982254\n",
      "94 train 3 48\n",
      "Phase: train. Epoch: 94. Loss: 0.10014496743679047\n",
      "94 train 4 60\n",
      "Phase: train. Epoch: 94. Loss: 0.08089374005794525\n",
      "94 train 5 72\n",
      "Phase: train. Epoch: 94. Loss: 0.09285207092761993\n",
      "94 train 6 84\n",
      "Phase: train. Epoch: 94. Loss: 0.07127695530653\n",
      "94 train 7 96\n",
      "Phase: train. Epoch: 94. Loss: 0.11986634135246277\n",
      "94 train 8 108\n",
      "Phase: train. Epoch: 94. Loss: 0.0866750031709671\n",
      "94 train 9 120\n",
      "Phase: train. Epoch: 94. Loss: 0.07606497406959534\n",
      "94 train 10 132\n",
      "Phase: train. Epoch: 94. Loss: 0.060612186789512634\n",
      "94 train 11 144\n",
      "Phase: train. Epoch: 94. Loss: 0.06572645902633667\n",
      "94 train 12 156\n",
      "Phase: train. Epoch: 94. Loss: 0.07670735567808151\n",
      "94 train 13 168\n",
      "Phase: train. Epoch: 94. Loss: 0.07871907949447632\n",
      "94 train 14 180\n",
      "Phase: train. Epoch: 94. Loss: 0.09902247041463852\n",
      "94 train 15 192\n",
      "Phase: train. Epoch: 94. Loss: 0.09335707873106003\n",
      "94 train 16 204\n",
      "Phase: train. Epoch: 94. Loss: 0.09587430208921432\n",
      "94 train 17 216\n",
      "Phase: train. Epoch: 94. Loss: 0.10804712772369385\n",
      "94 train 18 228\n",
      "Phase: train. Epoch: 94. Loss: 0.07124602049589157\n",
      "94 train 19 240\n",
      "Phase: train. Epoch: 94. Loss: 0.06541489064693451\n",
      "94 train 20 252\n",
      "Phase: train. Epoch: 94. Loss: 0.08016593754291534\n",
      "94 train 21 264\n",
      "Phase: train. Epoch: 94. Loss: 0.09826652705669403\n",
      "94 train 22 276\n",
      "Phase: train. Epoch: 94. Loss: 0.08648686856031418\n",
      "94 train 23 288\n",
      "Phase: train. Epoch: 94. Loss: 0.0669824406504631\n",
      "94 train 24 300\n",
      "Phase: train. Epoch: 94. Loss: 0.08655744045972824\n",
      "94 train 25 312\n",
      "Phase: train. Epoch: 94. Loss: 0.09769588708877563\n",
      "94 train 26 324\n",
      "Phase: train. Epoch: 94. Loss: 0.08743830025196075\n",
      "94 train 27 336\n",
      "Phase: train. Epoch: 94. Loss: 0.09973304718732834\n",
      "94 train 28 348\n",
      "Phase: train. Epoch: 94. Loss: 0.07100946456193924\n",
      "94 train 29 360\n",
      "Phase: train. Epoch: 94. Loss: 0.06539134681224823\n",
      "94 train 30 372\n",
      "Phase: train. Epoch: 94. Loss: 0.05665038526058197\n",
      "94 train 31 384\n",
      "Phase: train. Epoch: 94. Loss: 0.08943244814872742\n",
      "94 train 32 396\n",
      "Phase: train. Epoch: 94. Loss: 0.07605056464672089\n",
      "94 train 33 408\n",
      "Phase: train. Epoch: 94. Loss: 0.06814789772033691\n",
      "94 train 34 420\n",
      "Phase: train. Epoch: 94. Loss: 0.07730977237224579\n",
      "94 train 35 432\n",
      "Phase: train. Epoch: 94. Loss: 0.06910761445760727\n",
      "94 train 36 444\n",
      "Phase: train. Epoch: 94. Loss: 0.05481351166963577\n",
      "94 train 37 456\n",
      "Phase: train. Epoch: 94. Loss: 0.08924971520900726\n",
      "94 train 38 468\n",
      "Phase: train. Epoch: 94. Loss: 0.07102560251951218\n",
      "94 train 39 480\n",
      "Phase: train. Epoch: 94. Loss: 0.10258235782384872\n",
      "94 train 40 492\n",
      "Phase: train. Epoch: 94. Loss: 0.08279445767402649\n",
      "94 train 41 504\n",
      "Phase: train. Epoch: 94. Loss: 0.08613041043281555\n",
      "94 train 42 516\n",
      "Phase: train. Epoch: 94. Loss: 0.10238198935985565\n",
      "94 train 43 528\n",
      "Phase: train. Epoch: 94. Loss: 0.08862234652042389\n",
      "94 train 44 540\n",
      "Phase: train. Epoch: 94. Loss: 0.07688136398792267\n",
      "94 train 45 552\n",
      "Phase: train. Epoch: 94. Loss: 0.0698172077536583\n",
      "94 train 46 564\n",
      "Phase: train. Epoch: 94. Loss: 0.06543849408626556\n",
      "94 train 47 576\n",
      "Phase: train. Epoch: 94. Loss: 0.06966698914766312\n",
      "94 train 48 588\n",
      "Phase: train. Epoch: 94. Loss: 0.10579562187194824\n",
      "94 train 49 600\n",
      "Phase: train. Epoch: 94. Loss: 0.10183032602071762\n",
      "94 train 50 612\n",
      "Phase: train. Epoch: 94. Loss: 0.09325163066387177\n",
      "94 train 51 624\n",
      "Phase: train. Epoch: 94. Loss: 0.12010204792022705\n",
      "94 train 52 636\n",
      "Phase: train. Epoch: 94. Loss: 0.07510557025671005\n",
      "94 train 53 648\n",
      "Phase: train. Epoch: 94. Loss: 0.0799235850572586\n",
      "94 train 54 660\n",
      "Phase: train. Epoch: 94. Loss: 0.09603506326675415\n",
      "94 train 55 672\n",
      "Phase: train. Epoch: 94. Loss: 0.08127856254577637\n",
      "94 train 56 684\n",
      "Phase: train. Epoch: 94. Loss: 0.08139496296644211\n",
      "94 train 57 696\n",
      "Phase: train. Epoch: 94. Loss: 0.08173450827598572\n",
      "94 train 58 708\n",
      "Phase: train. Epoch: 94. Loss: 0.09709443897008896\n",
      "94 train 59 720\n",
      "Phase: train. Epoch: 94. Loss: 0.0801236554980278\n",
      "94 train 60 732\n",
      "Phase: train. Epoch: 94. Loss: 0.0728907436132431\n",
      "94 train 61 744\n",
      "Phase: train. Epoch: 94. Loss: 0.07542645931243896\n",
      "94 train 62 751\n",
      "Phase: train. Epoch: 94. Loss: 0.08404655754566193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 val 0 763\n",
      "Phase: val. Epoch: 94. Loss: 0.0688883513212204\n",
      "94 val 1 775\n",
      "Phase: val. Epoch: 94. Loss: 0.0866229236125946\n",
      "94 val 2 787\n",
      "Phase: val. Epoch: 94. Loss: 0.07314181327819824\n",
      "94 val 3 799\n",
      "Phase: val. Epoch: 94. Loss: 0.09709738194942474\n",
      "94 val 4 811\n",
      "Phase: val. Epoch: 94. Loss: 0.07001037150621414\n",
      "94 val 5 823\n",
      "Phase: val. Epoch: 94. Loss: 0.09333960711956024\n",
      "94 val 6 835\n",
      "Phase: val. Epoch: 94. Loss: 0.0773945301771164\n",
      "94 val 7 847\n",
      "Phase: val. Epoch: 94. Loss: 0.10470353066921234\n",
      "94 val 8 859\n",
      "Phase: val. Epoch: 94. Loss: 0.07360069453716278\n",
      "94 val 9 871\n",
      "Phase: val. Epoch: 94. Loss: 0.10102777183055878\n",
      "94 val 10 883\n",
      "Phase: val. Epoch: 94. Loss: 0.059941522777080536\n",
      "94 val 11 884\n",
      "Phase: val. Epoch: 94. Loss: 0.05280155688524246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 train 0 12\n",
      "Phase: train. Epoch: 95. Loss: 0.07137416303157806\n",
      "95 train 1 24\n",
      "Phase: train. Epoch: 95. Loss: 0.0797836109995842\n",
      "95 train 2 36\n",
      "Phase: train. Epoch: 95. Loss: 0.07476499676704407\n",
      "95 train 3 48\n",
      "Phase: train. Epoch: 95. Loss: 0.07140763849020004\n",
      "95 train 4 60\n",
      "Phase: train. Epoch: 95. Loss: 0.061121389269828796\n",
      "95 train 5 72\n",
      "Phase: train. Epoch: 95. Loss: 0.07541251182556152\n",
      "95 train 6 84\n",
      "Phase: train. Epoch: 95. Loss: 0.09087064862251282\n",
      "95 train 7 96\n",
      "Phase: train. Epoch: 95. Loss: 0.0813189446926117\n",
      "95 train 8 108\n",
      "Phase: train. Epoch: 95. Loss: 0.06627427786588669\n",
      "95 train 9 120\n",
      "Phase: train. Epoch: 95. Loss: 0.08861227333545685\n",
      "95 train 10 132\n",
      "Phase: train. Epoch: 95. Loss: 0.08772452175617218\n",
      "95 train 11 144\n",
      "Phase: train. Epoch: 95. Loss: 0.07594731450080872\n",
      "95 train 12 156\n",
      "Phase: train. Epoch: 95. Loss: 0.0859069675207138\n",
      "95 train 13 168\n",
      "Phase: train. Epoch: 95. Loss: 0.06941322237253189\n",
      "95 train 14 180\n",
      "Phase: train. Epoch: 95. Loss: 0.1318848729133606\n",
      "95 train 15 192\n",
      "Phase: train. Epoch: 95. Loss: 0.09200693666934967\n",
      "95 train 16 204\n",
      "Phase: train. Epoch: 95. Loss: 0.08261701464653015\n",
      "95 train 17 216\n",
      "Phase: train. Epoch: 95. Loss: 0.06254495680332184\n",
      "95 train 18 228\n",
      "Phase: train. Epoch: 95. Loss: 0.07754786312580109\n",
      "95 train 19 240\n",
      "Phase: train. Epoch: 95. Loss: 0.10101999342441559\n",
      "95 train 20 252\n",
      "Phase: train. Epoch: 95. Loss: 0.05970273166894913\n",
      "95 train 21 264\n",
      "Phase: train. Epoch: 95. Loss: 0.08977966010570526\n",
      "95 train 22 276\n",
      "Phase: train. Epoch: 95. Loss: 0.12044364213943481\n",
      "95 train 23 288\n",
      "Phase: train. Epoch: 95. Loss: 0.07497712969779968\n",
      "95 train 24 300\n",
      "Phase: train. Epoch: 95. Loss: 0.06215479597449303\n",
      "95 train 25 312\n",
      "Phase: train. Epoch: 95. Loss: 0.06911202520132065\n",
      "95 train 26 324\n",
      "Phase: train. Epoch: 95. Loss: 0.07163301110267639\n",
      "95 train 27 336\n",
      "Phase: train. Epoch: 95. Loss: 0.0746547058224678\n",
      "95 train 28 348\n",
      "Phase: train. Epoch: 95. Loss: 0.0721292570233345\n",
      "95 train 29 360\n",
      "Phase: train. Epoch: 95. Loss: 0.10488336533308029\n",
      "95 train 30 372\n",
      "Phase: train. Epoch: 95. Loss: 0.08198481798171997\n",
      "95 train 31 384\n",
      "Phase: train. Epoch: 95. Loss: 0.09503793716430664\n",
      "95 train 32 396\n",
      "Phase: train. Epoch: 95. Loss: 0.09294043481349945\n",
      "95 train 33 408\n",
      "Phase: train. Epoch: 95. Loss: 0.07317881286144257\n",
      "95 train 34 420\n",
      "Phase: train. Epoch: 95. Loss: 0.07955563068389893\n",
      "95 train 35 432\n",
      "Phase: train. Epoch: 95. Loss: 0.07680752873420715\n",
      "95 train 36 444\n",
      "Phase: train. Epoch: 95. Loss: 0.07124358415603638\n",
      "95 train 37 456\n",
      "Phase: train. Epoch: 95. Loss: 0.07304304838180542\n",
      "95 train 38 468\n",
      "Phase: train. Epoch: 95. Loss: 0.07997581362724304\n",
      "95 train 39 480\n",
      "Phase: train. Epoch: 95. Loss: 0.07716581970453262\n",
      "95 train 40 492\n",
      "Phase: train. Epoch: 95. Loss: 0.07946097105741501\n",
      "95 train 41 504\n",
      "Phase: train. Epoch: 95. Loss: 0.09895595163106918\n",
      "95 train 42 516\n",
      "Phase: train. Epoch: 95. Loss: 0.07418552041053772\n",
      "95 train 43 528\n",
      "Phase: train. Epoch: 95. Loss: 0.06761042773723602\n",
      "95 train 44 540\n",
      "Phase: train. Epoch: 95. Loss: 0.057756274938583374\n",
      "95 train 45 552\n",
      "Phase: train. Epoch: 95. Loss: 0.10319337248802185\n",
      "95 train 46 564\n",
      "Phase: train. Epoch: 95. Loss: 0.0861092060804367\n",
      "95 train 47 576\n",
      "Phase: train. Epoch: 95. Loss: 0.06548725813627243\n",
      "95 train 48 588\n",
      "Phase: train. Epoch: 95. Loss: 0.12458351254463196\n",
      "95 train 49 600\n",
      "Phase: train. Epoch: 95. Loss: 0.0787198543548584\n",
      "95 train 50 612\n",
      "Phase: train. Epoch: 95. Loss: 0.11670579761266708\n",
      "95 train 51 624\n",
      "Phase: train. Epoch: 95. Loss: 0.10373809933662415\n",
      "95 train 52 636\n",
      "Phase: train. Epoch: 95. Loss: 0.07699186354875565\n",
      "95 train 53 648\n",
      "Phase: train. Epoch: 95. Loss: 0.09379091113805771\n",
      "95 train 54 660\n",
      "Phase: train. Epoch: 95. Loss: 0.10011063516139984\n",
      "95 train 55 672\n",
      "Phase: train. Epoch: 95. Loss: 0.07155366241931915\n",
      "95 train 56 684\n",
      "Phase: train. Epoch: 95. Loss: 0.09169131517410278\n",
      "95 train 57 696\n",
      "Phase: train. Epoch: 95. Loss: 0.06931702792644501\n",
      "95 train 58 708\n",
      "Phase: train. Epoch: 95. Loss: 0.05991654843091965\n",
      "95 train 59 720\n",
      "Phase: train. Epoch: 95. Loss: 0.08581811189651489\n",
      "95 train 60 732\n",
      "Phase: train. Epoch: 95. Loss: 0.09104792773723602\n",
      "95 train 61 744\n",
      "Phase: train. Epoch: 95. Loss: 0.08791431039571762\n",
      "95 train 62 751\n",
      "Phase: train. Epoch: 95. Loss: 0.10745132714509964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 val 0 763\n",
      "Phase: val. Epoch: 95. Loss: 0.05816240236163139\n",
      "95 val 1 775\n",
      "Phase: val. Epoch: 95. Loss: 0.051062799990177155\n",
      "95 val 2 787\n",
      "Phase: val. Epoch: 95. Loss: 0.11293336749076843\n",
      "95 val 3 799\n",
      "Phase: val. Epoch: 95. Loss: 0.08304096758365631\n",
      "95 val 4 811\n",
      "Phase: val. Epoch: 95. Loss: 0.0840495154261589\n",
      "95 val 5 823\n",
      "Phase: val. Epoch: 95. Loss: 0.09957081079483032\n",
      "95 val 6 835\n",
      "Phase: val. Epoch: 95. Loss: 0.060140591114759445\n",
      "95 val 7 847\n",
      "Phase: val. Epoch: 95. Loss: 0.046267710626125336\n",
      "95 val 8 859\n",
      "Phase: val. Epoch: 95. Loss: 0.08275371789932251\n",
      "95 val 9 871\n",
      "Phase: val. Epoch: 95. Loss: 0.09422104060649872\n",
      "95 val 10 883\n",
      "Phase: val. Epoch: 95. Loss: 0.09591434895992279\n",
      "95 val 11 884\n",
      "Phase: val. Epoch: 95. Loss: 0.1457090675830841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 train 0 12\n",
      "Phase: train. Epoch: 96. Loss: 0.10208448022603989\n",
      "96 train 1 24\n",
      "Phase: train. Epoch: 96. Loss: 0.08143575489521027\n",
      "96 train 2 36\n",
      "Phase: train. Epoch: 96. Loss: 0.07781164348125458\n",
      "96 train 3 48\n",
      "Phase: train. Epoch: 96. Loss: 0.09951768815517426\n",
      "96 train 4 60\n",
      "Phase: train. Epoch: 96. Loss: 0.08503352105617523\n",
      "96 train 5 72\n",
      "Phase: train. Epoch: 96. Loss: 0.12245498597621918\n",
      "96 train 6 84\n",
      "Phase: train. Epoch: 96. Loss: 0.09633520245552063\n",
      "96 train 7 96\n",
      "Phase: train. Epoch: 96. Loss: 0.0909404456615448\n",
      "96 train 8 108\n",
      "Phase: train. Epoch: 96. Loss: 0.06906536966562271\n",
      "96 train 9 120\n",
      "Phase: train. Epoch: 96. Loss: 0.08229760825634003\n",
      "96 train 10 132\n",
      "Phase: train. Epoch: 96. Loss: 0.07989552617073059\n",
      "96 train 11 144\n",
      "Phase: train. Epoch: 96. Loss: 0.08329547941684723\n",
      "96 train 12 156\n",
      "Phase: train. Epoch: 96. Loss: 0.09168702363967896\n",
      "96 train 13 168\n",
      "Phase: train. Epoch: 96. Loss: 0.09765535593032837\n",
      "96 train 14 180\n",
      "Phase: train. Epoch: 96. Loss: 0.07879075407981873\n",
      "96 train 15 192\n",
      "Phase: train. Epoch: 96. Loss: 0.07309888303279877\n",
      "96 train 16 204\n",
      "Phase: train. Epoch: 96. Loss: 0.06413186341524124\n",
      "96 train 17 216\n",
      "Phase: train. Epoch: 96. Loss: 0.09632354974746704\n",
      "96 train 18 228\n",
      "Phase: train. Epoch: 96. Loss: 0.11024569720029831\n",
      "96 train 19 240\n",
      "Phase: train. Epoch: 96. Loss: 0.06666260212659836\n",
      "96 train 20 252\n",
      "Phase: train. Epoch: 96. Loss: 0.07727783173322678\n",
      "96 train 21 264\n",
      "Phase: train. Epoch: 96. Loss: 0.08963136374950409\n",
      "96 train 22 276\n",
      "Phase: train. Epoch: 96. Loss: 0.08703317493200302\n",
      "96 train 23 288\n",
      "Phase: train. Epoch: 96. Loss: 0.08528265357017517\n",
      "96 train 24 300\n",
      "Phase: train. Epoch: 96. Loss: 0.06430628895759583\n",
      "96 train 25 312\n",
      "Phase: train. Epoch: 96. Loss: 0.08011463284492493\n",
      "96 train 26 324\n",
      "Phase: train. Epoch: 96. Loss: 0.07345530390739441\n",
      "96 train 27 336\n",
      "Phase: train. Epoch: 96. Loss: 0.07700513303279877\n",
      "96 train 28 348\n",
      "Phase: train. Epoch: 96. Loss: 0.0951017364859581\n",
      "96 train 29 360\n",
      "Phase: train. Epoch: 96. Loss: 0.08572430908679962\n",
      "96 train 30 372\n",
      "Phase: train. Epoch: 96. Loss: 0.08982700109481812\n",
      "96 train 31 384\n",
      "Phase: train. Epoch: 96. Loss: 0.07428701221942902\n",
      "96 train 32 396\n",
      "Phase: train. Epoch: 96. Loss: 0.06723859906196594\n",
      "96 train 33 408\n",
      "Phase: train. Epoch: 96. Loss: 0.09468021243810654\n",
      "96 train 34 420\n",
      "Phase: train. Epoch: 96. Loss: 0.08269041776657104\n",
      "96 train 35 432\n",
      "Phase: train. Epoch: 96. Loss: 0.1040419191122055\n",
      "96 train 36 444\n",
      "Phase: train. Epoch: 96. Loss: 0.10146322846412659\n",
      "96 train 37 456\n",
      "Phase: train. Epoch: 96. Loss: 0.06403589993715286\n",
      "96 train 38 468\n",
      "Phase: train. Epoch: 96. Loss: 0.08064047992229462\n",
      "96 train 39 480\n",
      "Phase: train. Epoch: 96. Loss: 0.06322289258241653\n",
      "96 train 40 492\n",
      "Phase: train. Epoch: 96. Loss: 0.07565385848283768\n",
      "96 train 41 504\n",
      "Phase: train. Epoch: 96. Loss: 0.08149015158414841\n",
      "96 train 42 516\n",
      "Phase: train. Epoch: 96. Loss: 0.08802527189254761\n",
      "96 train 43 528\n",
      "Phase: train. Epoch: 96. Loss: 0.09201537072658539\n",
      "96 train 44 540\n",
      "Phase: train. Epoch: 96. Loss: 0.08083750307559967\n",
      "96 train 45 552\n",
      "Phase: train. Epoch: 96. Loss: 0.06670073419809341\n",
      "96 train 46 564\n",
      "Phase: train. Epoch: 96. Loss: 0.07502972334623337\n",
      "96 train 47 576\n",
      "Phase: train. Epoch: 96. Loss: 0.06894128024578094\n",
      "96 train 48 588\n",
      "Phase: train. Epoch: 96. Loss: 0.05972864478826523\n",
      "96 train 49 600\n",
      "Phase: train. Epoch: 96. Loss: 0.08225033432245255\n",
      "96 train 50 612\n",
      "Phase: train. Epoch: 96. Loss: 0.08547493070363998\n",
      "96 train 51 624\n",
      "Phase: train. Epoch: 96. Loss: 0.07696513831615448\n",
      "96 train 52 636\n",
      "Phase: train. Epoch: 96. Loss: 0.06184714660048485\n",
      "96 train 53 648\n",
      "Phase: train. Epoch: 96. Loss: 0.08857405185699463\n",
      "96 train 54 660\n",
      "Phase: train. Epoch: 96. Loss: 0.07266309857368469\n",
      "96 train 55 672\n",
      "Phase: train. Epoch: 96. Loss: 0.08630664646625519\n",
      "96 train 56 684\n",
      "Phase: train. Epoch: 96. Loss: 0.09086444228887558\n",
      "96 train 57 696\n",
      "Phase: train. Epoch: 96. Loss: 0.09074317663908005\n",
      "96 train 58 708\n",
      "Phase: train. Epoch: 96. Loss: 0.06747465580701828\n",
      "96 train 59 720\n",
      "Phase: train. Epoch: 96. Loss: 0.1023310050368309\n",
      "96 train 60 732\n",
      "Phase: train. Epoch: 96. Loss: 0.09471338242292404\n",
      "96 train 61 744\n",
      "Phase: train. Epoch: 96. Loss: 0.07165273278951645\n",
      "96 train 62 751\n",
      "Phase: train. Epoch: 96. Loss: 0.06746367365121841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 val 0 763\n",
      "Phase: val. Epoch: 96. Loss: 0.06513088196516037\n",
      "96 val 1 775\n",
      "Phase: val. Epoch: 96. Loss: 0.14293716847896576\n",
      "96 val 2 787\n",
      "Phase: val. Epoch: 96. Loss: 0.09160514920949936\n",
      "96 val 3 799\n",
      "Phase: val. Epoch: 96. Loss: 0.07234646379947662\n",
      "96 val 4 811\n",
      "Phase: val. Epoch: 96. Loss: 0.08684635162353516\n",
      "96 val 5 823\n",
      "Phase: val. Epoch: 96. Loss: 0.10204732418060303\n",
      "96 val 6 835\n",
      "Phase: val. Epoch: 96. Loss: 0.08713077008724213\n",
      "96 val 7 847\n",
      "Phase: val. Epoch: 96. Loss: 0.07303363084793091\n",
      "96 val 8 859\n",
      "Phase: val. Epoch: 96. Loss: 0.10295949876308441\n",
      "96 val 9 871\n",
      "Phase: val. Epoch: 96. Loss: 0.07116517424583435\n",
      "96 val 10 883\n",
      "Phase: val. Epoch: 96. Loss: 0.08910040557384491\n",
      "96 val 11 884\n",
      "Phase: val. Epoch: 96. Loss: 0.06690254807472229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 train 0 12\n",
      "Phase: train. Epoch: 97. Loss: 0.0802825391292572\n",
      "97 train 1 24\n",
      "Phase: train. Epoch: 97. Loss: 0.10533379018306732\n",
      "97 train 2 36\n",
      "Phase: train. Epoch: 97. Loss: 0.08405383676290512\n",
      "97 train 3 48\n",
      "Phase: train. Epoch: 97. Loss: 0.0844126045703888\n",
      "97 train 4 60\n",
      "Phase: train. Epoch: 97. Loss: 0.07373006641864777\n",
      "97 train 5 72\n",
      "Phase: train. Epoch: 97. Loss: 0.05630464851856232\n",
      "97 train 6 84\n",
      "Phase: train. Epoch: 97. Loss: 0.0850692018866539\n",
      "97 train 7 96\n",
      "Phase: train. Epoch: 97. Loss: 0.0872027575969696\n",
      "97 train 8 108\n",
      "Phase: train. Epoch: 97. Loss: 0.12145708501338959\n",
      "97 train 9 120\n",
      "Phase: train. Epoch: 97. Loss: 0.08387722820043564\n",
      "97 train 10 132\n",
      "Phase: train. Epoch: 97. Loss: 0.08592968434095383\n",
      "97 train 11 144\n",
      "Phase: train. Epoch: 97. Loss: 0.07447760552167892\n",
      "97 train 12 156\n",
      "Phase: train. Epoch: 97. Loss: 0.07816062867641449\n",
      "97 train 13 168\n",
      "Phase: train. Epoch: 97. Loss: 0.09199129045009613\n",
      "97 train 14 180\n",
      "Phase: train. Epoch: 97. Loss: 0.06969746947288513\n",
      "97 train 15 192\n",
      "Phase: train. Epoch: 97. Loss: 0.06434129923582077\n",
      "97 train 16 204\n",
      "Phase: train. Epoch: 97. Loss: 0.08659323304891586\n",
      "97 train 17 216\n",
      "Phase: train. Epoch: 97. Loss: 0.08375184237957001\n",
      "97 train 18 228\n",
      "Phase: train. Epoch: 97. Loss: 0.06910192966461182\n",
      "97 train 19 240\n",
      "Phase: train. Epoch: 97. Loss: 0.13950318098068237\n",
      "97 train 20 252\n",
      "Phase: train. Epoch: 97. Loss: 0.07466211169958115\n",
      "97 train 21 264\n",
      "Phase: train. Epoch: 97. Loss: 0.12917082011699677\n",
      "97 train 22 276\n",
      "Phase: train. Epoch: 97. Loss: 0.08086979389190674\n",
      "97 train 23 288\n",
      "Phase: train. Epoch: 97. Loss: 0.10763482749462128\n",
      "97 train 24 300\n",
      "Phase: train. Epoch: 97. Loss: 0.07927256077528\n",
      "97 train 25 312\n",
      "Phase: train. Epoch: 97. Loss: 0.07688768953084946\n",
      "97 train 26 324\n",
      "Phase: train. Epoch: 97. Loss: 0.07151627540588379\n",
      "97 train 27 336\n",
      "Phase: train. Epoch: 97. Loss: 0.0712009146809578\n",
      "97 train 28 348\n",
      "Phase: train. Epoch: 97. Loss: 0.08554074168205261\n",
      "97 train 29 360\n",
      "Phase: train. Epoch: 97. Loss: 0.06750811636447906\n",
      "97 train 30 372\n",
      "Phase: train. Epoch: 97. Loss: 0.08901878446340561\n",
      "97 train 31 384\n",
      "Phase: train. Epoch: 97. Loss: 0.11514862626791\n",
      "97 train 32 396\n",
      "Phase: train. Epoch: 97. Loss: 0.11147898435592651\n",
      "97 train 33 408\n",
      "Phase: train. Epoch: 97. Loss: 0.0808294415473938\n",
      "97 train 34 420\n",
      "Phase: train. Epoch: 97. Loss: 0.0848112627863884\n",
      "97 train 35 432\n",
      "Phase: train. Epoch: 97. Loss: 0.07783316820859909\n",
      "97 train 36 444\n",
      "Phase: train. Epoch: 97. Loss: 0.08850656449794769\n",
      "97 train 37 456\n",
      "Phase: train. Epoch: 97. Loss: 0.08898144960403442\n",
      "97 train 38 468\n",
      "Phase: train. Epoch: 97. Loss: 0.08430512249469757\n",
      "97 train 39 480\n",
      "Phase: train. Epoch: 97. Loss: 0.07835748046636581\n",
      "97 train 40 492\n",
      "Phase: train. Epoch: 97. Loss: 0.06266623735427856\n",
      "97 train 41 504\n",
      "Phase: train. Epoch: 97. Loss: 0.09650268405675888\n",
      "97 train 42 516\n",
      "Phase: train. Epoch: 97. Loss: 0.08771701902151108\n",
      "97 train 43 528\n",
      "Phase: train. Epoch: 97. Loss: 0.06572586297988892\n",
      "97 train 44 540\n",
      "Phase: train. Epoch: 97. Loss: 0.07879340648651123\n",
      "97 train 45 552\n",
      "Phase: train. Epoch: 97. Loss: 0.07771356403827667\n",
      "97 train 46 564\n",
      "Phase: train. Epoch: 97. Loss: 0.07121050357818604\n",
      "97 train 47 576\n",
      "Phase: train. Epoch: 97. Loss: 0.08000548183917999\n",
      "97 train 48 588\n",
      "Phase: train. Epoch: 97. Loss: 0.07708118855953217\n",
      "97 train 49 600\n",
      "Phase: train. Epoch: 97. Loss: 0.06286101043224335\n",
      "97 train 50 612\n",
      "Phase: train. Epoch: 97. Loss: 0.0720272809267044\n",
      "97 train 51 624\n",
      "Phase: train. Epoch: 97. Loss: 0.08137762546539307\n",
      "97 train 52 636\n",
      "Phase: train. Epoch: 97. Loss: 0.0944007858633995\n",
      "97 train 53 648\n",
      "Phase: train. Epoch: 97. Loss: 0.07680143415927887\n",
      "97 train 54 660\n",
      "Phase: train. Epoch: 97. Loss: 0.08312481641769409\n",
      "97 train 55 672\n",
      "Phase: train. Epoch: 97. Loss: 0.07919378578662872\n",
      "97 train 56 684\n",
      "Phase: train. Epoch: 97. Loss: 0.0879478007555008\n",
      "97 train 57 696\n",
      "Phase: train. Epoch: 97. Loss: 0.07110045850276947\n",
      "97 train 58 708\n",
      "Phase: train. Epoch: 97. Loss: 0.0736197680234909\n",
      "97 train 59 720\n",
      "Phase: train. Epoch: 97. Loss: 0.07582759112119675\n",
      "97 train 60 732\n",
      "Phase: train. Epoch: 97. Loss: 0.06771095097064972\n",
      "97 train 61 744\n",
      "Phase: train. Epoch: 97. Loss: 0.05752825736999512\n",
      "97 train 62 751\n",
      "Phase: train. Epoch: 97. Loss: 0.07649573683738708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 val 0 763\n",
      "Phase: val. Epoch: 97. Loss: 0.10163068026304245\n",
      "97 val 1 775\n",
      "Phase: val. Epoch: 97. Loss: 0.07729636132717133\n",
      "97 val 2 787\n",
      "Phase: val. Epoch: 97. Loss: 0.09892617911100388\n",
      "97 val 3 799\n",
      "Phase: val. Epoch: 97. Loss: 0.09101881086826324\n",
      "97 val 4 811\n",
      "Phase: val. Epoch: 97. Loss: 0.05469675362110138\n",
      "97 val 5 823\n",
      "Phase: val. Epoch: 97. Loss: 0.16427603363990784\n",
      "97 val 6 835\n",
      "Phase: val. Epoch: 97. Loss: 0.0810326561331749\n",
      "97 val 7 847\n",
      "Phase: val. Epoch: 97. Loss: 0.08995991945266724\n",
      "97 val 8 859\n",
      "Phase: val. Epoch: 97. Loss: 0.07983792573213577\n",
      "97 val 9 871\n",
      "Phase: val. Epoch: 97. Loss: 0.06677847355604172\n",
      "97 val 10 883\n",
      "Phase: val. Epoch: 97. Loss: 0.08187093585729599\n",
      "97 val 11 884\n",
      "Phase: val. Epoch: 97. Loss: 0.03800337761640549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 train 0 12\n",
      "Phase: train. Epoch: 98. Loss: 0.06976964324712753\n",
      "98 train 1 24\n",
      "Phase: train. Epoch: 98. Loss: 0.07407788932323456\n",
      "98 train 2 36\n",
      "Phase: train. Epoch: 98. Loss: 0.08084973692893982\n",
      "98 train 3 48\n",
      "Phase: train. Epoch: 98. Loss: 0.06906650960445404\n",
      "98 train 4 60\n",
      "Phase: train. Epoch: 98. Loss: 0.08052307367324829\n",
      "98 train 5 72\n",
      "Phase: train. Epoch: 98. Loss: 0.09776296466588974\n",
      "98 train 6 84\n",
      "Phase: train. Epoch: 98. Loss: 0.08390974253416061\n",
      "98 train 7 96\n",
      "Phase: train. Epoch: 98. Loss: 0.08251781761646271\n",
      "98 train 8 108\n",
      "Phase: train. Epoch: 98. Loss: 0.10687427222728729\n",
      "98 train 9 120\n",
      "Phase: train. Epoch: 98. Loss: 0.06291220337152481\n",
      "98 train 10 132\n",
      "Phase: train. Epoch: 98. Loss: 0.09924664348363876\n",
      "98 train 11 144\n",
      "Phase: train. Epoch: 98. Loss: 0.07468675822019577\n",
      "98 train 12 156\n",
      "Phase: train. Epoch: 98. Loss: 0.06632915139198303\n",
      "98 train 13 168\n",
      "Phase: train. Epoch: 98. Loss: 0.09977898001670837\n",
      "98 train 14 180\n",
      "Phase: train. Epoch: 98. Loss: 0.062082260847091675\n",
      "98 train 15 192\n",
      "Phase: train. Epoch: 98. Loss: 0.08652494847774506\n",
      "98 train 16 204\n",
      "Phase: train. Epoch: 98. Loss: 0.0618380606174469\n",
      "98 train 17 216\n",
      "Phase: train. Epoch: 98. Loss: 0.0899166390299797\n",
      "98 train 18 228\n",
      "Phase: train. Epoch: 98. Loss: 0.07210971415042877\n",
      "98 train 19 240\n",
      "Phase: train. Epoch: 98. Loss: 0.09209325164556503\n",
      "98 train 20 252\n",
      "Phase: train. Epoch: 98. Loss: 0.12092312425374985\n",
      "98 train 21 264\n",
      "Phase: train. Epoch: 98. Loss: 0.06559926271438599\n",
      "98 train 22 276\n",
      "Phase: train. Epoch: 98. Loss: 0.081067755818367\n",
      "98 train 23 288\n",
      "Phase: train. Epoch: 98. Loss: 0.06410959362983704\n",
      "98 train 24 300\n",
      "Phase: train. Epoch: 98. Loss: 0.06688608229160309\n",
      "98 train 25 312\n",
      "Phase: train. Epoch: 98. Loss: 0.09360264241695404\n",
      "98 train 26 324\n",
      "Phase: train. Epoch: 98. Loss: 0.077634297311306\n",
      "98 train 27 336\n",
      "Phase: train. Epoch: 98. Loss: 0.1044902428984642\n",
      "98 train 28 348\n",
      "Phase: train. Epoch: 98. Loss: 0.07507428526878357\n",
      "98 train 29 360\n",
      "Phase: train. Epoch: 98. Loss: 0.10031616687774658\n",
      "98 train 30 372\n",
      "Phase: train. Epoch: 98. Loss: 0.085091233253479\n",
      "98 train 31 384\n",
      "Phase: train. Epoch: 98. Loss: 0.059123825281858444\n",
      "98 train 32 396\n",
      "Phase: train. Epoch: 98. Loss: 0.07600593566894531\n",
      "98 train 33 408\n",
      "Phase: train. Epoch: 98. Loss: 0.08294028043746948\n",
      "98 train 34 420\n",
      "Phase: train. Epoch: 98. Loss: 0.07953032851219177\n",
      "98 train 35 432\n",
      "Phase: train. Epoch: 98. Loss: 0.08910776674747467\n",
      "98 train 36 444\n",
      "Phase: train. Epoch: 98. Loss: 0.0840100571513176\n",
      "98 train 37 456\n",
      "Phase: train. Epoch: 98. Loss: 0.0618041567504406\n",
      "98 train 38 468\n",
      "Phase: train. Epoch: 98. Loss: 0.07391469925642014\n",
      "98 train 39 480\n",
      "Phase: train. Epoch: 98. Loss: 0.0808248445391655\n",
      "98 train 40 492\n",
      "Phase: train. Epoch: 98. Loss: 0.09711601585149765\n",
      "98 train 41 504\n",
      "Phase: train. Epoch: 98. Loss: 0.06528818607330322\n",
      "98 train 42 516\n",
      "Phase: train. Epoch: 98. Loss: 0.0641896054148674\n",
      "98 train 43 528\n",
      "Phase: train. Epoch: 98. Loss: 0.0707056000828743\n",
      "98 train 44 540\n",
      "Phase: train. Epoch: 98. Loss: 0.08278223127126694\n",
      "98 train 45 552\n",
      "Phase: train. Epoch: 98. Loss: 0.07584714889526367\n",
      "98 train 46 564\n",
      "Phase: train. Epoch: 98. Loss: 0.07249384373426437\n",
      "98 train 47 576\n",
      "Phase: train. Epoch: 98. Loss: 0.08173198997974396\n",
      "98 train 48 588\n",
      "Phase: train. Epoch: 98. Loss: 0.102878138422966\n",
      "98 train 49 600\n",
      "Phase: train. Epoch: 98. Loss: 0.1324409544467926\n",
      "98 train 50 612\n",
      "Phase: train. Epoch: 98. Loss: 0.11636962741613388\n",
      "98 train 51 624\n",
      "Phase: train. Epoch: 98. Loss: 0.08542302250862122\n",
      "98 train 52 636\n",
      "Phase: train. Epoch: 98. Loss: 0.07791517674922943\n",
      "98 train 53 648\n",
      "Phase: train. Epoch: 98. Loss: 0.0885365828871727\n",
      "98 train 54 660\n",
      "Phase: train. Epoch: 98. Loss: 0.09687823057174683\n",
      "98 train 55 672\n",
      "Phase: train. Epoch: 98. Loss: 0.06366296857595444\n",
      "98 train 56 684\n",
      "Phase: train. Epoch: 98. Loss: 0.07178233563899994\n",
      "98 train 57 696\n",
      "Phase: train. Epoch: 98. Loss: 0.08230948448181152\n",
      "98 train 58 708\n",
      "Phase: train. Epoch: 98. Loss: 0.10582242906093597\n",
      "98 train 59 720\n",
      "Phase: train. Epoch: 98. Loss: 0.07706385105848312\n",
      "98 train 60 732\n",
      "Phase: train. Epoch: 98. Loss: 0.10264065861701965\n",
      "98 train 61 744\n",
      "Phase: train. Epoch: 98. Loss: 0.11282210052013397\n",
      "98 train 62 751\n",
      "Phase: train. Epoch: 98. Loss: 0.06462264060974121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 val 0 763\n",
      "Phase: val. Epoch: 98. Loss: 0.11704077571630478\n",
      "98 val 1 775\n",
      "Phase: val. Epoch: 98. Loss: 0.0911564752459526\n",
      "98 val 2 787\n",
      "Phase: val. Epoch: 98. Loss: 0.04534114524722099\n",
      "98 val 3 799\n",
      "Phase: val. Epoch: 98. Loss: 0.06076854467391968\n",
      "98 val 4 811\n",
      "Phase: val. Epoch: 98. Loss: 0.10191934555768967\n",
      "98 val 5 823\n",
      "Phase: val. Epoch: 98. Loss: 0.085594043135643\n",
      "98 val 6 835\n",
      "Phase: val. Epoch: 98. Loss: 0.10312091559171677\n",
      "98 val 7 847\n",
      "Phase: val. Epoch: 98. Loss: 0.06296311318874359\n",
      "98 val 8 859\n",
      "Phase: val. Epoch: 98. Loss: 0.13374778628349304\n",
      "98 val 9 871\n",
      "Phase: val. Epoch: 98. Loss: 0.06763660907745361\n",
      "98 val 10 883\n",
      "Phase: val. Epoch: 98. Loss: 0.05043887346982956\n",
      "98 val 11 884\n",
      "Phase: val. Epoch: 98. Loss: 0.09426135569810867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 train 0 12\n",
      "Phase: train. Epoch: 99. Loss: 0.11475691199302673\n",
      "99 train 1 24\n",
      "Phase: train. Epoch: 99. Loss: 0.0651865005493164\n",
      "99 train 2 36\n",
      "Phase: train. Epoch: 99. Loss: 0.07751940190792084\n",
      "99 train 3 48\n",
      "Phase: train. Epoch: 99. Loss: 0.07579667866230011\n",
      "99 train 4 60\n",
      "Phase: train. Epoch: 99. Loss: 0.058750223368406296\n",
      "99 train 5 72\n",
      "Phase: train. Epoch: 99. Loss: 0.10006849467754364\n",
      "99 train 6 84\n",
      "Phase: train. Epoch: 99. Loss: 0.0847376212477684\n",
      "99 train 7 96\n",
      "Phase: train. Epoch: 99. Loss: 0.07852809131145477\n",
      "99 train 8 108\n",
      "Phase: train. Epoch: 99. Loss: 0.07372386753559113\n",
      "99 train 9 120\n",
      "Phase: train. Epoch: 99. Loss: 0.08037512004375458\n",
      "99 train 10 132\n",
      "Phase: train. Epoch: 99. Loss: 0.08104504644870758\n",
      "99 train 11 144\n",
      "Phase: train. Epoch: 99. Loss: 0.09464272856712341\n",
      "99 train 12 156\n",
      "Phase: train. Epoch: 99. Loss: 0.08819364011287689\n",
      "99 train 13 168\n",
      "Phase: train. Epoch: 99. Loss: 0.0961795300245285\n",
      "99 train 14 180\n",
      "Phase: train. Epoch: 99. Loss: 0.057455163449048996\n",
      "99 train 15 192\n",
      "Phase: train. Epoch: 99. Loss: 0.0759996771812439\n",
      "99 train 16 204\n",
      "Phase: train. Epoch: 99. Loss: 0.06924990564584732\n",
      "99 train 17 216\n",
      "Phase: train. Epoch: 99. Loss: 0.09334379434585571\n",
      "99 train 18 228\n",
      "Phase: train. Epoch: 99. Loss: 0.07336176186800003\n",
      "99 train 19 240\n",
      "Phase: train. Epoch: 99. Loss: 0.08978867530822754\n",
      "99 train 20 252\n",
      "Phase: train. Epoch: 99. Loss: 0.07018990069627762\n",
      "99 train 21 264\n",
      "Phase: train. Epoch: 99. Loss: 0.08179160952568054\n",
      "99 train 22 276\n",
      "Phase: train. Epoch: 99. Loss: 0.10495849698781967\n",
      "99 train 23 288\n",
      "Phase: train. Epoch: 99. Loss: 0.0703360065817833\n",
      "99 train 24 300\n",
      "Phase: train. Epoch: 99. Loss: 0.07868822664022446\n",
      "99 train 25 312\n",
      "Phase: train. Epoch: 99. Loss: 0.08476941287517548\n",
      "99 train 26 324\n",
      "Phase: train. Epoch: 99. Loss: 0.09547840058803558\n",
      "99 train 27 336\n",
      "Phase: train. Epoch: 99. Loss: 0.10269948095083237\n",
      "99 train 28 348\n",
      "Phase: train. Epoch: 99. Loss: 0.06484802812337875\n",
      "99 train 29 360\n",
      "Phase: train. Epoch: 99. Loss: 0.07775800675153732\n",
      "99 train 30 372\n",
      "Phase: train. Epoch: 99. Loss: 0.09140291064977646\n",
      "99 train 31 384\n",
      "Phase: train. Epoch: 99. Loss: 0.08228221535682678\n",
      "99 train 32 396\n",
      "Phase: train. Epoch: 99. Loss: 0.0818643867969513\n",
      "99 train 33 408\n",
      "Phase: train. Epoch: 99. Loss: 0.06211009621620178\n",
      "99 train 34 420\n",
      "Phase: train. Epoch: 99. Loss: 0.0950218141078949\n",
      "99 train 35 432\n",
      "Phase: train. Epoch: 99. Loss: 0.09425932168960571\n",
      "99 train 36 444\n",
      "Phase: train. Epoch: 99. Loss: 0.06343895941972733\n",
      "99 train 37 456\n",
      "Phase: train. Epoch: 99. Loss: 0.07545570284128189\n",
      "99 train 38 468\n",
      "Phase: train. Epoch: 99. Loss: 0.06977018713951111\n",
      "99 train 39 480\n",
      "Phase: train. Epoch: 99. Loss: 0.07107122242450714\n",
      "99 train 40 492\n",
      "Phase: train. Epoch: 99. Loss: 0.08259495347738266\n",
      "99 train 41 504\n",
      "Phase: train. Epoch: 99. Loss: 0.08441692590713501\n",
      "99 train 42 516\n",
      "Phase: train. Epoch: 99. Loss: 0.07350319623947144\n",
      "99 train 43 528\n",
      "Phase: train. Epoch: 99. Loss: 0.10778825730085373\n",
      "99 train 44 540\n",
      "Phase: train. Epoch: 99. Loss: 0.07482107728719711\n",
      "99 train 45 552\n",
      "Phase: train. Epoch: 99. Loss: 0.0688905119895935\n",
      "99 train 46 564\n",
      "Phase: train. Epoch: 99. Loss: 0.07814020663499832\n",
      "99 train 47 576\n",
      "Phase: train. Epoch: 99. Loss: 0.10065028071403503\n",
      "99 train 48 588\n",
      "Phase: train. Epoch: 99. Loss: 0.09881582111120224\n",
      "99 train 49 600\n",
      "Phase: train. Epoch: 99. Loss: 0.053408097475767136\n",
      "99 train 50 612\n",
      "Phase: train. Epoch: 99. Loss: 0.08001117408275604\n",
      "99 train 51 624\n",
      "Phase: train. Epoch: 99. Loss: 0.06895279884338379\n",
      "99 train 52 636\n",
      "Phase: train. Epoch: 99. Loss: 0.0869598239660263\n",
      "99 train 53 648\n",
      "Phase: train. Epoch: 99. Loss: 0.06355888396501541\n",
      "99 train 54 660\n",
      "Phase: train. Epoch: 99. Loss: 0.07609207928180695\n",
      "99 train 55 672\n",
      "Phase: train. Epoch: 99. Loss: 0.07254037261009216\n",
      "99 train 56 684\n",
      "Phase: train. Epoch: 99. Loss: 0.08535969257354736\n",
      "99 train 57 696\n",
      "Phase: train. Epoch: 99. Loss: 0.10979687422513962\n",
      "99 train 58 708\n",
      "Phase: train. Epoch: 99. Loss: 0.08207142353057861\n",
      "99 train 59 720\n",
      "Phase: train. Epoch: 99. Loss: 0.0953570306301117\n",
      "99 train 60 732\n",
      "Phase: train. Epoch: 99. Loss: 0.06529612839221954\n",
      "99 train 61 744\n",
      "Phase: train. Epoch: 99. Loss: 0.08416770398616791\n",
      "99 train 62 751\n",
      "Phase: train. Epoch: 99. Loss: 0.09280877560377121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 val 0 763\n",
      "Phase: val. Epoch: 99. Loss: 0.07254338264465332\n",
      "99 val 1 775\n",
      "Phase: val. Epoch: 99. Loss: 0.07293856889009476\n",
      "99 val 2 787\n",
      "Phase: val. Epoch: 99. Loss: 0.04947638511657715\n",
      "99 val 3 799\n",
      "Phase: val. Epoch: 99. Loss: 0.08269530534744263\n",
      "99 val 4 811\n",
      "Phase: val. Epoch: 99. Loss: 0.06461445987224579\n",
      "99 val 5 823\n",
      "Phase: val. Epoch: 99. Loss: 0.08288377523422241\n",
      "99 val 6 835\n",
      "Phase: val. Epoch: 99. Loss: 0.07418348640203476\n",
      "99 val 7 847\n",
      "Phase: val. Epoch: 99. Loss: 0.1349334567785263\n",
      "99 val 8 859\n",
      "Phase: val. Epoch: 99. Loss: 0.08675138652324677\n",
      "99 val 9 871\n",
      "Phase: val. Epoch: 99. Loss: 0.07630429416894913\n",
      "99 val 10 883\n",
      "Phase: val. Epoch: 99. Loss: 0.08230116963386536\n",
      "99 val 11 884\n",
      "Phase: val. Epoch: 99. Loss: 0.08593384921550751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Training Cell \n",
    "\n",
    "brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, model_state = train_validate(train_dataset, valid_dataset,lr)\n",
    "    \n",
    "date = datetime.now()\n",
    "pkl_name = \"results_TBI_model-End-\" + str(date.date()) + '-' + str(date.hour) + '.pkl'\n",
    "\n",
    "torch.save(model_state, os.path.join(os.getcwd(), \"Registered_Brains_FA/models_saved\", \"TBI_model-End-\" + str(date.date()) + '-' + str(date.hour) +\".pt\"))\n",
    "    \n",
    "# Saving the objects:\n",
    "with open(pkl_name, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.05 0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65\n",
      " 0.7  0.75 0.8  0.85 0.9  0.95 1.  ]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(0,1.05,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_TBI_model-End-2020-10-01-16.pkl\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.26it/s]\n",
      "<ipython-input-194-b5f90c2ede54>:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  FPR = CM_values[1] / (CM_values[1] + CM_values[3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15000000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:28,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:28,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:28,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:28,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9500000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.35000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-194-b5f90c2ede54>:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR = CM_values[0] / (CM_values[0] + CM_values[2])\n"
     ]
    }
   ],
   "source": [
    "#Only run this when the model has gone through training. \n",
    "\n",
    "del model #This removes any confliction with an existing model running on the GPU. \n",
    "model = smp.Unet(encoder_name = ENCODER, in_channels=1, classes = 1, aux_params = aux_params)\n",
    "    \n",
    "#read pickle\n",
    "if 'pkl_name' not in locals():\n",
    "    pkl_name = \"results_TBI_model-End-2020-10-01-16.pkl\"\n",
    "print(pkl_name)\n",
    "\n",
    "with open(pkl_name,'rb') as f:  \n",
    "    brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, test_dataset = pickle.load(f)\n",
    "    \n",
    "modelPath = \"/home/mccrinbc/Registered_Brains_FA/models_saved/TBI_model-End-2020-10-01-16.pt\"\n",
    "#modelPath = \"/Users/brianmccrindle/Documents/Research/TBIFinder_Final/Registered_Brains_FA/models_saved/TBI_model-epoch2-2020-08-27-9-55.pt\"\n",
    "    \n",
    "thresholds = np.arange(0,1.05,0.05) #skipping every other element, [[0.   0.05 0.1 ... 1]\n",
    "#thresholds = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "TPR_list = [] #This is also known as RECALL. \n",
    "FPR_list = []\n",
    "IoUs = []\n",
    "Dice = []\n",
    "total_error = []\n",
    "precision = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(threshold)\n",
    "    #test the model to capture performance. Reported in the Confusion Matrix values\n",
    "    CM_values = testModel(test_dataset, modelPath, threshold) #tp, fp, fn, tn, [mean_IoUs]\n",
    "    \n",
    "    TPR = CM_values[0] / (CM_values[0] + CM_values[2])\n",
    "    FPR = CM_values[1] / (CM_values[1] + CM_values[3])\n",
    "    TPR_list.append(TPR)\n",
    "    FPR_list.append(FPR)\n",
    "    \n",
    "    #IoUs, Dice, Total Error, and Precision. \n",
    "    IoUs.append(CM_values[0] / (CM_values[0] + CM_values[1] + CM_values[2]))  #IoU = TP / (TP + FP + FN)\n",
    "    Dice.append(2 * CM_values[0] / (2 * CM_values[0] + CM_values[1] + CM_values[2])) #Dice = 2TP / (2TP + FP + FN)\n",
    "    total_error.append(CM_values[1] + CM_values[2]) #Error = FP + FN. Weighted equally for now. \n",
    "    precision.append(CM_values[0] / (CM_values[0] + CM_values[1]))\n",
    "\n",
    "#Within the same cell. Save the information from testing. \n",
    "difference_array = np.array(TPR_list) - (1-np.array(FPR_list))\n",
    "best_acc_thresh = thresholds[abs(difference_array).argmin()] #Thresholds is already defined. \n",
    "best_IoU_thresh = thresholds[np.where(meanIoUs_each_threshold == np.max(meanIoUs_each_threshold))[0][0]]\n",
    "print(best_acc_thresh)\n",
    "print(best_IoU_thresh)\n",
    "\n",
    "#This is saving the test results into a pkl file\n",
    "with open('ROC_AUC_results_better_metrics.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([TPR_list, FPR_list, precision, thresholds, best_acc_thresh, IoUs, Dice], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is saving the test results into a pkl file\n",
    "with open('ROC_AUC_results_better_metrics.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([TPR_list, FPR_list, precision, thresholds, best_acc_thresh, IoUs, Dice], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021287148685778603,\n",
       " 0.1445629197332417,\n",
       " 0.20308967241611797,\n",
       " 0.2435086696017613,\n",
       " 0.28178815862868883,\n",
       " 0.3129934430616888,\n",
       " 0.3407922316954836,\n",
       " 0.356328486339905,\n",
       " 0.35481113329119623,\n",
       " 0.34139475582546264,\n",
       " 0.31803394935376345,\n",
       " 0.27871352667394045,\n",
       " 0.20933851139344684,\n",
       " 0.09230288151888809,\n",
       " 0.028246631535465874,\n",
       " 0.009912263879566594,\n",
       " 0.007665438307985934,\n",
       " 0.006904955320877335,\n",
       " 0.006121579771836402,\n",
       " 0.005133890910843838,\n",
       " nan]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoU_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Best IoU Threshold 0.35000000000000003\n",
      "Mean IoU:  0.5921543122705316\n",
      "Best Accuracy Threshold: 0.0\n",
      "Optimal Accuracy Sens: 0.021287148685778603\n",
      "Optimal Accuracy Spec: nan\n",
      "100 0.35000000000000003 0.5921543122705316 0.0 0.021287148685778603 nan\n",
      " \n",
      "meanIoUs_each_threshold\n",
      "[0.010643574326648494, 0.48724552243898367, 0.5277135401258942, 0.5498475766771139, 0.5654683588917647, 0.5770027665667714, 0.5870921905554032, 0.5921543122705316, 0.5913341782895771, 0.5872179844915397, 0.5807112679464413, 0.5701522687190473, 0.5516100207905706, 0.5193004721530773, 0.4994955101816096, 0.49298471781593084, 0.49216181062079545, 0.49189680363829835, 0.49162714977411653, 0.4912942981604842, 0.49999999923706057]\n",
      " \n",
      "IoU_test\n",
      "[0.021287148685778603, 0.1445629197332417, 0.20308967241611797, 0.2435086696017613, 0.28178815862868883, 0.3129934430616888, 0.3407922316954836, 0.356328486339905, 0.35481113329119623, 0.34139475582546264, 0.31803394935376345, 0.27871352667394045, 0.20933851139344684, 0.09230288151888809, 0.028246631535465874, 0.009912263879566594, 0.007665438307985934, 0.006904955320877335, 0.006121579771836402, 0.005133890910843838, nan]\n",
      " \n",
      "Dice\n",
      "[0.04168690208854877, 0.2526080781420638, 0.33761352469805667, 0.391647723179519, 0.4396797656956945, 0.4767631471667346, 0.5083445796289238, 0.5254309555961162, 0.5237794768179468, 0.5090145974446969, 0.4825884029917386, 0.4359280180587465, 0.3462033325181034, 0.16900602036412735, 0.05494135486402826, 0.019629950509737832, 0.015214252700494119, 0.013715207745058492, 0.012168668071356996, 0.010215337393889981, 0.0]\n",
      " \n",
      "precision\n",
      "[1.0, 0.9762748847640731, 0.9441934943032022, 0.9048784221486246, 0.8506586125035537, 0.789997254262575, 0.7194002143133106, 0.6452774531220006, 0.5687977314863187, 0.4944805818047514, 0.42283870372035276, 0.3432511960966791, 0.23733376746762824, 0.09611052929556041, 0.028745197996826604, 0.01004745509018411, 0.007758530617402145, 0.006980976656372175, 0.0061815539901882415, 0.005175593553105721, 0.0]\n",
      " \n",
      "recall\n",
      "[0.021287148685778603, 0.14507257727030318, 0.2055570984593004, 0.2499057149491129, 0.2964539271342827, 0.34139854103815837, 0.39303659590511375, 0.4431291799602195, 0.4853646449964855, 0.5244288677859527, 0.5620028484783893, 0.5971592830571525, 0.6396002907452639, 0.699686897454494, 0.6195663559233268, 0.4241895773491998, 0.38981809302893417, 0.3880334954078876, 0.38686131386861317, 0.389183263292527, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Precision vs Recall')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NOTE: \n",
    "#I have implemented a custom IoU functon that determines the MEAN IoU, which includes the background IoU\n",
    "#This is a heavily biased metric, and I think it makes the model seem better than it is. \n",
    "#Heavy class imbalance at the edges of the image due to padding causes the MeanIoU to skyrocket (~double reg IoU)\n",
    "#IoU from sklearn is probably better. \n",
    "\n",
    "#Total Error metric does not seem very informative. \n",
    "\n",
    "#Utility to Plot Relavent Information \n",
    "print(EPOCHS)\n",
    "print('Best IoU Threshold',best_IoU_thresh)\n",
    "print('Mean IoU: ', np.max(meanIoUs_each_threshold))\n",
    "print('Best Accuracy Threshold:', best_acc_thresh)\n",
    "\n",
    "sens = TPR_list[abs(difference_array).argmin()]\n",
    "spec = 1 - FPR_list[abs(difference_array).argmin()]\n",
    "\n",
    "print('Optimal Accuracy Sens:', sens)\n",
    "print('Optimal Accuracy Spec:', spec)\n",
    "\n",
    "#copy this over into excel, split with , delimiter. \n",
    "print(EPOCHS,best_IoU_thresh,np.max(meanIoUs_each_threshold),best_acc_thresh,sens,spec)\n",
    "print(' ')\n",
    "print('meanIoUs_each_threshold')\n",
    "print(meanIoUs_each_threshold)\n",
    "print(' ')\n",
    "print('IoUs')\n",
    "print(IoUs)\n",
    "print(' ')\n",
    "print('Dice')\n",
    "print(Dice)\n",
    "\n",
    "print(' ')\n",
    "print('precision')\n",
    "print(precision)\n",
    "\n",
    "print(' ')\n",
    "print('recall')\n",
    "print(TPR_list)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(thresholds,meanIoUs_each_threshold)\n",
    "plt.plot(thresholds, IoUs)\n",
    "plt.title(\"IoUs\")\n",
    "plt.legend(['MeanIoUs My Function', 'IoU_Test'])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(thresholds,Dice)\n",
    "plt.plot(thresholds,IoU_test)\n",
    "plt.title(\"IoU_test w/ Dice\")\n",
    "plt.legend(['Dice', 'IoU_Test'])\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(thresholds, precision)\n",
    "plt.plot(thresholds, TPR_list)\n",
    "plt.title('Precision and Recall')\n",
    "plt.xlabel('Thresholds')\n",
    "plt.ylabel('Performance')\n",
    "plt.legend(['Precision','Recall'])\n",
    "\n",
    "#Quarter-circle type curve\n",
    "plt.figure(5)\n",
    "plt.plot(precision,TPR_list)\n",
    "plt.title('Precision vs Recall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the best threshold is not a variable, we're probably missing info. \n",
    "#Load this into memory. \n",
    "with open('ROC_AUC_results_better_metrics.pkl','rb') as f:  \n",
    "    TPR_list, FPR_list, precision, thresholds, best_acc_thresh, IoUs, Dice = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021287148685778603,\n",
       " 0.1445629197332417,\n",
       " 0.20308967241611797,\n",
       " 0.2435086696017613,\n",
       " 0.28178815862868883,\n",
       " 0.3129934430616888,\n",
       " 0.3407922316954836,\n",
       " 0.356328486339905,\n",
       " 0.35481113329119623,\n",
       " 0.34139475582546264,\n",
       " 0.31803394935376345,\n",
       " 0.27871352667394045,\n",
       " 0.20933851139344684,\n",
       " 0.09230288151888809,\n",
       " 0.028246631535465874,\n",
       " 0.009912263879566594,\n",
       " 0.007665438307985934,\n",
       " 0.006904955320877335,\n",
       " 0.006121579771836402,\n",
       " 0.005133890910843838,\n",
       " nan]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe9dd7dc250>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the ROC curve and determine the optimal cut-off threshold point\n",
    "#evaluateModel.ROC_AUC(FPR_list, TPR_list)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(FPR_list, TPR_list)\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('1 - Specificity (FPR)')\n",
    "ax.set_ylabel('Sensitivity (TPR)')\n",
    "ax.set_title('Receiver-Operating Characteristic')\n",
    "ax.legend('T',loc = \"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe9e46b4eb0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Mean IoU vs Threshold\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.plot(thresholds,meanIoUs_each_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is made to analyze how well the model performed overall. \n",
    "del model\n",
    "model = smp.Unet(encoder_name = ENCODER, in_channels=1, classes = 1, aux_params = aux_params)\n",
    "modelPath = \"/home/mccrinbc/Registered_Brains_FA/models_saved/TBI_model-End-2020-09-30-19.pt\"\n",
    "model.load_state_dict(torch.load(modelPath))\n",
    "model.eval() #put into evaluation mode\n",
    "model.to(torch.device('cpu'))\n",
    "    \n",
    "#These need to be the results correspodning to the model you're loading. \n",
    "with open('results_TBI_model-End-2020-09-30-19.pkl','rb') as f:  \n",
    "    brains, labels, predictions, single_class, loss_train, loss_valid, epochLoss_train, epochLoss_valid, test_dataset = pickle.load(f)\n",
    "\n",
    "loader = DataLoader(test_dataset, batch_size = 1, shuffle = False, num_workers = num_workers)\n",
    "\n",
    "#data is called from the loader as loader.dataset[image-label_number][0 OR 1] (shape: [1,256,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomAffine(degrees=(0, 0), scale=[1, 1])\n",
      "    RandomHorizontalFlip(p=0)\n",
      "    Pad(padding=37, fill=0, padding_mode=constant)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_result(loader, index, model, threshold):\n",
    "    #The model expects there to be a batch of images. Need to specify a single image in batch. \n",
    "    \n",
    "    brain = loader.dataset[index][0].unsqueeze(0) #index brain/label, get brain only, compress to only 2D\n",
    "    label = loader.dataset[index][1][0]\n",
    "    \n",
    "    predictions, _ = model(brain)\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    thresh_prediction = predictions > threshold\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,15))\n",
    "    #fig.suptitle('Brain, True Label, Prediction')\n",
    "    ax1.imshow(brain[0][0].detach().numpy())\n",
    "    ax2.imshow(label.detach().numpy())\n",
    "    ax3.imshow(predictions[0][0].detach().numpy() > threshold)\n",
    "    \n",
    "    mean_IoU = IoU(thresh_prediction, label)\n",
    "    print(mean_IoU)\n",
    "    \n",
    "    return thresh_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571426568195942\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresh_prediction = load_image_result(loader, 30, model, best_thresh) #Takes a second to run if on CPU. \n",
    "\n",
    "#non-deterministic behaviour might be occuring because of the transform function explicity built into the\n",
    "#dataset call above. Potentially retrain and remove transform from testing dataset. \n",
    "\n",
    "#Each call of loader.dataset could be passing in the FIRST image you expect through a set of RANDOM transformations\n",
    "#This would make sense as to why we're getting randomness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/tensor_numpy.cpp:135: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-aeeaaa1641bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbrains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Pick batches you'd like to see. Must be <= batch_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Load the brains, Determine the Predictions, Threshold the prediction with optimal value, plot\n",
    "\n",
    "image_loc = 1\n",
    "brains, labels = loader[2] #Pick batches you'd like to see. Must be <= batch_total\n",
    "\n",
    "predictions, _ = model(brains)\n",
    "predictions = torch.sigmoid(predictions)\n",
    "\n",
    "predictions = predictions > best_thresh #not sure if this is going to work with a batch of images. \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(predictions[image_loc,0,:,:])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plt(labels[image_loc,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sklearn.metrics.confusion_matrix([True, False, False, True], [True, True, True, False], labels = [True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, label = torch.zeros(10,10), torch.zeros(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[2:4,3:7] = 1\n",
    "prediction[3:5,2:6] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 2., 2., 2., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n",
    "print(prediction + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  sklearn.metrics.confusion_matrix(prediction.numpy().ravel() > 0,label.numpy().ravel() > 0, labels = [True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  5],\n",
       "       [ 5, 87]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "IoU = test[0][0] / (test[0][0] + test[0][1] + test[1][0])\n",
    "print(IoU)\n",
    "\n",
    "prediction = prediction > 0\n",
    "label = label > 0\n",
    "print(prediction.shape)\n",
    "intersection = int(torch.sum(torch.mul(prediction,label)))\n",
    "#mean_IoU = IoU(prediction > 0, label > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076745563495665\n"
     ]
    }
   ],
   "source": [
    "    #Good\n",
    "    intersection = int(torch.sum(torch.mul(prediction,label)))\n",
    "    union = int(torch.sum(prediction) + torch.sum(label)) - intersection\n",
    "    IOU_predicted = intersection / (union + 0.0001) #for stability\n",
    "    print(IOU_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5638368737420701\n"
     ]
    }
   ],
   "source": [
    "    #Background IoU\n",
    "    all_zeros = (prediction + label) > 0 #before the inversion\n",
    "    intersection = int(torch.sum(~all_zeros))\n",
    "    union = int(torch.sum(~ (prediction > 0)) + torch.sum(~ (label > 0)) - intersection)\n",
    "    IOU_background = intersection / (union + 0.0001)\n",
    "    \n",
    "    mean_IOU = (IOU_background + IOU_predicted)/2\n",
    "    print(mean_IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnetDecoder(\n",
       "  (center): Identity()\n",
       "  (blocks): ModuleList(\n",
       "    (0): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
